<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="k8s,storage,"><link rel="alternate" href="/atom.xml" title="月星墙的博客" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="简介 Ceph官网、官方文档 v14.2.4 Nautilus、官方中文文档、github源码 Ceph 提供3种存储类型 ^1 块存储(RBD) 典型设备： 磁盘阵列，硬盘。主要是将裸磁盘空间映射给主机使用的 优点：多块廉价的硬盘组合起来提高容量；缺点：主机之间无法共享数据 使用场景：docker容器、日志、文件 RBD是Ceph面向块存储的接口。这种接口通常以 QEMU Driver 或者 K"><meta name="keywords" content="k8s,storage"><meta property="og:type" content="article"><meta property="og:title" content="Ceph"><meta property="og:url" content="http://blog.aezo.cn/2019/11/14/devops/ceph/index.html"><meta property="og:site_name" content="月星墙的博客"><meta property="og:description" content="简介 Ceph官网、官方文档 v14.2.4 Nautilus、官方中文文档、github源码 Ceph 提供3种存储类型 ^1 块存储(RBD) 典型设备： 磁盘阵列，硬盘。主要是将裸磁盘空间映射给主机使用的 优点：多块廉价的硬盘组合起来提高容量；缺点：主机之间无法共享数据 使用场景：docker容器、日志、文件 RBD是Ceph面向块存储的接口。这种接口通常以 QEMU Driver 或者 K"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/ceph.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/ceph-pg.png"><meta property="og:updated_time" content="2021-08-31T06:07:20.343Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Ceph"><meta name="twitter:description" content="简介 Ceph官网、官方文档 v14.2.4 Nautilus、官方中文文档、github源码 Ceph 提供3种存储类型 ^1 块存储(RBD) 典型设备： 磁盘阵列，硬盘。主要是将裸磁盘空间映射给主机使用的 优点：多块廉价的硬盘组合起来提高容量；缺点：主机之间无法共享数据 使用场景：docker容器、日志、文件 RBD是Ceph面向块存储的接口。这种接口通常以 QEMU Driver 或者 K"><meta name="twitter:image" content="http://blog.aezo.cn/data/images/devops/ceph.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"BWD6R9FA4K",apiKey:"3330f3cbaa099dfc30395de5f5b20151",indexName:"blog",hits:{per_page:10},labels:{input_placeholder:"请输入关键字",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}}}</script><link rel="canonical" href="http://blog.aezo.cn/2019/11/14/devops/ceph/"><title>Ceph | 月星墙的博客</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?085f9cd91ef2ad985f791c677472f0d1";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">月星墙的博客</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Better Code, Better Life</h1></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br> 站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.aezo.cn/2019/11/14/devops/ceph/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="smalle"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="月星墙的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Ceph</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-14T09:38:00+08:00">2019-11-14</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/devops/" itemprop="url" rel="index"><span itemprop="name">devops</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li><a href="https://ceph.com/" target="_blank" rel="noopener">Ceph官网</a>、<a href="https://docs.ceph.com/docs/nautilus/start/intro/" target="_blank" rel="noopener">官方文档 v14.2.4 Nautilus</a>、<a href="http://docs.ceph.org.cn/" target="_blank" rel="noopener">官方中文文档</a>、<a href="https://github.com/ceph/ceph" target="_blank" rel="noopener">github源码</a></li><li>Ceph 提供3种存储类型 <a href="https://www.cnblogs.com/yangxiaoyi/p/7795274.html" target="_blank" rel="noopener">^1</a><ul><li>块存储(<code>RBD</code>)<ul><li>典型设备： 磁盘阵列，硬盘。主要是将裸磁盘空间映射给主机使用的</li><li>优点：多块廉价的硬盘组合起来提高容量；缺点：主机之间无法共享数据</li><li>使用场景：docker容器、日志、文件</li><li>RBD是Ceph面向块存储的接口。这种接口通常以 QEMU Driver 或者 Kernel Module 的方式存在，这种接口需要实现 Linux 的 Block Device 的接口或者 QEMU 提供的 Block Driver 接口</li><li>相关块存储：Ceph 的 RBD、AWS 的 EBS、阿里云的盘古系统。在常见的存储中 DAS、SAN 提供的也是块存储</li><li><code>GlusterFS</code> 只提供对象存储和文件系统存储，而 <code>Ceph</code> 则提供对象存储、块存储以及文件系统存储</li></ul></li><li>文件存储<ul><li>典型设备：FTP、NFS服务器。为了克服块存储文件无法共享的问题，所以有了文件存储</li><li>优点：方便文件共享；缺点：读写速率低</li><li>使用场景：日志、有目录结构的文件存储</li><li>通常意义是支持 POSIX 接口，它跟传统的文件系统如 Ext4 是一个类型的，但区别在于分布式存储提供了并行化的能力。如 Ceph 的 CephFS(CephFS是Ceph面向文件存储的接口)，但是有时候又会把 GlusterFS、HDFS 这种非POSIX接口的类文件存储接口归入此类。当然 NFS、NAS也是属于文件系统存储</li></ul></li><li>对象存储<ul><li>典型设备：内置大容量硬盘的分布式服务器(Swift 、S3 以及 Gluster)</li><li>优点：具备块存储的读写高速，具备文件存储的共享等特性</li><li>使用场景：适合更新变动较少的数据，如：图片存储、视频存储</li><li>也就是通常意义的键值存储，其接口就是简单的GET、PUT、DEL 和其他扩展</li></ul></li></ul></li><li><p>Ceph 组件及概念 <a href="https://www.cnblogs.com/yangxiaoyi/p/7795274.html" target="_blank" rel="noopener">^1</a></p><ul><li><p>Ceph核心组件包括：Ceph OSDs、Monitors、Managers、MDSs。Ceph存储集群至少需要一个Ceph Monitor，Ceph Manager和Ceph OSD。使用Ceph Filesystem文件存储时也需要Ceph元数据服务器(Metadata Server)；使用对象存储则另需要部署rgw(Gateway)</p><p><img src="/data/images/devops/ceph.png" alt="ceph组件"></p></li><li><strong><code>Monitor</code></strong>：负责监视整个集群的运行状况，信息由维护集群成员的守护程序来提供<ul><li>Ceph Monitor(ceph-mon)维护着展示集群状态的各种图表，包括监视器图、OSD 图、归置组(PG)图、和 CRUSH 图。 Ceph 保存着发生在Monitors、OSD 和 PG上的每一次状态变更的历史信息(称为 epoch)。监视器还负责管理守护进程和客户端之间的身份验证。冗余和高可用性通常至少需要3个监视器</li><li>不存储任何数据，主要包含以下Map<ul><li><code>Monitor map</code>：包括有关monitor 节点端到端的信息，其中包括 Ceph 集群ID，监控主机名和IP以及端口。并且存储当前版本信息以及最新更改信息，通过 <code>ceph mon dump</code> 查看 monitor map</li><li><code>OSD map</code>：包括一些常用的信息，如集群ID、创建OSD map的 版本信息和最后修改信息，以及pool相关信息，主要包括pool 名字、pool的ID、类型，副本数目以及PGP等，还包括数量、状态、权重、最新的清洁间隔和OSD主机信息。通过命令 <code>ceph osd dump</code> 查看</li><li><code>PG map</code>：包括当前PG版本、时间戳、最新的OSD Map的版本信息、空间使用比例，以及接近占满比例信息，同时包括每个PG ID、对象数目、状态、OSD 的状态以及深度清理的详细信息。通过命令 <code>ceph pg dump</code> 可以查看相关状态</li><li><code>CRUSH map</code>： 包括集群存储设备信息，故障域层次结构和存储数据时定义失败域规则信息。相关命令<code>ceph osd crush xxx</code></li><li><code>MDS map</code>：包括存储当前 MDS map 的版本信息、创建当前的Map的信息、修改时间、数据和元数据POOL ID、集群MDS数目和MDS状态，可通过 <code>ceph mds dump</code> 查看</li></ul></li></ul></li><li><strong><code>OSDs</code></strong>(Object Storage Device/Daemon)<ul><li>Ceph OSD 守护进程(ceph-osd)的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他 OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。冗余和高可用性通常至少需要3个Ceph OSD。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 active+clean 状态(Ceph 默认有3个副本)</li><li>是由物理磁盘驱动器、在其之上的 Linux 文件系统以及 Ceph OSD 服务组成。Ceph OSD 将数据以对象的形式存储到集群中的每个节点的物理磁盘上，完成存储数据的工作绝大多数是由 OSD daemon 进程实现。在构建 Ceph OSD的时候，建议采用SSD 磁盘以及xfs文件系统来格式化分区</li></ul></li><li><strong><code>Managers</code></strong>: Ceph Manager守护进程(ceph-mgr)负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager守护进程还托管基于python的插件来管理和公开Ceph集群信息，包括基于Web的Ceph Manager Dashboard和 REST API。高可用性通常至少需要2个管理器</li><li><strong><code>MDS</code></strong>(Ceph Metadata Server)：Ceph 元数据服务器(MDS)为 Ceph 文件系统存储元数据(也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS)。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令</li><li><strong><code>RADOSGW</code>(<code>rgw</code>)</strong>：对象网关(Gateway)守护进程，提供对象存储服务。使用对象存储则另需要部署</li><li><code>RADOS</code>(Reliable Autonomic Distributed Object Store)：RADOS是ceph存储集群的基础。在ceph中，所有数据都以对象的形式存储，并且无论什么数据类型，RADOS对象存储都将负责保存这些对象。RADOS层可以确保数据始终保持一致</li><li><code>librados</code> 和 RADOS 交互的基本库，为应用程度提供访问接口。同时也为块存储、对象存储、文件系统提供原生的接口。Ceph 通过原生协议和 RADOS 交互，Ceph 把这种功能封装进了 librados 库，这样也能定制自己的客户端</li><li><code>ADOS块设备</code>：能够自动精简配置并可调整大小，而且将数据分散存储在多个OSD上</li><li><code>CephFS</code>：Ceph文件系统，与POSIX兼容的文件系统，基于librados封装原生接口</li><li><code>Pool</code> 是存储空间的逻辑划分，一个集群可以分成多个Pool。Pool与数据安全策略相联系，定义池就要同时定义出Pool的pg数量和数据冗余策略(副本数和纠删码，以及使用的crush规则)</li><li><code>PG</code>(Placement Grouops)：是ceph的逻辑存储单元</li></ul></li></ul><h2 id="手动安装-基于ceph-deploy安装"><a href="#手动安装-基于ceph-deploy安装" class="headerlink" title="手动安装(基于ceph-deploy安装)"></a>手动安装(基于ceph-deploy安装)</h2><blockquote><p><a href="https://github.com/ceph/ceph-deploy/tree/v2.0.1" target="_blank" rel="noopener">https://github.com/ceph/ceph-deploy/tree/v2.0.1</a></p></blockquote><ol><li>准备工作(所有节点运行)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># node1(192.168.6.131)  mon osd mgr deploy(部署节点)</span></span><br><span class="line"><span class="comment"># node2(192.168.6.132)  mon osd</span></span><br><span class="line"><span class="comment"># node3(192.168.6.133)  mon osd</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 所有节点运行</span></span><br><span class="line">yum update -y</span><br><span class="line"><span class="comment"># 时间同步。建议参考 [NTP](/_posts/linux/CentOS服务器使用说明.md#NTP(Network%20Time%20Protocol))</span></span><br><span class="line">sudo yum install -y ntp ntpdate ntp-doc</span><br><span class="line">sudo ntpdate 0.cn.pool.ntp.org</span><br></pre></td></tr></table></figure><ol start="2"><li>安装ceph-deploy(deploy节点运行)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 Ceph 源。baseurl中的`rpm-nautilus`可换成`rpm-其他ceph版本`</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/ceph.repo &lt;&lt; EOM</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages <span class="keyword">for</span> x86_64</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line"><span class="built_in">type</span>=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line"><span class="built_in">type</span>=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph <span class="built_in">source</span> packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line"><span class="built_in">type</span>=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">EOM</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line"><span class="comment"># 安装ceph-deploy</span></span><br><span class="line">mkdir /opt/ceph-cluster &amp;&amp; <span class="built_in">cd</span> /opt/ceph-cluster</span><br><span class="line">yum -y install ceph-deploy</span><br><span class="line">ceph-deploy --version <span class="comment"># 2.0.1</span></span><br><span class="line"><span class="comment"># 配置deploy节点免密钥登录其他节点</span></span><br></pre></td></tr></table></figure><ol start="3"><li>安装Storage Cluster(deploy节点运行)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参考：https://docs.ceph.com/docs/nautilus/start/quick-ceph-deploy/#create-a-cluster</span></span><br><span class="line"><span class="comment"># 初始化一个集群(会在当前目录创建ceph.conf，即将以下节点做为mon节点；还会创建keyring文件)</span></span><br><span class="line">ceph-deploy new node1 node2 node3</span><br><span class="line"><span class="comment"># 安装ceph(会创建/var/lib/ceph/目录)。有可能其中某个节点因为下载rpm Timeout导致安装失败可重新install该节点，安装成功的可执行`ceph --version`查看版本</span></span><br><span class="line"><span class="comment"># 如果出现错误`No data was received after 300 seconds`，可检查yum源是否正确，如果确认为阿里云可重试几次</span></span><br><span class="line">ceph-deploy install --release nautilus node1 node2 node3</span><br><span class="line"><span class="comment"># ceph-deploy install --release nautilus node4 # 新增Ceph Node直接执行此命令即可（无需new）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始部署monitor(会自动启动ceph-mon，监听在6789端口)</span></span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"><span class="comment"># 拷贝配置文件和admin的秘钥到相应节点(会复制keying到/etc/ceph目录，可多次运行)，从而该节点可使用ceph CLI</span></span><br><span class="line">ceph-deploy admin node1 node3</span><br><span class="line">ceph -s <span class="comment"># 查看集群状态(ceph)。mon: 3 daemons, quorum node1,node2,node3 (age 7m)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署mgr</span></span><br><span class="line">ceph-deploy mgr create node1</span><br><span class="line">ceph -s <span class="comment"># mgr: node1(active, since 47s)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (可选)部署mds。如果使用CephFS才需要</span></span><br><span class="line">ceph-deploy mds create node1 <span class="comment"># &#123;0=node1=up:creating&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (可选)部署rgw(Gateway)。如果使用对象存储才需要</span></span><br><span class="line">ceph-deploy install --rgw node1</span><br><span class="line"></span><br><span class="line"><span class="comment"># (可选)扩展mon、mgr。测试可跳过</span></span><br><span class="line">ceph-deploy mon add node2</span><br><span class="line">ceph-deploy mgr create node3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建OSD(本质是基于ceph-volume进行创建，也可到各个节点上直接运行ceph-volume相关命令)。此处/dev/sdb为物理连接上去的空磁盘，ceph会自动进行分区</span></span><br><span class="line">ceph-deploy osd create --data /dev/sdb node1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb node2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb node3</span><br><span class="line"><span class="comment"># 如果在LVM卷上创建OSD，那么--data的参数必须是volume_group/lv_name，而不是块设备的路径</span></span><br><span class="line"></span><br><span class="line">ceph -s <span class="comment"># osd: 3 osds: 3 up (since 20s), 3 in (since 20s)</span></span><br><span class="line">lsblk <span class="comment"># 在OSD节点上运行查看磁盘分区，会发现有一个`ceph--a3202c2d-xxx`的lvm分区</span></span><br><span class="line"><span class="comment"># 在各节点执行可查看ceph相关服务状态</span></span><br><span class="line">systemctl status ceph*</span><br></pre></td></tr></table></figure><ol start="4"><li>(可选)启用Dashboard(mgr节点运行)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">yum install ceph-mgr-dashboard</span><br><span class="line"><span class="comment"># 启用模块</span></span><br><span class="line">ceph mgr module <span class="built_in">enable</span> dashboard</span><br><span class="line"><span class="comment"># 安装证书</span></span><br><span class="line">ceph dashboard create-self-signed-cert</span><br><span class="line"><span class="comment"># 创建具有管理员角色的用户和密码</span></span><br><span class="line">ceph dashboard <span class="built_in">set</span>-login-credentials admin admin</span><br><span class="line"><span class="comment"># 查看ceph-mgr服务</span></span><br><span class="line">ceph mgr services</span><br><span class="line"><span class="comment"># 访问 https://192.168.6.131:8443 使用 admin/admin 登录</span></span><br></pre></td></tr></table></figure><ul><li>安装失败可进行清理环境后重新安装</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy purge node1 node2 node3 <span class="comment"># 如果执行purge，则需要重新对该节点安装ceph</span></span><br><span class="line">ceph-deploy purgedata node1 node2 node3 <span class="comment"># 删除各节点的/var/lib/ceph、/etc/ceph目录</span></span><br><span class="line"><span class="comment"># 删除deploy节点集群数据</span></span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm -f ceph.* <span class="comment"># 移除/opt/ceph-cluster目录配置文件</span></span><br></pre></td></tr></table></figure><h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><h3 id="块设备使用-rbd"><a href="#块设备使用-rbd" class="headerlink" title="块设备使用(rbd)"></a>块设备使用(rbd)</h3><ul><li><p>ceph集群外机器(客户端)使用ceph的rbd存储</p><ul><li><code>yum install -y ceph-common</code> 安装rbd操作工具(参考上文增加rpm源配置)</li><li>将ceph服务器的<code>/etc/ceph/</code>目录下的集群配置文件<code>ceph.conf</code>和客户端秘钥文件<code>ceph.client.admin.keyring</code>复制到客户端机器的<code>/etc/ceph</code>目录</li></ul></li><li><p>以集群内机器为例</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 块设备使用。参考：https://docs.ceph.com/docs/nautilus/start/quick-rbd/#create-a-block-device-pool</span></span><br><span class="line"><span class="comment">## **(mon节点运行)**创建并初始化pool</span></span><br><span class="line"><span class="comment"># 创建pool。参考：https://docs.ceph.com/docs/nautilus/rados/operations/pools/#create-a-pool</span></span><br><span class="line"><span class="comment"># PG数量的预估。集群中单个池的PG数计算公式如下：PG 总数 = (OSD 数 * 100) / 最大副本数 / 池数 (结果必须舍入到最接近2的N次幂的值)</span></span><br><span class="line">ceph osd pool create mypool 128 <span class="comment"># pool 'mypool' created。创建名为mypool的存储池</span></span><br><span class="line"><span class="comment"># 初始化pool</span></span><br><span class="line">rbd pool init mypool</span><br><span class="line"></span><br><span class="line"><span class="comment">## **(在某个osd节点)**配置块设备</span></span><br><span class="line"><span class="comment"># 在mypool存储池中，创建块设备镜像myrbd，大小为4096M</span></span><br><span class="line">rbd create myrbd --size 4096 --image-feature layering -p mypool</span><br><span class="line"><span class="comment"># 映射块设备</span></span><br><span class="line">sudo rbd map myrbd --name client.admin -p mypool <span class="comment"># 打印如`/dev/rbd2`</span></span><br><span class="line"><span class="comment"># 给此块设备创建文件系统，期间需要回车几次</span></span><br><span class="line">sudo mkfs.ext4 -m2 /dev/rbd/mypool/myrbd <span class="comment"># 或者 `sudo mkfs.xfs /dev/rbd2</span></span><br><span class="line"><span class="comment"># 挂载</span></span><br><span class="line">sudo mount /dev/rbd2 /mnt</span><br><span class="line">df -h /mnt</span><br><span class="line"><span class="comment"># 将数据写入块设备来进行检测</span></span><br><span class="line">sudo dd <span class="keyword">if</span>=/dev/zero of=/mnt/<span class="built_in">test</span> count=100 bs=1M oflag=dsync <span class="comment"># 104857600 bytes (105 MB) copied, 11.3 s, 9.3 MB/s</span></span><br><span class="line">sudo ls -lh /mnt</span><br></pre></td></tr></table></figure><h3 id="文件存储使用-CephFS"><a href="#文件存储使用-CephFS" class="headerlink" title="文件存储使用(CephFS)"></a>文件存储使用(CephFS)</h3><ul><li>文件存储使用的是OSD剩余空间，和rbd没有关系</li><li>测试过程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件存储使用(CephFS)。参考：https://docs.ceph.com/docs/nautilus/start/quick-cephfs/#create-a-filesystem</span></span><br><span class="line"><span class="comment"># 必须已经部署mds服务</span></span><br><span class="line"><span class="comment"># ceph-deploy mds create node1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### deploy节点运行</span></span><br><span class="line">ceph osd pool create cephfs_data 128</span><br><span class="line">ceph osd pool create cephfs_metadata 32</span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">ceph fs new myfs cephfs_metadata cephfs_data</span><br><span class="line">ceph fs ls <span class="comment"># name: myfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 客户端测试(测试IP为192.168.6.130。在需要使用文件存储的普通客户端机器上操作)</span></span><br><span class="line">mkdir /mnt/mycephfs</span><br><span class="line"></span><br><span class="line"><span class="comment">## 法一：使用内核驱动进行挂载，但是对内核版本等有一定要求</span></span><br><span class="line"><span class="comment"># 在 /opt/ceph-cluster/ceph.client.admin.keyring 中可查看secret秘钥(获通过`ceph auth get-key client.admin`命令读取)。更安全的方法是把密码保存在文件中，通过secretfile参数指定</span></span><br><span class="line"><span class="comment"># dmesg -T | grep ceph # 出错可通过此命令查看mount错误。如报错：`libceph: mon0 192.168.6.131:6789 missing required protocol features`，最终选用ceph-fuse进行挂载</span></span><br><span class="line"><span class="comment"># sudo mount -t ceph &#123;ip-address-of-monitor1,ip-address-of-monitor2&#125;:6789:/ /mnt/mycephfs -o name=admin,secret=xxx</span></span><br><span class="line">sudo mount -t ceph 192.168.6.131:6789:/ /mnt/mycephfs -o name=admin,secret=AQA9BZ9dMUA7BBAAYCTiaV1cTACP7GSLDxDmBg== <span class="comment"># 提示`ceph-fuse[14371]: starting fuse`则正确</span></span><br><span class="line">df -h</span><br><span class="line">sudo dd <span class="keyword">if</span>=/dev/zero of=/mnt/mycephfs/<span class="built_in">test</span> count=1024 bs=1M oflag=dsync <span class="comment"># 测试。1073741824 bytes (1.1 GB) copied, 156.862 s, 6.8 MB/s</span></span><br><span class="line"><span class="comment"># 设置开机启动。此处使用秘钥文件，则需要将`AQA9BZ9dMUA7BBAAYCTiaV1cTACP7GSLDxDmBg==`保存到/etc/ceph/cephfskey</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"192.168.6.131:6789:/ /mnt/mycephfs ceph name=admin,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 2"</span> &gt;&gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment">## 法二：使用ceph-fuse进行挂载</span></span><br><span class="line">yum install ceph-fuse -y <span class="comment"># 在客户端安装ceph-fuse程序。需要同上文一样配置Ceph源(/etc/yum.repos.d/ceph.repo)</span></span><br><span class="line"><span class="comment"># 将某mgr节点的ceph配置和秘钥复制到客户端</span></span><br><span class="line">mkdir /etc/ceph</span><br><span class="line">scp root@192.168.6.131:/etc/ceph/ceph.conf /etc/ceph/ceph.conf</span><br><span class="line">scp root@192.168.6.131:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring <span class="comment"># chmod 600</span></span><br><span class="line"><span class="comment"># 在客户端运行ceph-fuse挂载</span></span><br><span class="line">ceph-fuse --keyring /etc/ceph/ceph.client.admin.keyring --name client.admin -m 192.168.6.131:6789 /mnt/mycephfs</span><br><span class="line"><span class="comment"># 设置开机启动。需要设置ceph-fuse@/mnt/mycephfs服务自启动(设置时/mnt/mycephfs可能需要先卸载)</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"id=admin,conf=/etc/ceph/ceph.conf /mnt/mycephfs fuse.ceph defaults 0 0"</span> &gt;&gt; /etc/fstab</span><br><span class="line">systemctl <span class="built_in">enable</span> ceph-fuse.target <span class="comment"># 必须(因为ceph-fuse服务的配置中[Install]要求ceph-fuse.target)</span></span><br><span class="line"><span class="comment"># 使用`ceph-fuse@/mnt/mycephfs`会报错`Failed to execute operation: Invalid argument` [^5]。会在`/etc/systemd/system/ceph-fuse.target.wants`目录创建链接文件</span></span><br><span class="line">systemctl <span class="built_in">enable</span> ceph-fuse@-mnt-mycephfs</span><br><span class="line">systemctl start ceph-fuse@/mnt/mycephfs <span class="comment"># 可以使用-或者/</span></span><br></pre></td></tr></table></figure><ul><li><p>创建不同用户和子目录来使用CephFS(上文<code>192.168.6.131:6789:/</code>使用的是根目录) <a href="http://manjusri.ucsc.edu/2017/09/25/ceph-fuse/" target="_blank" rel="noopener">^5</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## mgr节点运行</span></span><br><span class="line"><span class="comment"># 创建用户(客户端)</span></span><br><span class="line">ceph auth add client.aezo mon <span class="string">'allow r'</span> mgr <span class="string">'allow r'</span> osd <span class="string">'allow rw pool=cephfs_data'</span> mds <span class="string">'allow rw path=/aezo'</span></span><br><span class="line"><span class="comment"># 获取用户秘钥(保存在当前运行目录，如：~)</span></span><br><span class="line">ceph auth get-or-create client.aezo -o ceph.client.aezo.keyring</span><br><span class="line">cat ceph.client.aezo.keyring</span><br><span class="line">ceph auth get client.aezo <span class="comment"># 获取用户权限</span></span><br><span class="line"><span class="comment"># 更新用户权限</span></span><br><span class="line"><span class="comment"># ceph auth caps client.aezo mon 'allow r' mgr 'allow r' osd 'allow rw pool=cephfs_data' mds 'allow rw path=/test'</span></span><br><span class="line"><span class="comment"># 约束用户只能在 myfs 存储池(上文创建)的/aezo目录读写</span></span><br><span class="line">ceph fs authorize myfs client.aezo /aezo rw</span><br><span class="line"></span><br><span class="line"><span class="comment">## 客户端节点运行</span></span><br><span class="line">scp root@192.168.6.131:~/ceph.client.aezo.keyring /etc/ceph/ceph.client.aezo.keyring</span><br><span class="line">chmod 600 /etc/ceph/ceph.client.aezo.keyring</span><br><span class="line"><span class="comment"># 创建数据目录并挂载</span></span><br><span class="line">mkdir /mnt/cephaezo</span><br><span class="line">chmod 1777 /mnt/cephaezo</span><br><span class="line">ceph-fuse -n client.aezo -m 192.168.6.131:6789 /mnt/cephaezo --keyring /etc/ceph/ceph.client.aezo.keyring -r /aezo <span class="comment"># -r/--client_mountpoint指定子路径</span></span><br><span class="line"><span class="comment"># 设置开机启动。可和上文的/mnt/mycephfs同时成功挂载</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"none /mnt/cephaezo fuse.ceph ceph.id=aezo,ceph.client_mountpoint=/aezo,defaults,_netdev 0 0"</span> &gt;&gt; /etc/fstab <span class="comment"># 上文是老板写法，这是新版本写法</span></span><br><span class="line">vi /usr/lib/systemd/system/ceph-fuse@.service</span><br><span class="line">systemctl <span class="built_in">enable</span> ceph-fuse@\x2dn\x20client.aezo\x20\x2dr\x20-aezo\x20-mnt-cephaezo --now <span class="comment"># --now立即启动。转义符(\x2d为-; \x20为空格; -为/)参考[http://blog.aezo.cn/2017/01/16/arch/nginx/](/_posts/arch/nginx.md#自定义服务)</span></span><br></pre></td></tr></table></figure><ul><li>常见错误<ul><li>执行<code>ceph-fuse</code>时提示<code>failed to fetch mon config (--no-mon-config to skip)</code><ul><li>可能由于–keyring秘钥错误</li></ul></li><li>执行<code>ceph-fuse</code>时提示<code>ceph-fuse[12595]: ceph mount failed with (2) No such file or directory</code><ul><li>本案例是因为<code>/aezo</code>目录没有创建。可在上文myfs绑定的客户端目录(/mnt/mycephfs)下创建aezo目录</li><li>貌似还可以使用<code>cephfs-shell</code>创建目录。关于cephfs-shell(目前处于alpha阶段)安装和使用可参考 <a href="https://docs.ceph.com/docs/master/cephfs/cephfs-shell/" target="_blank" rel="noopener">https://docs.ceph.com/docs/master/cephfs/cephfs-shell/</a> 。其中cephfs-shell源码位于 <a href="https://raw.githubusercontent.com/ceph/ceph/v14.2.4/src/tools/cephfs/cephfs-shell" target="_blank" rel="noopener">https://raw.githubusercontent.com/ceph/ceph/v14.2.4/src/tools/cephfs/cephfs-shell</a></li></ul></li></ul></li></ul></li><li>也可将CephFS导出为NFS服务器、在Hadoop中使用</li></ul><h3 id="对象存储"><a href="#对象存储" class="headerlink" title="对象存储"></a>对象存储</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对象存储使用。参考：https://docs.ceph.com/docs/nautilus/start/quick-rgw/</span></span><br><span class="line"><span class="comment"># 必须已经部署Object Gateway服务</span></span><br><span class="line"><span class="comment"># ceph-deploy install --rgw node1 node3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建网关实例</span></span><br><span class="line"><span class="comment"># 创建一个网关实例。会在此节点启动一个 radosgw 服务，默认监听在7480端口</span></span><br><span class="line">ceph-deploy rgw create node1</span><br><span class="line"><span class="comment"># 修改端口。在网关实例节点修改配置</span></span><br><span class="line">cat &gt;&gt; /etc/ceph/ceph.conf &lt;&lt; EOM</span><br><span class="line">[client.rgw.node1]</span><br><span class="line">rgw_frontends = <span class="string">"civetweb port=80"</span></span><br><span class="line">EOM</span><br><span class="line"><span class="comment"># 在网关实例节点启动</span></span><br><span class="line">systemctl restart ceph-radosgw@rgw.node1</span><br><span class="line">wget http://192.168.6.131:80 <span class="comment"># 默认监听在7480端口。显示`&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;`</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用</span></span><br><span class="line"><span class="comment"># 可使用第三方软件访问，如亚马逊 s3 客户端</span></span><br></pre></td></tr></table></figure><h2 id="k8s使用ceph存储"><a href="#k8s使用ceph存储" class="headerlink" title="k8s使用ceph存储"></a>k8s使用ceph存储</h2><h3 id="直接使用"><a href="#直接使用" class="headerlink" title="直接使用"></a>直接使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 在Kubernetes(v1.15)集群的所有Node上安装Ceph-common包</span></span><br><span class="line"><span class="comment"># yum install -y ceph-common</span></span><br><span class="line">yum install -y ceph-common-14.2.4-0.el7.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在mon节点，获取账号秘钥并进行base64</span></span><br><span class="line">grep key /etc/ceph/ceph.client.admin.keyring |awk <span class="string">'&#123;printf "%s", $NF&#125;'</span>|base64 <span class="comment"># ceph auth get-key client.admin | base64</span></span><br><span class="line">kubectl apply -f ceph-secret.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">## 静态PV</span></span><br><span class="line"><span class="comment"># ceph-mon节点上，创建ceph存储池rbd，且在此池中创建10G大小的ceph-image镜像(也可使用原有的)</span></span><br><span class="line">ceph osd pool create rbd 128</span><br><span class="line">rbd pool init rbd</span><br><span class="line">rbd create ceph-image -s 10240 -p rbd</span><br><span class="line"><span class="comment"># CentOS的3.10及以下内核需执行</span></span><br><span class="line"><span class="comment"># rbd feature disable ceph-image object-map fast-diff deep-flatten</span></span><br><span class="line"><span class="comment"># 当pv被使用时，会自动执行 rbd map 对镜像进行映射和创建文件系统</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并查看PV的状态是否正常，如果获取的状态是Available则说明该PV处于可用状态，并且没有被PVC绑定</span></span><br><span class="line">kubectl apply -f ceph-pv.yaml</span><br><span class="line">kubectl get pv</span><br><span class="line"><span class="comment"># 创建pvc</span></span><br><span class="line">kubectl apply -f ceph-pvc.yaml</span><br><span class="line"><span class="comment"># 测试pod(pod被分派到的k8s节点只需要安装ceph-common即可，认证用到的key)</span></span><br><span class="line">kubectl apply -f ceph-pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">## 动态PV(只适用于手动安装kubernetes，如果基于kubeadm安装请见下文rbd-provisioner使用)</span></span><br><span class="line">kubectl apply -f ceph-sc.yaml</span><br><span class="line">kubectl apply -f ceph-pvc.yaml <span class="comment"># 开启storageClassName配置</span></span><br><span class="line"><span class="comment"># 此时会发现 pvc 一直处于pending状态。`kubectl describe pvc ceph-pvc`时提示 `Failed to provision volume with StorageClass "ceph-sc": failed to create rbd image: executable file not found in $PATH, command output`。解决方法见下文使用rbd-provisioner提供rbd持久化存储</span></span><br></pre></td></tr></table></figure><ul><li>yaml文件</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## ceph-secret.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"kubernetes.io/rbd"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCcldjdGRwNndMS1JBQU9IZHVZdG83SHZwOU96Q01oc255emc11Q==</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ceph-pv.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteOnce"]</span></span><br><span class="line"><span class="attr">  rbd:</span></span><br><span class="line"><span class="attr">    monitors:</span> </span><br><span class="line"><span class="bullet">    -</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.131</span><span class="string">:6789</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.132</span><span class="string">:6789</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.133</span><span class="string">:6789</span></span><br><span class="line"><span class="attr">    pool:</span> <span class="string">rbd</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">ceph-image</span></span><br><span class="line"><span class="attr">    user:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">    secretRef:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">    fsType:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">    readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ceph-pvc.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># storageClassName: ceph-sc # 使用动态PV时需要开启</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteOnce"]</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ceph-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-pod</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ceph-busybox</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["sleep",</span> <span class="string">"60000"</span><span class="string">]</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">ceph-vol1</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/usr/share/busybox</span> </span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ceph-vol1</span></span><br><span class="line"><span class="attr">    persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">      claimName:</span> <span class="string">ceph-pvc</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ceph-sc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-sc</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  monitors:</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.131</span><span class="string">:6789,192.168.6.132:6789,192.168.6.133:6789</span></span><br><span class="line"><span class="attr">  adminId:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  adminSecretName:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">  adminSecretNamespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  pool:</span> <span class="string">rbd</span></span><br><span class="line">  <span class="comment"># 正式建议使用特定的ceph用户</span></span><br><span class="line"><span class="attr">  userId:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  userSecretName:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="使用rbd-provisioner-推荐"><a href="#使用rbd-provisioner-推荐" class="headerlink" title="使用rbd-provisioner(推荐)"></a>使用rbd-provisioner(推荐)</h3><ul><li><code>rbd-provisioner</code>为kubernetes 1.5+版本提供了类似于<code>kubernetes.io/rbd</code>的ceph rbd持久化存储动态配置实现 <a href="https://jimmysong.io/kubernetes-handbook/practice/rbd-provisioner.html" target="_blank" rel="noopener">^2</a></li><li>如果使用kubeadm来部署集群，或者将<code>kube-controller-manager</code>以容器的方式运行。这种方式下，kubernetes在创建使用ceph rbd pv/pvc时没任何问题，但使用dynamic provisioning自动管理存储生命周期时会报错，提示”rbd: create volume failed, err: failed to create rbd image: executable file not found in $PATH:”。问题来自gcr.io提供的kube-controller-manager容器镜像未打包<code>ceph-common</code>组件，缺少了rbd命令，因此无法通过rbd命令为pod创建rbd image</li><li>安装及使用</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/k8s/ceph/rbd-provisioner</span><br><span class="line"></span><br><span class="line"><span class="comment">## 部署rbd-provisioner</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> clusterrole.yaml clusterrolebinding.yaml deployment.yaml role.yaml rolebinding.yaml serviceaccount.yaml ; <span class="keyword">do</span> wget https://github.com/kubernetes-incubator/external-storage/raw/v5.5.0/ceph/rbd/deploy/rbac/<span class="variable">$file</span>; <span class="keyword">done</span></span><br><span class="line">sed -r -i <span class="string">"s/namespace: [^ ]+/namespace: kube-system/g"</span> ./clusterrolebinding.yaml ./rolebinding.yaml</span><br><span class="line">sed -r -i <span class="string">"s/quay.io/quay.mirrors.ustc.edu.cn/g"</span> *.yaml</span><br><span class="line">kubectl -n kube-system apply -f ./</span><br><span class="line">kubectl describe deployments.apps -n kube-system rbd-provisioner</span><br><span class="line"></span><br><span class="line"><span class="comment"># (可选)创建kube存储池，和创建客户端(用户)kube并授权。也可使用已有的</span></span><br><span class="line">ceph osd pool create kube 128</span><br><span class="line"><span class="comment"># 创建kube客户端</span></span><br><span class="line">ceph auth get-or-create client.kube mon <span class="string">'allow r'</span> osd <span class="string">'allow class-read object_prefix rbd_children, allow rwx pool=kube'</span> -o ceph.client.kube.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建(最终会创建StorageClass)。当PVC创建后StorageClass就会自动创建PV、rbd镜像(eg：`rbd ls -p kube` 显示 `kubernetes-dynamic-pvc-**`)</span></span><br><span class="line">kubectl create -f ceph-rbd-pool-default.yaml</span><br><span class="line"><span class="comment"># 此方式创建的 PV 名称和ceph镜像名称并不对应，但是可在 PV 详情中查看对应的镜像名称</span></span><br><span class="line"><span class="comment"># kubectl get pv pvc-41fea0ae-f8aa-422a-a5b3-84fab3a0a9e3 -o go-template='&#123;&#123;.spec.rbd.image&#125;&#125;' # 查看pv对应哪个rbd image</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于helm部署，如果删除helm-release则会与原PV关联断开，即使重新部署也会产生新的PV；如果仅仅删除pod，则原PV是继续使用的(RS会重新创建)</span></span><br></pre></td></tr></table></figure><ul><li>ceph-rbd-pool-default.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## ceph-rbd-pvc-test.yaml (可选，测试自动创建PVC配置)</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-rbd-pvc-test</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 尽管定义了默认SC，但此时不定义 StorageClass 会失败</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">ceph-rbd-sc-default</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteOnce"]</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">## ceph-kube-secret.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-kube-secret</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"kubernetes.io/rbd"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="comment"># ceph auth get-key client.kube | base64</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCcldjdGRwNndMS1JBQU9IZHVZdG83SHZwOU96Q01oc255e65321==</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">## ceph-admin-secret.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-admin-secret</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"kubernetes.io/rbd"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="comment"># ceph auth get-key client.admin | base64</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCcldjdGRwNndMS1JBQU9IZHVZdG83SHZwOU96Q01oc255emc11Q==</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">## ceph-rbd-sc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-rbd-sc-default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="comment"># 默认StorageClass，当PVC不定义 storageClassName 时则模式使用此 StorageClass</span></span><br><span class="line">    <span class="string">storageclass.beta.kubernetes.io/is-default-class:</span> <span class="string">"true"</span></span><br><span class="line"><span class="comment"># provisioner需要设置为ceph.com/rbd，不是默认的kubernetes.io/rbd，这样rbd的请求将由rbd-provisioner来处理</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">ceph.com/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  monitors:</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.131</span><span class="string">:6789,192.168.6.132:6789,192.168.6.133:6789</span></span><br><span class="line"><span class="attr">  adminId:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  adminSecretName:</span> <span class="string">ceph-admin-secret</span></span><br><span class="line"><span class="attr">  adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  pool:</span> <span class="string">kube</span></span><br><span class="line">  <span class="comment"># pod需要使用此sc提供的pv时，该pod所在命名空间必须要有客户端 kube 对应的秘钥 ceph-kube-secret</span></span><br><span class="line"><span class="attr">  userId:</span> <span class="string">kube</span></span><br><span class="line"><span class="attr">  userSecretName:</span> <span class="string">ceph-kube-secret</span></span><br><span class="line"><span class="attr">  fsType:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">  imageFormat:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">  imageFeatures:</span> <span class="string">"layering"</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="运维案例"><a href="#运维案例" class="headerlink" title="运维案例"></a>运维案例</h2><h3 id="增加Ceph-Node-添加OSD"><a href="#增加Ceph-Node-添加OSD" class="headerlink" title="增加Ceph Node/添加OSD"></a>增加Ceph Node/添加OSD</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 增加Ceph Node相关操作(略)：yum 源，免秘钥配置，ceph的版本，主机名，防火墙，selinux，ntp</span></span><br><span class="line"><span class="comment"># deploy节点运行</span></span><br><span class="line">ceph-deploy install --release nautilus node4</span><br><span class="line">ceph -s <span class="comment"># 此时集群无任何变化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在node4上添加OSD(本质是先ceph-deploy config push推送ceph.conf配置到远程，再远程执行ceph-volume命令)</span></span><br><span class="line">ceph-deploy osd create --data /dev/sdb --zap-disk node4</span><br></pre></td></tr></table></figure><h3 id="更换Ceph-Node-更换osd、更换mon"><a href="#更换Ceph-Node-更换osd、更换mon" class="headerlink" title="更换Ceph Node(更换osd、更换mon)"></a>更换Ceph Node(更换osd、更换mon)</h3><ul><li>说明 <a href="https://blog.csdn.net/xiongwenwu/article/details/53120415" target="_blank" rel="noopener">^3</a><ul><li>原先有3个ceph节点(1个ssd+2个hhd)，现需要将其中2个hhd节点(node2、node3)换成新的2个ssd节点(node4、node5)。且总共3个OSD，pool副本数设置为3</li><li>整个迁移过程将会消耗很长时间，此处由于涉及的osd较少，大概几个小时即可。如果数据较多，有可能迁移几天</li><li>实际测试过程中ceph状态为HEALTH_WARN(此时128个PG并非全部处于active状态)时，客户端无法使用存储；当ceph状态变为HEALTH_OK(128个PG都有active状态。如：1 active+recovering+remapped, 110 active+clean, 17 active+remapped+backfill_wait)时，客户端可正常使用</li></ul></li><li>更换osd</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 以node4为例，node5同理</span></span><br><span class="line"><span class="comment">## 增加Ceph Node相关操作(略)：yum 源，免秘钥配置，ceph的版本，主机名，防火墙，selinux，ntp</span></span><br><span class="line"><span class="comment"># deploy节点运行，在node4上安装ceph</span></span><br><span class="line">ceph-deploy install --release nautilus node4</span><br><span class="line">ceph -w <span class="comment"># 此时集群无任何变化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (视情况执行)由于osd个数=副本数，当out出一个osd后，pool状态一直会停留在active+clean+remapped(因为此时剩余的osd不够创建副本数)。此处先将kube池的副本设置成2</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> kube size 2</span><br><span class="line"></span><br><span class="line"><span class="comment">## 销毁老的OSD</span></span><br><span class="line"><span class="comment"># 将osd out。如：ceph osd out 2</span></span><br><span class="line">ceph osd out &#123;osd-num&#125;</span><br><span class="line"><span class="comment"># 关闭原来node3上的osd进程</span></span><br><span class="line">ssh node3 &amp;&amp; systemctl stop ceph-osd@&#123;osd-num&#125; &amp;&amp; <span class="built_in">exit</span></span><br><span class="line"><span class="comment"># 将osd标记为已销毁(此时tree中还存在此osd)。保持ID完整（允许重复使用此ID），但删除cephx密钥，使数据永久不可读</span></span><br><span class="line">ceph osd destroy &#123;osd-num&#125; --yes-i-really-mean-it</span><br><span class="line"></span><br><span class="line"><span class="comment">## 复制ceph.conf集群配置文件和ceph.bootstrap-osd.keyring秘钥文件到新节点</span></span><br><span class="line">ceph-deploy config push node4</span><br><span class="line">scp ceph.bootstrap-osd.keyring root@node4:/var/lib/ceph/bootstrap-osd/ceph.keyring</span><br><span class="line"><span class="comment">## (在新节点node4上执行)使用原来的osd编号(在新节点上)创建新的osd</span></span><br><span class="line">ssh node4</span><br><span class="line">ceph-volume lvm zap /dev/sdX <span class="comment"># 此时&#123;osd-num&#125;状态仍然为node3 destroy</span></span><br><span class="line"><span class="comment"># prepare + activate = create</span></span><br><span class="line">ceph-volume lvm prepare --osd-id &#123;osd-num&#125; --data /dev/sdX <span class="comment"># 此时&#123;osd-num&#125;状态仍然为node3 down</span></span><br><span class="line">ceph-volume lvm activate &#123;osd-num&#125; <span class="comment"># 此时&#123;osd-num&#125;状态仍然为node4 up。此时会产生PG迁移</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (视情况执行)将kube池设置回3个副本</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> kube size 3</span><br></pre></td></tr></table></figure><ul><li>更换mon等</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 更新所有ceph节点的ceph.conf配置文件</span></span><br><span class="line"><span class="comment"># 编辑配置文件。对文件中的`mon_host`参数添加新的节点ip；增加参数`public_network=192.168.1.0/24`</span></span><br><span class="line">vi /opt/ceph-cluster/ceph.conf</span><br><span class="line">ceph-deploy --overwrite-conf config push node1 node2 node3 node4 node5 <span class="comment"># 用上述ceph.conf文件覆盖所有ceph节点的ceph.conf配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 待node4操作完后，node5同样操作</span></span><br><span class="line"><span class="comment"># 添加一个mon节点</span></span><br><span class="line">ceph-deploy mon add node4 <span class="comment"># 会启动该节点的system mon进程</span></span><br><span class="line"><span class="comment"># 观察ceph状态，直到新mon节点重新进入，并且处于HEALTH_OK状态再继续后续操作</span></span><br><span class="line">ceph -w</span><br><span class="line"><span class="comment"># 当集群状态处于HEALTH_OK状态后，再移除历史mon节点(建议等待1-3min再执行)</span></span><br><span class="line">ceph-deploy mon destroy node2 <span class="comment"># 会停止该节点的system mon进程，并将备份原mon数据到/var/lib/ceph/mon-removed目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 再操作node5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 再次更新所有ceph节点的ceph.conf配置文件</span></span><br><span class="line"><span class="comment"># 编辑配置文件。对文件中的`mon_host`参数，去掉旧的节点ip；修改`mon_initial_members`参数，去掉历史旧的mon节点名称，增加新的mon节点名称</span></span><br><span class="line">vi /opt/ceph-cluster/ceph.conf</span><br><span class="line">ceph-deploy --overwrite-conf config push node1 node4 node5 <span class="comment"># 用上述ceph.conf文件覆盖所有ceph节点的ceph.conf配置文件</span></span><br></pre></td></tr></table></figure><h3 id="删除OSD"><a href="#删除OSD" class="headerlink" title="删除OSD"></a>删除OSD</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 将osd移出集群(此时osd状态由in up变为out up)</span></span><br><span class="line"><span class="comment"># ceph osd crush reweight osd.&#123;osd-num&#125; 0 # 此命令类似out，都会导致该osd上的所有PG迁出</span></span><br><span class="line">ceph osd out &#123;osd-num&#125; </span><br><span class="line"><span class="comment"># 观察集群状态，等到重新变为active+clean再进行后续操作</span></span><br><span class="line">ceph -w</span><br><span class="line"></span><br><span class="line"><span class="comment">## 停止osd进程(此时osd状态由out up变为out down)</span></span><br><span class="line">ssh &#123;osd-host&#125;</span><br><span class="line">sudo systemctl stop ceph-osd@&#123;osd-num&#125;</span><br><span class="line"><span class="comment"># 退出osd-host，进入admin-node执行后续命令</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 删除</span></span><br><span class="line"><span class="comment"># 从CRUSH映射中删除OSD，并删除其身份验证密钥</span></span><br><span class="line">ceph osd purge &#123;osd-num&#125; --yes-i-really-mean-it <span class="comment"># purge命令为Luminous版本增加，类似于以下3个命令</span></span><br><span class="line"><span class="comment"># ceph osd crush remove osd.&#123;osd-num&#125; # 从CRUSH映射中删除OSD，使其不再接收数据</span></span><br><span class="line"><span class="comment"># ceph auth del osd.&#123;osd-num&#125; # 删除OSD身份验证密钥</span></span><br><span class="line"><span class="comment"># ceph osd rm &#123;osd-num&#125; # 卸下OSD</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 清理磁盘</span></span><br><span class="line">/usr/sbin/wipefs --all /dev/sdX</span><br><span class="line">ls /dev/mapper/ceph--9f84f55e--6baa--4ac2--a721--4dfd97f9a8f1-osd--block--cf4926bd--96c4--4787--a1fc--af3078ba3d0c | xargs -I% -- dmsetup remove % <span class="comment"># 此处可通过 `lsblk` 查看对应映射名称</span></span><br></pre></td></tr></table></figure><h3 id="镜像扩容缩容-rbd-images"><a href="#镜像扩容缩容-rbd-images" class="headerlink" title="镜像扩容缩容(rbd-images)"></a>镜像扩容缩容(rbd-images)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 管理界面操作</span></span><br><span class="line">Block - images - xxx - 编辑 - Size</span><br><span class="line"></span><br><span class="line"><span class="comment">## 管理端操作</span></span><br><span class="line">rbd ls -p kube <span class="comment"># 列举所有镜像</span></span><br><span class="line">rbd du kube/kubernetes-dynamic-pvc-8286cda0-09d1-11ea-89b1-5aa8347da671 <span class="comment"># 查看此镜像空间使用情况</span></span><br><span class="line"><span class="comment"># 调整大小为20G(1024换算)</span></span><br><span class="line">rbd resize kube/kubernetes-dynamic-pvc-8286cda0-09d1-11ea-89b1-5aa8347da671 --size 20480</span><br><span class="line">rbd du kube/kubernetes-dynamic-pvc-8286cda0-09d1-11ea-89b1-5aa8347da671 <span class="comment"># 重新查看镜像空间使用情况</span></span><br><span class="line"><span class="comment"># 然后修改k8s pvc对应大小为指定大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 客户端扩容方式略</span></span><br></pre></td></tr></table></figure><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ul><li>调试说明<ul><li>日志目录<code>/var/log/ceph</code></li></ul></li><li>安装deploy时，提示<code>No module named pkg_resources</code><ul><li>解决：在deploy节点安装<code>wget https://bootstrap.pypa.io/ez_setup.py -O - | python</code>(osd等节点可视情况安装)</li></ul></li><li>rbd map映射镜像时，提示<code>rbd: image ceph-image: image uses unsupported features: 0x38</code><ul><li>原因：CentOS的3.10内核仅支持layering、exclusive-lock，其他feature概不支持</li><li>解决<ul><li>升级内核</li><li>或者手动disable镜像feature<code>rbd feature disable ceph-image object-map fast-diff deep-flatten exclusive-lock</code></li><li>或者在各osd节点修改配置文件<code>/etc/ceph/ceph.conf</code>，添加配置<code>rbd_default_features = 1</code>。在创建镜像时指定–image-format参数如：<code>rbd create ceph-image --size 10G --image-format 1 --image-feature layering</code></li></ul></li></ul></li><li>创建存储池时提示<code>pg_num 128 size 3 would mean 768 total pgs, which exceeds max 750 (mon_max_pg_per_osd 250 * num_in_osds 3)</code><ul><li>原因：测试环境只有3个osd，设置复制个数为3，且每个osd默认最大pg数为250(mon_max_pg_per_osd)。而且已经创建过一个pood(128个pg)，因此创建第二个pool(128个pg)则报错，超过osd最大pg限制</li><li>解决：增加osd数，或临时提高osd最大pg数(正式环境不建议太高)，或者将pool的pg设置的小一些。参考<a href="#PG和PGP">PG</a></li></ul></li><li><p>k8s pod创建时提示<code>MountVolume.WaitForAttach failed for volume &quot;ceph-pv&quot; : rbd image rbd/ceph-image is still being used</code></p><ul><li>原因：强制删除了pod，导致重新创建此pod时，改镜像被旧pod锁定</li><li><p>解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看锁定</span></span><br><span class="line">rbd lock ls ceph-image</span><br><span class="line"><span class="comment"># 解除锁定</span></span><br><span class="line">rbd lock rm rbd/ceph-image <span class="string">"auto 18446462598732840961"</span> client.4259</span><br></pre></td></tr></table></figure></li></ul></li><li><p>k8s pod创建时提示<code>MountVolume.WaitForAttach failed for volume &quot;ceph-pv&quot; : rbd: map failed exit status 110...unable to find a keyring on /etc/ceph/ceph.client.admin.keyring...Connection timed out</code>，调度到对应的k8s节点上提示<code>missing required protocol features</code>(dmesg -T | grep ceph)</p><ul><li>原因：由于内核版本不够高导致一些 Ceph 需要的特性没有得到支持(此问题出现的版本为<code>Centos7 Linux 4.4.196-1</code>) <a href="https://github.com/grzhan/keng/issues/2" target="_blank" rel="noopener">^6</a></li><li><p>解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mgr节点运行。修改 CRUSH MAP 的配置，将 chooseleaf_vary_r 与 chooseleaf_stable 设为 0</span></span><br><span class="line">ceph osd getcrushmap -o crush <span class="comment"># 产生crush临时文件</span></span><br><span class="line">crushtool -i crush --<span class="built_in">set</span>-chooseleaf_vary_r 0  --<span class="built_in">set</span>-chooseleaf_stable 0 -o crush.new <span class="comment"># 产生crush.new临时文件</span></span><br><span class="line">ceph osd setcrushmap -i crush.new</span><br></pre></td></tr></table></figure></li></ul></li><li><p>ceph警告 <code>HEALTH_WARN application not enabled on 1 pool(s)</code>，且通过k8s storageClass创建的镜像无法查询到</p><ul><li>原因：新创建的pool没有开启rbd application</li><li>解决：<code>ceph osd pool application enable kube rbd</code> (此处存储池为kube)</li></ul></li><li><p><code>rbd: error: image still has watchers</code> <strong>无法删除镜像</strong>，参考：<a href="https://www.cnblogs.com/sisimi/p/7776633.html" target="_blank" rel="noopener">https://www.cnblogs.com/sisimi/p/7776633.html</a></p><ul><li>原因：镜像无法删除的原因一般为存在快照或者watcher(此情况)</li><li><p>解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 存在watcher的情况</span></span><br><span class="line">rbd status kube/img <span class="comment"># 获取kube/img镜像的watcher</span></span><br><span class="line">ceph osd blacklist add 192.168.6.131:0/1135656048 <span class="comment"># 添加watcher到黑名单1h(1小时候会自动移除)</span></span><br><span class="line"><span class="comment"># rbd rm kube/img # 可选，删除镜像</span></span><br><span class="line"><span class="comment"># ceph osd blacklist rm 192.168.6.131:0/1135656048 # 手动移除黑名单</span></span><br><span class="line">ceph osd blacklist ls <span class="comment"># 查看黑名单</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>更换mon时，执行<code>ceph-deploy mon add node4</code>报错<code>admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</code></p><ul><li>解决：修改<code>ceph.conf</code>文件中的<code>mon_host</code>、<code>public_network</code>并推送到所有节点 <a href="https://www.zybuluo.com/dyj2017/note/920621" target="_blank" rel="noopener">^7</a></li></ul></li></ul><h2 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h2><h3 id="常用"><a href="#常用" class="headerlink" title="常用"></a>常用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph -s/-w          <span class="comment"># 查看集群状态(-w实时状态查看)</span></span><br><span class="line">ceph osd tree       <span class="comment"># 查看所有osd</span></span><br><span class="line"></span><br><span class="line">rbd ls kube         <span class="comment"># 列举kube存储池所有存储块</span></span><br><span class="line">rbd showmapped      <span class="comment"># 列举本机已映射的块设备(pool、image等信息)。存储块必须映射后才能挂载</span></span><br><span class="line">rbd du kube/image-xxx <span class="comment"># 查看镜像空间使用情况</span></span><br></pre></td></tr></table></figure><h3 id="ceph"><a href="#ceph" class="headerlink" title="ceph"></a>ceph</h3><ul><li>概要</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ceph -h <span class="comment"># 查看帮助(非常多命令)。查看某个命令帮助：`ceph -h osd pool`</span></span><br><span class="line">ceph -v <span class="comment"># ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</span></span><br><span class="line">ceph -s/-w <span class="comment"># 查看集群状态(-w实时状态查看)</span></span><br><span class="line">ceph    <span class="comment"># 进入ceph命令行(exit退出)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取法定节点信息</span></span><br><span class="line">ceph quorum_status --format json-pretty</span><br><span class="line"><span class="comment"># 列举pool</span></span><br><span class="line">ceph osd pool ls</span><br></pre></td></tr></table></figure><h4 id="ceph-config"><a href="#ceph-config" class="headerlink" title="ceph config"></a>ceph config</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### ceph config &lt;xxx&gt;. eg: `ceph config ls`</span></span><br><span class="line">ls      <span class="comment"># 列举所有配置项名称</span></span><br><span class="line"><span class="built_in">help</span> &lt;key&gt;  <span class="comment"># 查看某个配置帮助。eg：`ceph config help mon_max_pg_per_osd -f json-pretty`(-f 以json格式输出)</span></span><br><span class="line">get &lt;who&gt; &#123;&lt;key&gt;&#125; <span class="comment"># 获取某个角色(如：osd.0、osd.1等)的配置。eg：`ceph config get osd.0 mon_max_pg_per_osd` 获取osd.0的mon_max_pg_per_osd(默认250)</span></span><br><span class="line"><span class="built_in">set</span> &lt;who&gt; &lt;name&gt; &lt;value&gt; &#123;--force&#125;</span><br><span class="line">rm &lt;who&gt; &lt;name&gt;</span><br><span class="line">show &lt;who&gt; &#123;&lt;key&gt;&#125;  <span class="comment"># 类似get</span></span><br><span class="line">show-with-defaults &lt;who&gt;</span><br><span class="line"><span class="built_in">log</span> &#123;&lt;int&gt;&#125;</span><br><span class="line">assimilate-conf</span><br><span class="line">dump</span><br></pre></td></tr></table></figure><h4 id="ceph-osd"><a href="#ceph-osd" class="headerlink" title="ceph osd"></a>ceph osd</h4><h5 id="ceph-osd-pool"><a href="#ceph-osd-pool" class="headerlink" title="ceph osd pool"></a>ceph osd pool</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### ceph osd pool &lt;xxx&gt;。eg: `ceph osd pool ls`</span></span><br><span class="line">ls      <span class="comment"># 列举 pool</span></span><br><span class="line">get &lt;poolname&gt; &lt;var&gt; <span class="comment"># 获取存储池参数</span></span><br><span class="line">destroy <span class="comment"># 将osd标记为已销毁。保持ID完整（允许重复使用此ID），但删除cephx密钥，使数据永久不可读</span></span><br><span class="line">rm      <span class="comment"># 删除存储池(会物理删除所有数据)</span></span><br><span class="line">        <span class="comment"># ceph osd pool rm rbd rbd --yes-i-really-really-mean-it # (警告)删除"rbd"存储池(会物理删除所有数据)，需重复输入存储池名称</span></span><br><span class="line">            <span class="comment"># 默认未开启mon删除存储池功能，如动态增加配置 `ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'` 后再执行删除方可。具体参考：https://stackoverflow.com/questions/45012905/removing-pool-mon-allow-pool-delete-config-option-to-true-before-you-can-destro</span></span><br></pre></td></tr></table></figure><h5 id="ceph-osd-crush"><a href="#ceph-osd-crush" class="headerlink" title="ceph osd crush"></a>ceph osd crush</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## ceph osd crush -h</span></span><br><span class="line">ceph osd crush remove osd.2         <span class="comment"># 从 crush 里面删除此OSD(会触发PG迁移)</span></span><br><span class="line">ceph osd crush reweight osd.2 0     <span class="comment"># reweight取值[0,1]的浮点数据；reweight值越小，从此OSD迁出的数据越多。此时将此OSD的权重设置为0(会触发PG迁移，会全部移走)</span></span><br></pre></td></tr></table></figure><h5 id="ceph-osd-1"><a href="#ceph-osd-1" class="headerlink" title="ceph osd "></a>ceph osd<xxx></xxx></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### ceph osd &lt;xxx&gt;</span></span><br><span class="line">df      <span class="comment"># 查看集群中每个osd上的分布情况(空间大小、使用空间、存放的PG数、状态信息)</span></span><br><span class="line">out &lt;ids&gt; [&lt;ids&gt;...] <span class="comment"># out某个编号的osd，或者使用 [any|all] 移除所有。此时会触发PG迁移，如果某个osd的进程停止并不会触发迁移(顶多主副PG角色变化)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### ceph osd blacklist &lt;xxx&gt;。黑名单</span></span><br><span class="line">ls</span><br><span class="line">add|rm &lt;EntityAddr&gt; &#123;&lt;<span class="built_in">float</span>[0.0-]&gt;&#125;</span><br><span class="line">    <span class="comment"># ceph osd blacklist add 192.168.6.131:0/1135656048 # 添加watcher到黑名单(`rbd status kube/img` 获取镜像的watcher)</span></span><br><span class="line">clear</span><br></pre></td></tr></table></figure><h4 id="ceph-xxx"><a href="#ceph-xxx" class="headerlink" title="ceph xxx"></a>ceph xxx</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### ceph tell</span></span><br><span class="line">tell &lt;name (type.id)&gt; &lt;args&gt; [&lt;args&gt;...] <span class="comment"># 发送一个命令到特定的守护进程</span></span><br><span class="line">    <span class="comment"># ceph tell mon.* injectargs '--mon_osd_report_timeout 400' # 正在匹配所有mon守护进程，分别注入参数(动态修改配置)。对比`ceph daemon`</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### ceph health</span></span><br><span class="line">ceph health [detail]    <span class="comment"># 查看集群健康状态</span></span><br></pre></td></tr></table></figure><h4 id="Local-commands"><a href="#Local-commands" class="headerlink" title="Local commands"></a>Local commands</h4><ul><li>Local commands表示只能在角色所在的主机上进行设置，其他一般为Monitor commands(在mon节点上设置即可)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon &#123;type.id|path&#125; &lt;cmd&gt; <span class="comment"># 基于某个角色的守护进程执行相关命令</span></span><br><span class="line">    <span class="comment"># ceph daemon osd.0 config get mon_max_pg_per_osd # 获取osd.0的mon_max_pg_per_osd配置值(如果此时osd.1不在该主机上则获取不到)</span></span><br><span class="line">    <span class="comment"># 注意使用daemon可以修改(set)临时修改配置，但是重启进程后配置会恢复到默认参数，在ceph.conf中修改可永久有效</span></span><br></pre></td></tr></table></figure><h3 id="ceph-volume"><a href="#ceph-volume" class="headerlink" title="ceph-volume"></a>ceph-volume</h3><ul><li>作用：使用物理磁盘或lvm创建Ceph OSDs(各Storage Node均可运行)</li><li><a href="https://docs.ceph.com/docs/nautilus/ceph-volume/" target="_blank" rel="noopener">Doc</a></li><li><a href="https://www.dovefi.com/post/ceph-volume%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90osd%E5%88%9B%E5%BB%BA%E5%92%8C%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/" target="_blank" rel="noopener">https://www.dovefi.com/post/ceph-volume%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90osd%E5%88%9B%E5%BB%BA%E5%92%8C%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ceph-volume -h</span><br><span class="line"></span><br><span class="line"><span class="comment">#Available subcommands:</span></span><br><span class="line">lvm                      <span class="comment">#Use LVM and LVM-based technologies like dmcache to deploy OSDs</span></span><br><span class="line">    prepare                  <span class="comment"># 准备OSD，一般和activate联合使用。Format an LVM device and associate it with an OSD</span></span><br><span class="line">    activate                 <span class="comment"># 启动systemd中osd进程。Discover and mount the LVM device associated with an OSD ID and start the Ceph OSD</span></span><br><span class="line">    create                   <span class="comment"># 创建OSD。效果同 prepare + activate(分别使这两个命令可以避免数据立即均衡)</span></span><br><span class="line">    list                     <span class="comment"># 列举和ceph相关的逻辑卷和设备</span></span><br><span class="line">    batch                    <span class="comment"># Automatically size devices for multi-OSD provisioning with minimal interaction</span></span><br><span class="line">        --osds-per-device       <span class="comment"># 此osd节点的每个设备可创建介个osd介质(分区，如osd0和osd1目录)，默认1</span></span><br><span class="line">        <span class="comment"># ceph-volume lvm batch --osds-per-device 2 /dev/sdb # 将/dev/sdb分成2个分区，运行命令后会显示预览，输入yes后正式进行分区(实际是使用系统lvm进行分区)</span></span><br><span class="line">    trigger                  <span class="comment"># systemd helper to activate an OSD</span></span><br><span class="line">    zap                      <span class="comment"># 格式化磁盘</span></span><br><span class="line">        <span class="comment"># ceph-volume lvm zap /dev/sdb  # 格式化。如果遇到逻辑卷无法删除可执行 `dmsetup remove &#123;lv-name&#125;`</span></span><br><span class="line">        <span class="comment"># ceph-volume lvm zap /dev/sdb --destroy</span></span><br><span class="line">simple                   <span class="comment"># 用于接管用ceph-disk创建的osd(ceph-disk为基于块设备创建，ceph-volume为基于lvm创建)</span></span><br><span class="line">inventory                <span class="comment">#Get this nodes available disk inventory # 查看节点的存储设备(如连接的物理磁盘)</span></span><br><span class="line"><span class="comment">#optional arguments:</span></span><br><span class="line">  -h, --<span class="built_in">help</span>            <span class="comment">#show this help message and exit</span></span><br><span class="line">  --cluster CLUSTER     <span class="comment">#Cluster name (defaults to "ceph")</span></span><br><span class="line">  --<span class="built_in">log</span>-level LOG_LEVEL <span class="comment">#Change the file log level (defaults to debug)</span></span><br><span class="line">  --<span class="built_in">log</span>-path LOG_PATH   <span class="comment">#Change the log path (defaults to /var/log/ceph)</span></span><br></pre></td></tr></table></figure><h3 id="rbd-块存储"><a href="#rbd-块存储" class="headerlink" title="rbd 块存储"></a>rbd 块存储</h3><ul><li><code>ceph-common</code>包中含有此命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">See <span class="string">'rbd help &lt;command&gt;'</span> <span class="keyword">for</span> <span class="built_in">help</span> on a specific <span class="built_in">command</span>.</span><br><span class="line"><span class="comment"># 全局可选项</span></span><br><span class="line">--name/-n arg <span class="comment"># client name</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### rbd &lt;xxx&gt; &lt;pool-name&gt;/&lt;image-name&gt;  # pool-name不填则为rbd</span></span><br><span class="line">info        <span class="comment"># 查看镜像信息 Show information about image size, striping, etc.</span></span><br><span class="line">    <span class="comment"># rbd info my-image         # 查看存储块信息</span></span><br><span class="line">        <span class="comment"># 如：features: layering(支持分层), exclusive-lock(支持独占锁), object-map(支持对象映射，依赖 exclusive-lock), fast-diff(快速计算差异，依赖 object-map), deep-flatten(支持快照扁平化操作), striping(支持条带化 v2), journaling(支持记录 IO 操作，依赖exclusive-lock)</span></span><br><span class="line">    <span class="comment"># rbd info replicapool-test/csi-vol-436aafe9-df4b-11e9-854c-1ae38aa085c6    # 查看某个存储块信息</span></span><br><span class="line">status      <span class="comment"># 查看状态，如watcher</span></span><br><span class="line">list (ls)   <span class="comment"># 列举所有存储块 List rbd images.</span></span><br><span class="line">    <span class="comment"># rbd ls 列举所有存储块</span></span><br><span class="line">    <span class="comment"># rbd ls [-p/--pool] replicapool-test   # 列举 replicapool-test 存储块池中的存储块</span></span><br><span class="line">remove (rm) <span class="comment"># 删除块设备映像</span></span><br><span class="line">    <span class="comment"># rbd rm &#123;pool-name&#125;/&#123;image-name&#125;</span></span><br><span class="line">resize      <span class="comment"># 调整块设备映像大小 Resize (expand or shrink) image.</span></span><br><span class="line">    <span class="comment"># rbd resize --size 2048 myrbd # 增大myrbd存储块。最终大小为2048M，下同</span></span><br><span class="line">    <span class="comment"># rbd resize --size 2048 myrbd --allow-shrink # 缩小myrbd存储块</span></span><br><span class="line">copy (cp)                         <span class="comment"># Copy src image to dest.</span></span><br><span class="line">create                            <span class="comment"># Create an empty image.</span></span><br><span class="line">    <span class="comment"># rbd create myrbd --size 10G --image-feature layering -p mypool # 在mypool存储池中，创建块设备镜像myrbd，大小为10G</span></span><br><span class="line">deep copy (deep cp)               <span class="comment"># Deep copy src image to dest.</span></span><br><span class="line">device list (showmapped)          <span class="comment"># List mapped rbd images.</span></span><br><span class="line">    <span class="comment"># rbd showmapped    # 列举已映射块设备(pool、image等信息)，存储块必须映射后才能挂载</span></span><br><span class="line">device map (map)                  <span class="comment"># 映射块设备</span></span><br><span class="line">    <span class="comment"># rbd map --name client.admin mypool/myrbd # 执行成功打印设备名称，如`/dev/rbd2`</span></span><br><span class="line">device unmap (unmap)              <span class="comment"># 取消块设备映射</span></span><br><span class="line">    <span class="comment"># rbd unmap /dev/rbd2 # 取消块设备映射</span></span><br><span class="line">disk-usage (du)                   <span class="comment"># Show disk usage stats for pool, image or snapshot.</span></span><br><span class="line">    <span class="comment"># rbd du pool-test/csi-image    # 显示 pool-test/csi-image 镜像已使用空间大小</span></span><br><span class="line">diff                              <span class="comment"># Print extents that differ since a previous snap, or image creation.</span></span><br><span class="line">    <span class="comment"># rbd diff kube/img | awk '&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024 " MB" &#125;' # 计算 kube/img 镜像已使用空间大小(可在对应客户端通过`df -h`查看)</span></span><br><span class="line">lock list (lock ls)               <span class="comment"># Show locks held on an image.</span></span><br><span class="line">    <span class="comment"># rbd lock list pool-test/csi-image     # 显示 pool-test/csi-image 镜像被锁定列表</span></span><br><span class="line">lock remove (lock rm)             <span class="comment"># Release a lock on an image.</span></span><br><span class="line">    <span class="comment"># rbd lock rm rbd/ceph-image "auto 18446462598732840961" client.4259 # 解除锁定(image id locker)</span></span><br></pre></td></tr></table></figure><h3 id="rados"><a href="#rados" class="headerlink" title="rados"></a>rados</h3><ul><li><code>ceph-common</code>包中含有此命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rados -h</span><br><span class="line">rados -v <span class="comment"># ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### rados &lt;xxx&gt;</span></span><br><span class="line"><span class="comment">## POOL COMMANDS</span></span><br><span class="line">lspools         <span class="comment"># 列举存储池</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## OBJECT COMMANDS</span></span><br><span class="line">listwatchers &lt;obj-name&gt;   <span class="comment"># 列举对象的watchers。eg: `rados -p kube listwatchers rbd_header.1041643c9869` (ID可在`rbd info kube/img`中查看此image的header对象block_name_prefix)</span></span><br></pre></td></tr></table></figure><h2 id="ceph-conf-配置文件"><a href="#ceph-conf-配置文件" class="headerlink" title="ceph.conf 配置文件"></a>ceph.conf 配置文件</h2><ul><li><code>ceph-deploy new</code>初始化出来的文件</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[global]</span></span><br><span class="line"><span class="attr">fsid</span> = <span class="number">29</span>f2b08c-ba52-<span class="number">4</span>ed8-be0e-b593739da912</span><br><span class="line"><span class="attr">mon_initial_members</span> = node1, node2, node3</span><br><span class="line"><span class="attr">mon_host</span> = <span class="number">192.168</span>.<span class="number">1.131</span>,<span class="number">192.168</span>.<span class="number">1.132</span>,<span class="number">192.168</span>.<span class="number">1.133</span></span><br><span class="line"><span class="attr">auth_cluster_required</span> = cephx</span><br><span class="line"><span class="attr">auth_service_required</span> = cephx</span><br><span class="line"><span class="attr">auth_client_required</span> = cephx</span><br></pre></td></tr></table></figure><ul><li>字段说明</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## monitors：https://docs.ceph.com/docs/nautilus/rados/configuration/common/#monitors</span></span><br><span class="line"><span class="comment">## 全局配置</span></span><br><span class="line"><span class="section">[global]</span></span><br><span class="line"><span class="comment"># 集群 ID</span></span><br><span class="line"><span class="attr">fsid</span> = <span class="number">29</span>f2b08c-ba52-<span class="number">4</span>ed8-be0e-b593739da912</span><br><span class="line"><span class="comment"># 可以使用下划线或空格，如 `mon initial members = ...`</span></span><br><span class="line"><span class="comment"># 集群启动期间，群集中初始监视器的ID。如果指定，则Ceph需要奇数个监视器来形成初始仲裁</span></span><br><span class="line"><span class="attr">mon_initial_members</span> = node1, node2, node3</span><br><span class="line"><span class="comment"># 所有mon节点的ip列表</span></span><br><span class="line"><span class="attr">mon_host</span> = <span class="number">192.168</span>.<span class="number">1.131</span>,<span class="number">192.168</span>.<span class="number">1.132</span>,<span class="number">192.168</span>.<span class="number">1.133</span></span><br><span class="line"><span class="comment"># 所有集群的前端公共网络</span></span><br><span class="line"><span class="attr">public_network</span> = <span class="number">192.168</span>.<span class="number">1.0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment"># 各服务认证方式使用cephx</span></span><br><span class="line"><span class="attr">auth_cluster_required</span> = cephx</span><br><span class="line"><span class="attr">auth_service_required</span> = cephx</span><br><span class="line"><span class="attr">auth_client_required</span> = cephx</span><br></pre></td></tr></table></figure><h2 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h2><h3 id="PG和PGP"><a href="#PG和PGP" class="headerlink" title="PG和PGP"></a>PG和PGP</h3><ul><li><p><code>PG</code>(Placement Groups) 它是ceph的逻辑存储单元。在数据存储到ceph时，先打散成一系列对象，再结合基于对象名的哈希操作、复制级别、PG数量，产生目标PG号。根据复制级别的不同，每个PG在不同的OSD上进行复制和分发。可以把PG想象成存储了多个对象的逻辑容器，这个容器映射到多个具体的OSD</p><p> <img src="/data/images/devops/ceph-pg.png" alt="ceph-pg"></p><ul><li>PG存在的意义是提高ceph存储系统的性能和扩展性。如果没有PG，就难以管理和跟踪数以亿计的对象，它们分布在数百个OSD上。对ceph来说，管理PG比直接管理每个对象要简单得多</li><li>每个PG需要消耗一定的系统资源包括CPU、内存等。通常来说，增加PG的数量可以减少OSD的负载，一个推荐配置是每OSD对应50-100个PG。如果数据规模增大，在集群扩容的同时PG数量也需要调整。CRUSH会管理PG的重新分配</li><li>Pool由PG构成，对象存到Pool是存到特定的PG中，可以理解为对象的虚拟目录。在创建Pool的时候就要把pg数量规划好，PG数量只可以增大不可以缩小</li><li>在架构层次上，PG 位于 RADOS 层的中间。往上负责接收和处理来自客户端的请求，往下负责将这些数据请求翻译为能够被本地对象存储所能理解的事务</li><li>正常的 PG 状态是 <code>100% active + clean</code>，这表示所有的 PG 是可访问的，所有副本都对全部 PG 都可用。<a href="https://www.infoq.cn/article/4N2whf1y1lH_Hd5QYOkW" target="_blank" rel="noopener">PG 状态详解</a></li></ul></li><li><code>PGP</code>(Placement Group for Placement)</li><li><p>对应Pool的PG和PGP个数调整</p><ul><li>PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数</li><li>PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中</li><li>PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动</li><li><p>相关命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get rbd pg_num <span class="comment"># 获取rbd池的pg数</span></span><br><span class="line">ceph osd pool get rbd pgp_num <span class="comment"># 获取rbd池的pgp数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整PG数量</span></span><br><span class="line">    <span class="comment"># 调整pg时，原则上每次增加一倍</span></span><br><span class="line">    <span class="comment"># 此命令本质是当前pg的分裂(1个pg分裂成2个)。mon会首先更新自身的osd map中的pg数量，然后将osd map同步给osd；osd根据新的pg数量进行计算，进行本地分裂；分裂过程就是创建新目录，然后数据移动过去</span></span><br><span class="line">    <span class="comment"># 分裂期间CPU和IO会打满，负载非常高，影响时间看数据量而定(30s-10min)</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> &lt;pool_name&gt; pg_num &lt;pg_num&gt;</span><br><span class="line"><span class="comment"># 调整PGP数量</span></span><br><span class="line">    <span class="comment"># pg数量大于pgp数量时，heath状态显示warning。需要扩充pgp数量</span></span><br><span class="line">    <span class="comment"># 调整pgp数量会使pg在集群内重新分布</span></span><br><span class="line">    <span class="comment"># 该操作会影响一半的数据进行迁移，对集群影响非常大</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> &lt;pool_name&gt; pgp_num &lt;pg_num&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>每个Pool分配PG建议个数计算公式 <code>Total PGs = ((Total_number_of_OSD * 100) / max_replication_count) / pool_count</code>(结果往上取靠近2的N次方的值)</p></li><li><p>OSD最大PG数默认为<code>mon_max_pg_per_osd=250</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config get osd.0 mon_max_pg_per_osd <span class="comment"># 获取osd.0的最大pg数</span></span><br></pre></td></tr></table></figure></li><li><p>PG迁移</p><ul><li>由于CRUSH算法的伪随机性，对于一个PG来说，如果 OSD tree 结构不变的话，它所分布在的 OSD 集合总是固定的(同一棵tree下的OSD结构不变/不增减)，即此PG不会进行迁移。反之 OSD tree 变化则会触发PG迁移</li></ul></li></ul><hr><p>参考文章</p></div><div></div><div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> smalle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="http://blog.aezo.cn/2019/11/14/devops/ceph/" title="Ceph">http://blog.aezo.cn/2019/11/14/devops/ceph/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/k8s/" rel="tag"># k8s</a> <a href="/tags/storage/" rel="tag"># storage</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/11/08/db/sqlite/" rel="next" title="sqlite"><i class="fa fa-chevron-left"></i> sqlite</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2019/11/22/mobile/flutter/" rel="prev" title="Flutter">Flutter<i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div> <a target="_blank" href="https://github.com/oldinaction/ChatGPT-MP"><img class="nofancybox" style="width:300px;margin:0 auto" src="https://cdn7.aezo.cn/one/chat/chat-gpt-open-banner.jpg" alt="ChatGPT开源小程序" title="ChatGPT开源小程序"></a></div></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400" alt="smalle"><p class="site-author-name" itemprop="name">smalle</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">162</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">14</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">149</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"></div><div> 欢迎关注公众号：阿壹族 <img class="nofancybox" style="width:120px;margin:0 auto" src="https://cdn7.aezo.cn/common/qrcode/ayz_qrcode.jpg" alt="欢迎关注公众号：阿壹族" title="欢迎关注公众号：阿壹族"></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#手动安装-基于ceph-deploy安装"><span class="nav-number">2.</span> <span class="nav-text">手动安装(基于ceph-deploy安装)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简单使用"><span class="nav-number">3.</span> <span class="nav-text">简单使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#块设备使用-rbd"><span class="nav-number">3.1.</span> <span class="nav-text">块设备使用(rbd)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储使用-CephFS"><span class="nav-number">3.2.</span> <span class="nav-text">文件存储使用(CephFS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对象存储"><span class="nav-number">3.3.</span> <span class="nav-text">对象存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k8s使用ceph存储"><span class="nav-number">4.</span> <span class="nav-text">k8s使用ceph存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#直接使用"><span class="nav-number">4.1.</span> <span class="nav-text">直接使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用rbd-provisioner-推荐"><span class="nav-number">4.2.</span> <span class="nav-text">使用rbd-provisioner(推荐)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运维案例"><span class="nav-number">5.</span> <span class="nav-text">运维案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#增加Ceph-Node-添加OSD"><span class="nav-number">5.1.</span> <span class="nav-text">增加Ceph Node/添加OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更换Ceph-Node-更换osd、更换mon"><span class="nav-number">5.2.</span> <span class="nav-text">更换Ceph Node(更换osd、更换mon)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#删除OSD"><span class="nav-number">5.3.</span> <span class="nav-text">删除OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#镜像扩容缩容-rbd-images"><span class="nav-number">5.4.</span> <span class="nav-text">镜像扩容缩容(rbd-images)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常见问题"><span class="nav-number">6.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相关命令"><span class="nav-number">7.</span> <span class="nav-text">相关命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常用"><span class="nav-number">7.1.</span> <span class="nav-text">常用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph"><span class="nav-number">7.2.</span> <span class="nav-text">ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ceph-config"><span class="nav-number">7.2.1.</span> <span class="nav-text">ceph config</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ceph-osd"><span class="nav-number">7.2.2.</span> <span class="nav-text">ceph osd</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ceph-osd-pool"><span class="nav-number">7.2.2.1.</span> <span class="nav-text">ceph osd pool</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ceph-osd-crush"><span class="nav-number">7.2.2.2.</span> <span class="nav-text">ceph osd crush</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ceph-osd-1"><span class="nav-number">7.2.2.3.</span> <span class="nav-text">ceph osd</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ceph-xxx"><span class="nav-number">7.2.3.</span> <span class="nav-text">ceph xxx</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Local-commands"><span class="nav-number">7.2.4.</span> <span class="nav-text">Local commands</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph-volume"><span class="nav-number">7.3.</span> <span class="nav-text">ceph-volume</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rbd-块存储"><span class="nav-number">7.4.</span> <span class="nav-text">rbd 块存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rados"><span class="nav-number">7.5.</span> <span class="nav-text">rados</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ceph-conf-配置文件"><span class="nav-number">8.</span> <span class="nav-text">ceph.conf 配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#进阶知识"><span class="nav-number">9.</span> <span class="nav-text">进阶知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PG和PGP"><span class="nav-number">9.1.</span> <span class="nav-text">PG和PGP</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2016 - <span itemprop="copyrightYear">2024</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">smalle</span>&nbsp;&nbsp;&nbsp;&nbsp;<div class="powered-by"> 由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="powered-by"> 主题 - <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="theme-info"> <a class="theme-link" target="_blank" href="https://tongji.baidu.com/main/overview/10000542408/overview/index?siteId=18991897">站长统计</a></div></div><div class="ad"> <span style="font-weight:700">AD&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=oby5nolb">阿里云大礼包</a></div></div><div class="aezocn"> <span style="font-weight:700">&copy;AEZO.CN&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="powered-by"> <a target="_blank" href="https://shengqitech.aezo.cn/">圣骑科技</a></div><div class="powered-by"> <a target="_blank" href="https://cdn7.aezo.cn/common/qrcode/one_qrcode.jpg">【One能抽屉】小程序</a></div><div class="theme-info"> <a target="_blank" href="http://shop.aezo.cn/">杂货铺(省钱小助手)</a></div></div><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?085f9cd91ef2ad985f791c677472f0d1";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.1"></script></body></html>