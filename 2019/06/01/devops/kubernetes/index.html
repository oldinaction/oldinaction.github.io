<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="k8s,docker,cncf,"><link rel="alternate" href="/atom.xml" title="Smalle's Blog | AEZOCN" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="简介 官网、github、Doc 相关文章：https://github.com/rootsongjc/kubernetes-handbook/ 、 https://www.cnblogs.com/linuxk/category/1248289.html (视频相关) 、 https://feisky.gitbooks.io/kubernetes/content/ 知识图谱 国内镜像参考http:"><meta name="keywords" content="k8s,docker,cncf"><meta property="og:type" content="article"><meta property="og:title" content="Kubernetes"><meta property="og:url" content="http://blog.aezo.cn/2019/06/01/devops/kubernetes/index.html"><meta property="og:site_name" content="Smalle&#39;s Blog | AEZOCN"><meta property="og:description" content="简介 官网、github、Doc 相关文章：https://github.com/rootsongjc/kubernetes-handbook/ 、 https://www.cnblogs.com/linuxk/category/1248289.html (视频相关) 、 https://feisky.gitbooks.io/kubernetes/content/ 知识图谱 国内镜像参考http:"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/kubernetes-arch.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-service.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-pod-lifecycle.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-pod-action.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-userspace.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-ipvs.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-ingress.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-network.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/devops/k8s-network2.webp"><meta property="og:updated_time" content="2021-08-31T06:07:20.347Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Kubernetes"><meta name="twitter:description" content="简介 官网、github、Doc 相关文章：https://github.com/rootsongjc/kubernetes-handbook/ 、 https://www.cnblogs.com/linuxk/category/1248289.html (视频相关) 、 https://feisky.gitbooks.io/kubernetes/content/ 知识图谱 国内镜像参考http:"><meta name="twitter:image" content="http://blog.aezo.cn/data/images/devops/kubernetes-arch.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"BWD6R9FA4K",apiKey:"3330f3cbaa099dfc30395de5f5b20151",indexName:"blog",hits:{per_page:10},labels:{input_placeholder:"请输入关键字",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}}}</script><link rel="canonical" href="http://blog.aezo.cn/2019/06/01/devops/kubernetes/"><title>Kubernetes | Smalle's Blog | AEZOCN</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div style="display:none"><script src="//s95.cnzz.com/z_stat.php?id=cnzz_stat_icon_1276691827&web_id=cnzz_stat_icon_1276691827" language="JavaScript"></script></div></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Smalle's Blog | AEZOCN</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Better Code, Better Life</h1></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br> 站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.aezo.cn/2019/06/01/devops/kubernetes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="smalle"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Smalle's Blog | AEZOCN"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Kubernetes</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-01T12:38:00+08:00">2019-06-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/devops/" itemprop="url" rel="index"><span itemprop="name">devops</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><div></div><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li><a href="https://kubernetes.io/zh" target="_blank" rel="noopener">官网</a>、<a href="https://github.com/kubernetes/kubernetes" target="_blank" rel="noopener">github</a>、<a href="https://kubernetes.io/zh/docs/" target="_blank" rel="noopener">Doc</a></li><li>相关文章：<a href="https://github.com/rootsongjc/kubernetes-handbook/" target="_blank" rel="noopener">https://github.com/rootsongjc/kubernetes-handbook/</a> 、 <a href="https://www.cnblogs.com/linuxk/category/1248289.html" target="_blank" rel="noopener">https://www.cnblogs.com/linuxk/category/1248289.html</a> (视频相关) 、 <a href="https://feisky.gitbooks.io/kubernetes/content/" target="_blank" rel="noopener">https://feisky.gitbooks.io/kubernetes/content/</a></li><li><a href="https://github.com/yangchuansheng/k8s-knowledge" target="_blank" rel="noopener">知识图谱</a></li><li>国内镜像参考<a href="/_posts/devops/docker.md#Docker介绍">http://blog.aezo.cn/2017/06/25/devops/docker/</a></li><li><strong>本文若无特殊说明，kubernetes版本均为 v1.15.0</strong></li><li>对所有环境进行集成的<a href="https://www.rancher.cn/" target="_blank" rel="noopener">rancher</a>、适用于物联网/树莓派的轻量级Kubernetes版本<a href="https://www.rancher.cn/k3s/" target="_blank" rel="noopener">k3s</a></li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li><code>Kubernetes</code>是Google基于<code>Borg</code>开源的容器编排调度引擎，作为<code>CNCF</code>(Cloud Native Computing Foundation)最重要的组件之一，它的目标不仅仅是一个编排系统，而是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes可以帮你将系统自动地达到和维持在这个状态。Kubernetes作为云原生应用的基石</li><li>自动化运维演进<ul><li><code>Ansible</code>是一种自动化运维工具，基于paramiko开发的，并且基于模块化工作，Ansible是一种集成IT系统的配置管理、应用部署、执行特定任务的开源平台，它是基于python语言，由Paramiko和PyYAML两个关键模块构建。同类型的如<code>Puppet</code></li><li>Docker容器编排<ul><li>Docker三剑客：docker compose(面向单击编排)、docker swarm(面向多机编排)、docker machine(将一个主机初始化为swarm节点)</li><li>mesos(资源分配工具)、marathon(面向容器的编排框架)</li><li>kubernetes(市场占据80%份额)</li></ul></li></ul></li><li><code>Docker</code>与<code>K8s</code>(kubernetes)<ul><li>Docker本质上是一种虚拟化技术，类似于KVM、XEN、VMWARE，但其更轻量化，且将Docker部署在Linux环境时，其依赖于Linux容器技术(LXC)。Docker较传统KVM等虚拟化技术的一个区别是无内核，即多个Docker虚拟机共享宿主机内核，简而言之，可把Docker看作是无内核的虚拟机，每个Docker虚拟机有自己的软件环境，相互独立</li><li>K8s与Docker之间的关系，如同Openstack之于KVM、VSphere之于VMWARE。K8S是容器集群管理系统，底层容器虚拟化可使用Docker技术，应用人员无需与底层Docker节点直接打交道，通过K8s统筹管理即可</li></ul></li><li>相关概念<ul><li><code>DevOps</code> 是开发与运维之间沟通的过程。透过自动化”软件交付”和”架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠</li><li><code>CI</code> 持续集成</li><li><code>CD</code> 持续交付，Delivery</li><li><code>CD</code> 持续部署，Deployment</li><li><code>Service Mesh</code></li></ul></li><li><code>kubefed</code>(<code>Kubernetes Federation V2</code>) K8s 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 K8s 的调度和计算存储连接要求。而集群联邦(Federation)就是为提供跨 Region 跨服务商 K8s 集群服务而设计的</li></ul><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li><p>整体架构</p><p> <img src="/data/images/devops/kubernetes-arch.png" alt="kubernetes-arch"></p><ul><li>用户执行kubectl/userClient向apiserver发起一个命令</li><li>经过认证授权后，经过scheduler的各种策略，得到一个目标node，然后告诉apiserver</li><li>apiserver 会请求相关node的kubelet，通过kubelet把pod运行起来，apiserver还会将pod的信息保存在etcd</li><li>pod运行起来后，controllermanager就会负责管理pod的状态，如，若pod挂了，controllermanager就会重新创建一个一样的pod，或者像扩缩容等</li><li>pod有一个独立的ip地址，但pod的IP是易变的，如异常重启，或服务升级的时候，IP都会变，这就有了service；完成service工作的具体模块是kube-proxy；在每个node上都会有一个kube-proxy，在任何一个节点上访问一个service的虚拟ip，都可以访问到pod；service的IP可以在集群内部访问到，外部访问需要暴露服务</li></ul></li><li>Kubernetes主要由以下几个核心组件组成<ul><li><code>etcd</code> 保存了整个集群的状态</li><li><code>API Server</code> 提供了资源操作的唯一入口(CLI/GUI)，并提供认证、授权、访问控制、API注册和发现等机制</li><li><code>Controller Manager</code> 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等</li><li><code>Scheduler</code> 负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上</li><li><code>Kubelet</code> 负责维护容器的生命周期，同时也负责Volume(CSI)和网络(CNI)的管理</li><li><code>Container Runtime</code> 负责镜像管理以及Pod和容器的真正运行(CRI)</li><li><code>Kube-proxy</code> 负责为Service提供cluster内部的服务发现和负载均衡</li></ul></li><li>其他附件(AddOns)，推荐额外插件<ul><li><code>CoreDNS</code> 负责为整个集群提供DNS服务</li><li><code>Dashboard</code> 提供GUI</li><li><code>Ingress Controller</code> 为服务提供外网入口</li><li><code>Prometheus</code> 提供资源监控</li><li><code>Federation</code> 提供跨可用区的集群</li></ul></li><li><p>名词概念</p><ul><li><code>Master</code> 包含etcd、API Server、Scheduler、Controller Manager；Kubernetes由<code>Master</code>和<code>Node</code>节点组成；Master至少一个，也可部署多个实现HA</li><li><code>Node</code> 包含Kubelet、Container Runtime、Kube-proxy；Node节点一般为多个，运行容器的物理节点</li><li><code>Pod</code> 每个Node可以理解为一个docker宿主机；一个Node中可以包含多个<code>Pod</code>(Kubernetes的最小可执行单元)；一个Pod上可以运行多个容器(一般只有一个)，K8s不直接操作容器而是操作Pod，此时Pod中运行的多个容器共用一些物理参数(如hostname)，此时Pod类似于虚拟机<ul><li><code>Label</code> 为了区分Pod，可以给每个Pod加一些元数据标签Label(Key-Value)。可通过<code>Label Selector</code>挑选出对应的Label</li></ul></li><li><strong>控制器<code>Controller Manager</code></strong>(自动管理Pod的生命周期)<ul><li><code>ReplicationController</code>(RC，每个Pod保存一个副本，早先K8s版本)</li><li><code>ReplicaSet</code>(RS，副本集)：<code>Deployment</code>(管理无状态)、<code>StatefulSet</code>(管理有状态，如mysql节点)、<code>DaemonSet</code>(每个Node运行一个特定Pod)、<code>Job</code>(运行一次任务)、<code>Cronjob</code>(周期运行任务)</li><li><code>HorizontalPodAutoscaler</code>(<code>HPA</code>，自动水平伸缩控制器)</li></ul></li><li><p><code>Service</code> 服务</p><ul><li>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题，一个Pod的IP和端口随时可能发生变化，要稳定地提供服务需要服务发现和负载均衡能力</li><li>客户端需要访问的服务就是Service对象，每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务</li><li>在Kubernetes集群中微服务的负载均衡是由Kube-proxy实现的</li><li><p>Service是手动创建的，可以创建成供K8s外部访问或只能内部访问的</p><p><img src="/data/images/devops/k8s-service.png" alt="k8s-service"></p></li></ul></li><li>Cluster(集群) 和 Namespace(命名空间)<ul><li>Cluster 是计算、存储和网络资源的集合，Kubernetes 利用这些资源运行各种基于容器的应用，最简单的 Cluster 可以只有一台主机(它既是 Mater 也是 Node)，安装的默认集群为<code>kubernetes</code></li><li>Namespace 可以将一个物理的 Cluster 逻辑上划分成多个虚拟 Cluster，每个 Cluster 就是一个 Namespace。不同 Namespace 里的资源是完全隔离的<ul><li>Kubernetes 默认创建了两个 Namespace：kube-system 和 default</li></ul></li></ul></li></ul></li></ul><h2 id="K8s集群安装"><a href="#K8s集群安装" class="headerlink" title="K8s集群安装"></a>K8s集群安装</h2><h3 id="基于kubeadm安装k8s"><a href="#基于kubeadm安装k8s" class="headerlink" title="基于kubeadm安装k8s"></a>基于kubeadm安装k8s</h3><ul><li>环境介绍<ul><li>基于Centos7.3(3.10.0-514.el7.x86_64)安装<code>kubernetes-1.15.0</code></li><li>node1(192.168.6.131，2C2G勉强可测试)、node2(192.168.6.132，2C2G)、node3(192.168.6.133，2C2G)；node1为Master节点，node2和node3为Node节点</li><li>默认使用root用户操作</li></ul></li><li>安装步骤 <a href="https://webcache.googleusercontent.com/search?q=cache:63AJZgZ4YK4J:https://ciweigg2.github.io/2019/06/01/kubernetes-1.15.0-ji-qun-an-zhuang-he-dashbaord-mian-ban/+&amp;cd=10&amp;hl=zh-CN&amp;ct=clnk&amp;gl=hk" target="_blank" rel="noopener">^1</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### (所有节点)环境配置</span></span><br><span class="line"><span class="comment"># 更新软件版本和内核次版本。初始化机器可执行，生产环境不建议重复更新内核版本，生产环境可使用 `yum upgrade`</span></span><br><span class="line">yum update -y</span><br><span class="line"></span><br><span class="line"><span class="comment">## A.关闭防火墙等</span></span><br><span class="line">systemctl stop firewalld &amp;&amp; systemctl <span class="built_in">disable</span> firewalld &amp;&amp; setenforce 0</span><br><span class="line">sed -i <span class="string">'s/SELINUX=enforcing/SELINUX=disabled/g'</span> /etc/selinux/config</span><br><span class="line"><span class="comment"># 开启bridge转发</span></span><br><span class="line">cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line"><span class="comment"># 关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动；亦可通过参数设置不关闭Swap。特别是已经运行了其他应用的服务器，可通过参数忽略Swap校验，此时则无需关闭</span></span><br><span class="line"><span class="comment"># vm.swappiness=0</span></span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 关闭交换分区需要执行。也可将/etc/fstab中swap的挂载注释掉</span></span><br><span class="line">swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</span><br><span class="line"><span class="comment"># 使生效</span></span><br><span class="line">modprobe br_netfilter <span class="comment"># 加载内核br_netfilter模块。注意：建议设置开机自启动，参考[启动设置](/_posts/linux/linux.md#启动设置)</span></span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## B.配置hostname(需要保证唯一)。新加入的节点没有配置hostname也可运行</span></span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hostname node1 <span class="comment"># 可考虑取名成 k8s-main-master-1 等</span></span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hotname node2</span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hostname node3</span><br><span class="line">cat &gt;&gt; /etc/hosts &lt;&lt;EOF</span><br><span class="line">192.168.6.131 node1</span><br><span class="line">192.168.6.132 node2</span><br><span class="line">192.168.6.133 node3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">## C.(可选配置)关于ipvs：如果以下前提条件不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式</span></span><br><span class="line"><span class="comment"># 由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块。以下文件保证在节点重启后能自动加载所需模块</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br><span class="line">yum install -y ipset <span class="comment"># 需要确保各个节点上已经安装了ipset软件</span></span><br><span class="line"><span class="comment"># yum install ipvsadm # 便于查看ipvs的代理规则</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## D.安装Docker(下列1-3步骤网上部分案例未执行)</span></span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo</span><br><span class="line">yum list docker-ce.x86_64 --showduplicates | sort -r <span class="comment"># 查看docker版本</span></span><br><span class="line">yum makecache fast <span class="comment"># 更新缓存</span></span><br><span class="line">yum install -y --<span class="built_in">setopt</span>=obsoletes=0 docker-ce-18.09.7-3.el7 <span class="comment"># 安装docker</span></span><br><span class="line">systemctl start docker &amp;&amp; systemctl <span class="built_in">enable</span> docker</span><br><span class="line"><span class="comment"># 1.确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT</span></span><br><span class="line">iptables -nvL | grep <span class="string">'Chain FORWARD'</span></span><br><span class="line"><span class="comment"># 2.如果registry为http可修改`vi /etc/docker/daemon.json`，并且提前进行harbor认证</span></span><br><span class="line"><span class="comment"># cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span></span><br><span class="line"><span class="comment"># &#123;"insecure-registries": ["192.168.6.131:10000"]&#125;</span></span><br><span class="line"><span class="comment"># EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 使用kubeadm部署Kubernetes</span></span><br><span class="line"><span class="comment">## E.(所有节点安装)添加kubernetes yum源</span></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 刷新缓存</span></span><br><span class="line">yum makecache fast</span><br><span class="line"><span class="comment"># 安装kubelet(运行Pod)、kubeadm(初始化节点)、kubectl(操作集群/API Server入口)。此时，kubelet也需要安装在Master机器上，因为API Server等也是基于Pod运行的(相当于自己运行自己)；kubectl一般安装在Master节点，都安装也行</span></span><br><span class="line">yum install -y kubeadm-1.15.0 kubelet-1.15.0 kubectl-1.15.0</span><br><span class="line"><span class="comment"># (可选配置)如果不希望禁用swap</span></span><br><span class="line">cat &gt; /etc/sysconfig/kubelet &lt;&lt;EOF</span><br><span class="line">KUBELET_EXTRA_ARGS=--fail-swap-on=<span class="literal">false</span></span><br><span class="line">DAEMON_ARGS=--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 注意：kubelet无需手动启动，在kubeadm init初始化时会自动启动</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">## F.(**仅Master节点执行**，此处的node1机器需要执行) 启动(初始化)一个 Kubernetes主节点。如果不希望禁用swap，则需要加上`--ignore-preflight-errors=swap`；也可基于config.yml进行初始化，但是测试失败</span></span><br><span class="line"><span class="comment"># 安装成功显示`Your Kubernetes control-plane has initialized successfully!`，具体日志见下文`kubeadm init执行成功日志`；安装失败见常见错误处理。其中`10.244.0.0/16`为pod的网络，启动pod后会在Node上产生一个`cni0`的网桥</span></span><br><span class="line">kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=Swap --image-repository=registry.aliyuncs.com/google_containers</span><br><span class="line"><span class="comment"># 配置常规用户如何使用kubectl访问集群(使用非root用户操作，root用户也可如此操作)，即需要使用kubectl命令的机器进行配置(一般在Master上操作)</span></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"><span class="comment"># 查看一下集群状态，确认个组件都处于healthy状态</span></span><br><span class="line">kubectl get cs</span><br><span class="line">kubectl get node <span class="comment"># 此时只有一个主节点，状态为NotReady(由于还没有部署网络插件)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## G.(仅Master节点执行)安装网络插件(Pod Network插件flannel/canal。此处使用canal，canal内部会安装flannel镜像)。</span></span><br><span class="line">mkdir -p ~/k8s/ &amp;&amp; <span class="built_in">cd</span> ~/k8s</span><br><span class="line"><span class="comment">#wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span></span><br><span class="line"><span class="comment">#kubectl apply -f kube-flannel.yml # 需要注意保证docker register私有仓库中已经有flannel镜像</span></span><br><span class="line">wget https://docs.projectcalico.org/v3.8/manifests/canal.yaml</span><br><span class="line">kubectl apply -f canal.yaml</span><br><span class="line"><span class="comment"># 查看集群状态(稍等一会全部进入Running状态)</span></span><br><span class="line">kubectl get pods -o wide --all-namespaces <span class="comment"># 其中kube-flannel-xxx/canal-xxx为Running状态</span></span><br><span class="line">kubectl get node <span class="comment"># 此时节点状态为Ready</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## I.（仅所有Node节点执行）向Kubernetes集群中添加Node节点</span></span><br><span class="line"><span class="comment"># 在Master节点打印获取加入集群的命令</span></span><br><span class="line">kubeadm token create --<span class="built_in">print</span>-join-command</span><br><span class="line"><span class="comment"># Node节点运行加入集群命令，注意后面新加了swap参数。安装成功显示`This node has joined the cluster`</span></span><br><span class="line">kubeadm join 192.168.6.131:6443 --token 3v4bja.hw4mwq5uknl3ruqn --discovery-token-ca-cert-hash sha256:3f315f28918e58cb5cdb1c4fbf47db8d1d3ab6169146079a7f8f60197ae17c12 --ignore-preflight-errors=Swap</span><br><span class="line"><span class="comment"># (在Master节点)查看节点是否成功加入</span></span><br><span class="line">kubectl get node -o wide <span class="comment"># ROLES显示`&lt;none&gt;`为正常，如果STATUS显示`NotReady`则表示节点还没有加入到集群</span></span><br><span class="line"><span class="comment"># (在Master节点)查看Pod运行情况，此时可以发现 flannel 对应的Pod在node1-node3都运行了。</span></span><br><span class="line"><span class="comment"># 至此，集群正常运行，安装完毕</span></span><br><span class="line">kubectl get pods -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">### (可选配置)Kube-proxy开启ipvs</span></span><br><span class="line"><span class="comment">## J.(**Master节点执行**)Kube-proxy开启ipvs</span></span><br><span class="line"><span class="comment"># 此命令即可打开配置文件，修改文件中config.conf的mode配置为 `mode: "ipvs"`</span></span><br><span class="line">kubectl edit cm kube-proxy -n kube-system <span class="comment"># 或者修改`/etc/sysconfig/kubelet`加入`KUBE_PROXY_MODE=ipvs`</span></span><br><span class="line"><span class="comment"># 打印 kube-proxy</span></span><br><span class="line">kubectl get pods -n kube-system | grep kube-proxy</span><br><span class="line"><span class="comment"># 下列命令可重启各个节点上的kube-proxy pod。可能会卡死，可适当终止进程查看是否有变化</span></span><br><span class="line">kubectl get pods -n kube-system | grep kube-proxy | awk <span class="string">'&#123;system("kubectl delete pod "$1" -n kube-system")&#125;'</span></span><br><span class="line"><span class="comment"># 再次打印 kube-proxy(编号会变化)</span></span><br><span class="line">kubectl get pods -n kube-system | grep kube-proxy</span><br><span class="line"><span class="comment"># 选择其中某个 kube-proxy 执行下列命令，提示`Using ipvs Proxier`表示ipvs模式已经开启(默认是`Using iptables Proxier`)</span></span><br><span class="line">kubectl logs kube-proxy-xxxxx -n kube-system</span><br></pre></td></tr></table></figure><ul><li>kubeadm init执行成功日志</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=swap --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --ignore-preflight-errors=SystemVerification</span></span><br><span class="line">[init] Using Kubernetes version: v1.15.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"etcd/ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node1 localhost] and IPs [192.168.6.131 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node1 localhost] and IPs [192.168.6.131 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.6.131]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"sa"</span> key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"kubelet.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[etcd] Creating static Pod manifest <span class="keyword">for</span> <span class="built_in">local</span> etcd <span class="keyword">in</span> <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[<span class="built_in">wait</span>-control-plane] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="string">"/etc/kubernetes/manifests"</span>. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 17.502337 seconds</span><br><span class="line">[upload-config] Storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap <span class="string">"kubelet-config-1.15"</span> <span class="keyword">in</span> namespace kube-system with the configuration <span class="keyword">for</span> the kubelets <span class="keyword">in</span> the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node node1 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: rxqii4.ov3v99x5bk2qi4ia</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstrap-token] Creating the <span class="string">"cluster-info"</span> ConfigMap <span class="keyword">in</span> the <span class="string">"kube-public"</span> namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.6.131:6443 --token rxqii4.ov3v99x5bk2qi4ia \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:7a9a8a910ae2cad21a032afd289a00097ebfbc6d361fd9673644db0e264a4fd1</span><br></pre></td></tr></table></figure><ul><li><strong>常用扩展安装</strong>：<code>Helm</code>、<code>Ingress Control</code>、<code>Dashboard</code>、<code>metrics-server</code> 可手动安装或通过Helm安装</li><li>集群初始化(kubeadm init/kubeadm join)如果遇到问题，可以使用下面的命令进行清理</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">rm -rf /var/lib/etcd </span><br><span class="line">rm -rf /var/lib/cni/</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br></pre></td></tr></table></figure><ul><li><strong>从集群中移除Node</strong>(以移除node3为例)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.在master节点上执行</span></span><br><span class="line">kubectl drain node3 --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node node3</span><br><span class="line"><span class="comment"># 2.在node3上执行上述清理命令</span></span><br></pre></td></tr></table></figure><h3 id="kubelet-说明"><a href="#kubelet-说明" class="headerlink" title="kubelet 说明"></a>kubelet 说明</h3><ul><li><code>systemctl status kubelet</code> 查看服务状态。可看到kubelet启动命令参数配置文件位于<code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code></li><li><code>cat 10-kubeadm.conf</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"</span></span><br><span class="line"><span class="comment"># This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span></span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line"><span class="comment"># This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use</span></span><br><span class="line"><span class="comment"># the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span></span><br><span class="line">EnvironmentFile=-/etc/sysconfig/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet <span class="variable">$KUBELET_KUBECONFIG_ARGS</span> <span class="variable">$KUBELET_CONFIG_ARGS</span> <span class="variable">$KUBELET_KUBEADM_ARGS</span> <span class="variable">$KUBELET_EXTRA_ARGS</span></span><br></pre></td></tr></table></figure><ul><li>参数说明(<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</a>)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--bootstrap-kubeconfig                  <span class="comment"># 如果--kubeconfig未定义则以此参数为准</span></span><br><span class="line">--kubeconfig                            <span class="comment"># 集群contexts和users配置(kind: Config)</span></span><br><span class="line">--config=/var/lib/kubelet/config.yaml   <span class="comment"># kubelet配置(kind: KubeletConfiguration)。像 --eviction-hard 等参数也可在--config中指定</span></span><br><span class="line">--eviction-hard=imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% <span class="comment"># Node资源小于对应阀值则驱逐pod。对应--config文件中evictionHard属性</span></span><br><span class="line">--system-reserved                       <span class="comment"># Node需要保留的资源值(剩余的资源可交由kubelet调度)。当剩余资源不足时不会将pod调度到此节点，历史调度上去的pod不会因为配置修改而被驱逐(驱逐参考--eviction-hard)</span></span><br></pre></td></tr></table></figure><ul><li>修改配置文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在此配置文件中添加 `KUBELET_EXTRA_ARGS` 变量即可。如 `KUBELET_EXTRA_ARGS=--fail-swap-on=false`</span></span><br><span class="line">vi /etc/sysconfig/kubelet</span><br><span class="line"><span class="comment"># 或者修改 --config 中可配置的参数</span></span><br><span class="line">vi /var/lib/kubelet/config.yaml</span><br><span class="line"><span class="comment"># 重新加载</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure><ul><li>配置示例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /var/lib/kubelet/config.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># Node需要保留的资源值(剩余的资源可交由kubelet调度)。当剩余资源不足时不会将pod调度到此节点，历史调度上去的pod不会因为配置修改而被驱逐(驱逐参考--eviction-hard)。可在Dashboard-Node-限制值中显示</span></span><br><span class="line">systemReserved:</span><br><span class="line">  cpu: 1000m</span><br><span class="line">  memory: 1024Mi</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><ul><li>镜像：k8s-rpm源和docker镜像的k8s仓库(image-repository)都需要使用国内镜像地址</li><li><p>kubelet启动报错，<code>journalctl -xe</code>查看日志如下(<code>sudo journalctl -u kubelet -f -n 100</code>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Failed to create [<span class="string">"kubepods"</span>] cgroup</span><br><span class="line">Failed to start ContainerManager Cannot <span class="built_in">set</span> property TasksAccounting, or unknown property</span><br></pre></td></tr></table></figure><ul><li>解决方法：先执行<code>yum update</code>(<a href="https://github.com/kubernetes/kubernetes/issues/76820" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/76820</a>)</li></ul></li><li>kubelet启动报错<code>failed to load Kubelet config file /var/lib/kubelet/config.yaml</code>，此时是重置kubeadm导致<ul><li>解决方法：<code>systemctl stop kubelet &amp;&amp; systemctl enable kubelet</code>，然后重新kubeadm init(会自动启动kubelet)</li></ul></li><li>kubeadm reset报错<code>[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty</code><ul><li>解决办法：手动删除/var/lib/etcd目录</li></ul></li><li>kubeadm join执行报错<code>kubeadm join [ERROR DirAvailable--etc-kubernetes-manifests]: /etc/kubernetes/manifests is not empty</code><ul><li>情况一：在master节点(已经执行了kubeadm init)上执行kubeadm join会出现。kubeadm应该在node节点上执行(如果在master上执行也不会对master产生影响)</li><li>情况二：在node节点上执行出现，可能是之前已经初始化过此node节点。如果需要重新初始化，需要先执行<code>kubeadm reset</code></li></ul></li></ul><h2 id="命令使用"><a href="#命令使用" class="headerlink" title="命令使用"></a>命令使用</h2><blockquote><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/</a></p></blockquote><h3 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h3><ul><li>命令说明</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl</span></span><br><span class="line"><span class="comment"># kubectl set --help</span></span><br><span class="line"><span class="comment"># kubectl set image --help</span></span><br><span class="line">kubectl controls the Kubernetes cluster manager.</span><br><span class="line"></span><br><span class="line"> Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基础命令</span></span><br><span class="line">Basic Commands (Beginner):</span><br><span class="line">  create         Create a resource from a file or from stdin.</span><br><span class="line">    configmap           <span class="comment"># 创建ConfigMap资源。语法：kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run] [options]</span></span><br><span class="line">    secret              <span class="comment"># 创建Secret资源</span></span><br><span class="line">      docker-registry       <span class="comment"># 创建docker-registry类型Secret资源</span></span><br><span class="line">      generic               <span class="comment"># 普通Secret资源(加密方式为base64)。语法：kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run] [options]</span></span><br><span class="line">      tsl                   <span class="comment"># TSL秘钥、证书Secret资源</span></span><br><span class="line">      <span class="comment"># kubectl create secret generic my-secret --from-literal=key1=my_val1 --from-literal=key2=my_val2</span></span><br><span class="line">      <span class="comment"># kubectl create secret docker-registry harbor-secret --docker-server=192.168.17.196:5000 --docker-username=smalle --docker-password=Hello666</span></span><br><span class="line">    serviceaccount      <span class="comment"># 创建serviceaccount用户(用于pod访问API Server)</span></span><br><span class="line">    role                <span class="comment"># 角色(namespace)</span></span><br><span class="line">    clusterrole         <span class="comment"># 集群角色</span></span><br><span class="line">    rolebinding         <span class="comment"># 角色绑定</span></span><br><span class="line">    namespace           <span class="comment"># 命名空间</span></span><br><span class="line">    -f                  <span class="comment"># 根据yaml文件创建资源</span></span><br><span class="line">    --dry-run           <span class="comment"># 仅仅运行测试，不进行实际操作。可通过此生成创建模板</span></span><br><span class="line">    -o                  <span class="comment"># 取值：yaml</span></span><br><span class="line">    <span class="comment"># kubectl create -f sq-pod.yaml</span></span><br><span class="line">    <span class="comment"># kubectl create role my-pods-reader --verb=get,list,watch --resource=pods --dry-run -o yaml # 干跑模式生成创建role配置文件，不会进行实际操作</span></span><br><span class="line">    <span class="comment"># kubectl create rolebinding aezo-read-pods --role=my-pods-reader --user=aezo --dry-run -o yaml &gt; rolebinding-demo.yaml</span></span><br><span class="line">  expose         Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service <span class="comment"># 暴露Pod/RS/RS为一个服务</span></span><br><span class="line">    --name              <span class="comment"># 暴露的服务名称</span></span><br><span class="line">    --port              <span class="comment"># 暴露的服务端口</span></span><br><span class="line">    --target-port       <span class="comment"># 被暴露pod的端口</span></span><br><span class="line">    --protocol          <span class="comment"># 协议(TCP)</span></span><br><span class="line">    --<span class="built_in">type</span>              <span class="comment"># 暴露的类型。ClusterIP(默认，仅在进群访问)、NodePort(提供外网访问，随机生成Node端口)、LoadBalancer、ExternalName</span></span><br><span class="line">    <span class="comment"># kubectl expose deployment sq-nginx --name=nginx --port=80 --target-port=80 --protocol=TCP # 将名为sq-nginx的pod暴露成服务</span></span><br><span class="line">  run            Run a particular image on the cluster <span class="comment"># 基于某个镜像创建Pod</span></span><br><span class="line">    --image         <span class="comment"># 指定容器的镜像，和docker镜像一致</span></span><br><span class="line">    --replicas      <span class="comment"># 部署的节点数量</span></span><br><span class="line">    --restart       <span class="comment"># 重启方式，取值：Never(表示不自动启动)</span></span><br><span class="line">    -it             <span class="comment"># 进入pod容器</span></span><br><span class="line">    --rm            <span class="comment"># 退出容器后删除</span></span><br><span class="line">    <span class="comment"># kubectl run nginx --image=nginx # 启动单实例</span></span><br><span class="line">    <span class="comment"># kubectl run sq-nginx --image=nginx:1.14-alpine --replicas=3 # 启动3个实例</span></span><br><span class="line">    <span class="comment"># kubectl run busybox1 -it --image=busybox --restart=Never --overrides='&#123; "apiVersion": "v1", "spec": &#123; "nodeName": "node1" &#125; &#125;' # 启动busybox并进入容器，此时添加了额外参数 --overrides 来覆盖资源配置</span></span><br><span class="line">  <span class="built_in">set</span>            Set specific features on objects <span class="comment"># 重设对象配置</span></span><br><span class="line">    image       <span class="comment"># 更新容器镜像</span></span><br><span class="line">    <span class="comment"># kubectl set image deployment sq-nginx sq-nginx=nginx:1.14-alpine # 更新sq-nginx部署资源中容器sq-nginx的镜像</span></span><br><span class="line"></span><br><span class="line">Basic Commands (Intermediate):</span><br><span class="line">  explain        Documentation of resources <span class="comment"># 资源描述文档</span></span><br><span class="line">    pods        <span class="comment"># 显示描述pods资源的字段说明文档</span></span><br><span class="line">    <span class="comment"># kubectl explain pods.metadata # 显示pods资源的matedata字段说明(可一直通过.字符进行描述子字段)</span></span><br><span class="line">  get            Display one or many resources <span class="comment"># 展示资源列表</span></span><br><span class="line">    all             <span class="comment"># **获取所有资源**</span></span><br><span class="line">    deployment      <span class="comment"># 获取部署列表。READY：1/3表示期望部署3个副本，目前只有1副本就绪</span></span><br><span class="line">        -w                  <span class="comment"># 一致观测部署变化(pods等也可使用)</span></span><br><span class="line">    pods/pod/po     <span class="comment"># 获取pod列表。READY：1/3表示此Pod期望部署3个容器，目前只有1个容器就绪</span></span><br><span class="line">        -o                  <span class="comment"># 取值：wide(显示详细信息)、yaml(显示yaml配置信息，可获取资源完整信息，如pod uid)、json、jsonpath(如 jsonpath=&#123;.data.token&#125; 仅显示此字段数据)</span></span><br><span class="line">        -l                  <span class="comment"># 基于标签过滤，如：-l app,tier(获取同时有此标签key的pods)；-l run!=busybox1；</span></span><br><span class="line">        -n                  <span class="comment"># 指定命名空间namespace，默认为default，如 `-n kube-system`</span></span><br><span class="line">        --show-labels       <span class="comment"># 展示标签</span></span><br><span class="line">        --all-namespaces    <span class="comment"># 显示所有命名空间下的资源</span></span><br><span class="line">    services/svc    <span class="comment"># 获取服务列表。PORTS：80:30435/TCP 表示80为所有Pod网络端口，30435为Node网络端口(只有部署有此pod的所有Node都是这个端口)</span></span><br><span class="line">    configmap/cm    <span class="comment"># ConfigMap资源</span></span><br><span class="line">    event/ev        <span class="comment"># 事件</span></span><br><span class="line">    <span class="comment"># kubectl get pods # 获取pods列表</span></span><br><span class="line">    <span class="comment"># kubectl get pods -o wide # 获取pods详细列表</span></span><br><span class="line">    <span class="comment"># kubectl get --raw /apis/apps/v1 # 获取 /apps/v1 可用资源类型</span></span><br><span class="line">  edit           Edit a resource on the server <span class="comment"># 编辑一个资源配置（保存配置后立即生效）</span></span><br><span class="line">    svc         <span class="comment"># 编辑服务配置</span></span><br><span class="line">    <span class="comment"># kubectl edit svc nginx # 编辑nginx服务配置</span></span><br><span class="line">  delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector <span class="comment"># 删除资源</span></span><br><span class="line">    pods        <span class="comment"># 删除pods。速度会较慢</span></span><br><span class="line">    svc         <span class="comment"># 删除服务</span></span><br><span class="line">    --force --grace-period=0 <span class="comment"># 删除资源状态一直是Terminating，可加以上参数</span></span><br><span class="line">    <span class="comment"># kubectl delete pods sq-nginx-75875cf46f-829nm # 删除某个pod</span></span><br><span class="line">    <span class="comment"># kubectl delete -f sq-pod.yaml # 基于配置文件删除资源</span></span><br><span class="line">    <span class="comment"># kubectl get pods -n devops | grep Terminating | awk '&#123;print $1&#125;' | xargs kubectl delete pods -n devops --force --grace-period=0 # ***批量强制删除Terminating状态的pods***</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署相关</span></span><br><span class="line">Deploy Commands:</span><br><span class="line">  rollout        Manage the rollout of a resource <span class="comment"># 滚动执行</span></span><br><span class="line">    status      <span class="comment"># 滚动显示某资源状态</span></span><br><span class="line">    undo        <span class="comment"># 默认撤销上一次资源操作</span></span><br><span class="line">        --to-revision       <span class="comment"># 设置回滚资源到某个历史版本</span></span><br><span class="line">    <span class="built_in">history</span>     <span class="comment"># 滚动显示资源历史</span></span><br><span class="line">    pause       <span class="comment"># 暂停</span></span><br><span class="line">    <span class="comment"># kubectl rollout status depolyment sq-nginx # 滚动显示sq-nginx的部署状态</span></span><br><span class="line">    <span class="comment"># kubectl rollout history depolyment sq-nginx # 查看部署版本状态</span></span><br><span class="line">    <span class="comment"># kubectl set image deployment sq-nginx sq-nginx=nginx:1.14-alpine &amp;&amp; kubectl rollout pause deployment sq-nginx</span></span><br><span class="line">  scale          Set a new size <span class="keyword">for</span> a Deployment, ReplicaSet, Replication Controller, or Job <span class="comment"># 重新设置资源数量(伸缩扩展)</span></span><br><span class="line">    <span class="comment"># kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) [options]</span></span><br><span class="line">    <span class="comment"># kubectl scale --replicas=2 deployment sq-nginx</span></span><br><span class="line">  autoscale      Auto-scale a Deployment, ReplicaSet, or ReplicationController</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群管理相关</span></span><br><span class="line">Cluster Management Commands:</span><br><span class="line">  certificate    Modify certificate resources.</span><br><span class="line">  cluster-info   Display cluster info <span class="comment"># 打印集群信息</span></span><br><span class="line">  top            Display Resource (CPU/Memory/Storage) usage.</span><br><span class="line">   <span class="comment"># kubectl top pods/nodes # 查看监控指标信息，必须启动 metrics-server 才能正常获取</span></span><br><span class="line">  cordon         Mark node as unschedulable <span class="comment"># 设置节点未不可调度</span></span><br><span class="line">    <span class="comment"># kubectl cordon node2</span></span><br><span class="line">  uncordon       Mark node as schedulable <span class="comment"># 设置节点未可调度</span></span><br><span class="line">    <span class="comment"># kubectl uncordon node2</span></span><br><span class="line">  drain          Drain node <span class="keyword">in</span> preparation <span class="keyword">for</span> maintenance <span class="comment"># 移除相应节点。此时k8s会将相应节点上的旧Pod删除，并在可调度节点上面起一个对应的Pod。当旧Pod没有被正常删除的情况下(如旧Pod一直处于Terminating状态)，新Pod不会起来</span></span><br><span class="line">  taint          Update the taints on one or more nodes <span class="comment"># 给节点增加污点。如Master默认就存在污点，其他Pod默认不能容忍此污点，因此普通Pod不会运行在Master节点上</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Debug命令</span></span><br><span class="line">Troubleshooting and Debugging Commands:</span><br><span class="line">  describe       Show details of a specific resource or group of resources <span class="comment"># 描述某资源详细信息</span></span><br><span class="line">    node        <span class="comment"># 描述节点信息</span></span><br><span class="line">    deployment  <span class="comment"># 描述部署信息</span></span><br><span class="line">    svc         <span class="comment"># 描述服务信息(默认描述全部服务，后面可接某个服务名)</span></span><br><span class="line">    --<span class="built_in">export</span>    <span class="comment"># 导出关键配置信息(去除了一些status信息)</span></span><br><span class="line">    <span class="comment"># kubectl describe node node1 # 描述节点node1的详细信息</span></span><br><span class="line">  logs           Print the logs <span class="keyword">for</span> a container <span class="keyword">in</span> a pod <span class="comment"># **打印pod中容器的日志**</span></span><br><span class="line">    -f          <span class="comment"># 实时打印日志</span></span><br><span class="line">    --all-containers <span class="comment"># 查看pod下所有容器日志</span></span><br><span class="line">    -c          <span class="comment"># 查看pod下某个容器日志</span></span><br><span class="line">    --previous  <span class="comment"># ***查看不在运行的pod日志***</span></span><br><span class="line">    <span class="comment"># kubectl logs sq-pod sq-busybox # 打印 sq-pod 中 sq-busybox 容器的日志</span></span><br><span class="line">  attach         Attach to a running container</span><br><span class="line">  <span class="built_in">exec</span>           Execute a <span class="built_in">command</span> <span class="keyword">in</span> a container <span class="comment"># 在容器中执行命令</span></span><br><span class="line">    <span class="comment"># kubectl exec -it sq-pod -c sq-busybox -- sh # -it同docker表示进入容器</span></span><br><span class="line">  port-forward   Forward one or more <span class="built_in">local</span> ports to a pod <span class="comment"># 通过端口转发映射本地端口到指定的应用端口(proxy)</span></span><br><span class="line">    <span class="comment"># 一般是为了测试将集群中的某个服务的端口映射到节点的端口上，此时命令行会使命令行一直处于监听状态</span></span><br><span class="line">    <span class="comment"># 语法：kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N]。此处REMOTE_PORT指的是pod的端口，而不是service的端口</span></span><br><span class="line">    <span class="comment"># eg:</span></span><br><span class="line">        <span class="comment"># kubectl port-forward --address 0.0.0.0 sq-pod-8696c98b6f-j2stv 8080:80 1443:443 # 此时访问 http://192.168.6.131:8080/ 即可</span></span><br><span class="line">        <span class="comment"># kubectl port-forward --address 0.0.0.0 $(kubectl get pods --namespace default -l "app=wordpress,tier=mysql" -o jsonpath="&#123;.items[0].metadata.name&#125;") 13306:3306</span></span><br><span class="line">        <span class="comment"># kubectl get pods -n rook-ceph | grep csi-cephfsplugin | awk '&#123;print $1&#125;' | xargs kubectl port-forward --address 0.0.0.0 13306:3306</span></span><br><span class="line">  proxy          Run a proxy to the Kubernetes API server</span><br><span class="line">    <span class="comment"># kubectl proxy 8080 # 将 API server 暴露到一个8080端口上，则可查看api信息 `curl http://localhost:8080/`</span></span><br><span class="line">  cp             Copy files and directories to and from containers.</span><br><span class="line">  auth           Inspect authorization</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高级命令</span></span><br><span class="line">Advanced Commands:</span><br><span class="line">  diff           Diff live version against would-be applied version</span><br><span class="line">  apply          Apply a configuration to a resource by filename or stdin <span class="comment"># 基于文件创建/更新资源，建议将资源的原始yaml保留备份，以备快速删除此资源</span></span><br><span class="line">    <span class="comment"># kubectl apply -f sq-pod.yaml # 应用一个文件，当通过vi命令修改配置文件后需要手动使配置生效。备注：假设原pod只有一个容器，如果修改yaml文件中镜像配置，最终可能pod中老容器也不会去除，总共会运行2个容器</span></span><br><span class="line">  patch          Update field(s) of a resource using strategic merge patch</span><br><span class="line">    <span class="comment"># kubectl patch deployment sq-deploy -p '&#123;"spec":&#123;"replicas":5&#125;&#125;' # 给配置打补丁，配置会自动生效</span></span><br><span class="line">    <span class="comment"># kubectl patch svc my-dev-mysql -p '&#123;"spec":&#123;"type":"NodePort"&#125;&#125;' # 将服务修改成边界服务</span></span><br><span class="line">  replace        Replace a resource by filename or stdin</span><br><span class="line">  <span class="built_in">wait</span>           Experimental: Wait <span class="keyword">for</span> a specific condition on one or many resources.</span><br><span class="line">  convert        Convert config files between different API versions</span><br><span class="line">  kustomize      Build a kustomization target from a directory or a remote url.</span><br><span class="line"></span><br><span class="line">Settings Commands:</span><br><span class="line">  label          Update the labels on a resource <span class="comment"># 给资源(Pod、Node等)添加一个Label标签</span></span><br><span class="line">    <span class="comment"># kubectl label pods sq-pod version=v1 [--overwrite] # 给pod添加标签，加`--overwrite`则表示修改标签</span></span><br><span class="line">    <span class="comment"># kubectl label nodes &#123;node1,node2,node3&#125; aezo.cn/storage-node=enabled # 给node添加标签</span></span><br><span class="line">    <span class="comment"># kubectl label nodes node1 storage-node- # 删除标签</span></span><br><span class="line">  annotate       Update the annotations on a resource <span class="comment"># 给资源添加描述</span></span><br><span class="line">  completion     Output shell completion code <span class="keyword">for</span> the specified shell (bash or zsh)</span><br><span class="line"></span><br><span class="line">Other Commands:</span><br><span class="line">  api-resources  Print the supported API resources on the server</span><br><span class="line">  api-versions   Print the supported API versions on the server, <span class="keyword">in</span> the form of <span class="string">"group/version"</span></span><br><span class="line">  config         Modify kubeconfig files</span><br><span class="line">    view                <span class="comment"># 查看集群k8s集群配置</span></span><br><span class="line">    <span class="built_in">set</span>-credentials     <span class="comment"># 创建用户证书</span></span><br><span class="line">    <span class="built_in">set</span>-context         <span class="comment"># 设置用户可访问的集群</span></span><br><span class="line">    <span class="built_in">set</span>-cluster         <span class="comment"># 创建集群</span></span><br><span class="line">    --kubeconfig        <span class="comment"># 配置文件目录，默认为 `$&#123;HOME&#125;/.kube/config`</span></span><br><span class="line">  plugin         Provides utilities <span class="keyword">for</span> interacting with plugins.</span><br><span class="line">  version        Print the client and server version information <span class="comment"># 打印版本信息</span></span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kubectl [flags] [options]</span><br><span class="line"></span><br><span class="line">Use <span class="string">"kubectl &lt;command&gt; --help"</span> <span class="keyword">for</span> more information about a given <span class="built_in">command</span>.</span><br><span class="line">Use <span class="string">"kubectl options"</span> <span class="keyword">for</span> a list of global <span class="built_in">command</span>-line options (applies to all commands).</span><br></pre></td></tr></table></figure><ul><li>入门命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看run命令的帮助信息</span></span><br><span class="line">kubectl run --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动nginx的Pod</span></span><br><span class="line">kubectl run nginx --image=nginx <span class="comment"># 启动单实例</span></span><br><span class="line">kubectl run sq-nginx --image=nginx:1.14-alpine --replicas=3 <span class="comment"># 启动3个实例</span></span><br><span class="line">kubectl run busybox1 -it --image=busybox --restart=Never <span class="comment"># 启动busybox，并进入容器，--restart=Never表示不自动启动。**测试常用**</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取部署列表</span></span><br><span class="line">kubectl get deployment</span><br><span class="line"><span class="comment"># 获取pod详细列表，可显示pod的ip地址，此时可以在K8s上访问此ip</span></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除某pod</span></span><br><span class="line">kubectl delete pods sq-nginx-75875cf46f-829nm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露pod为服务。如进入某pod容器可测试直接通过服务名访问：`wget nginx` 或 `wget -O - -q http://nginx:80`</span></span><br><span class="line">kubectl expose deployment sq-nginx --name=nginx --port=80 --target-port=80 --protocol=TCP</span><br><span class="line"></span><br><span class="line">kubectl logs sq-pod sq-busybox <span class="comment"># 打印 sq-pod 中 sq-busybox 容器的日志</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it sq-pod -c sq-busybox -- /bin/sh <span class="comment"># 执行容器中命令，-it同docker表示进入容器</span></span><br></pre></td></tr></table></figure><ul><li>常用命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取某 pod 的 uid(ID,唯一标识,podId)</span></span><br><span class="line">kubectl get pods cm-acme-http-solver-9vxsd -o go-template --template=<span class="string">'&#123;&#123;.metadata.uid&#125;&#125;&#123;&#123;"\n"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ul><li>资源(对象)<ul><li>工作负载(workload)：Pod、RelicaSet(rs)、Deployment(deploy)、StatefulSet、DaemonSet、Job、Cronjob</li><li>服务发现及负载均衡：Service(svc)、Ingress(ing)</li><li>配置与存储：Volume、CSI<ul><li>PersistentVolume(pv)、PersistentVolumeClaim(pvc)、ConfigMap(cm)、Secret</li><li>StorageClass(sc)</li><li>DownwardAPI</li></ul></li><li>集群级资源：Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding、ServiceAccount(sa)、NetworkPolicy(netpol)、APIService</li><li>元数据型资源：HPA、PodTemplate、LimitRange</li><li>CustomResourceDefinition(crd)<ul><li>是 v1.7 + 新增的无需改变代码就可以扩展 Kubernetes API 的机制，用来管理自定义对象(新资源类型)。它实际上是 ThirdPartyResources(TPR) 的升级版本，而 TPR 已经在 v1.8 中删除</li></ul></li><li>Event(ev)</li></ul></li><li>创建资源的方法<ul><li>apiserver仅接受json格式的资源定义</li><li>yaml格式提供的配置清单，apiserver可自动将其转为json格式，而后再提交<ul><li>k8s组件相关yaml配置文件位置<code>/etc/kubernetes/manifests</code>。如需要修改kube-apiserver启动参数，可先修改此配置文件后重新创建kube-apiserver对应pod</li></ul></li></ul></li><li>资源配置文件查看命令举例<ul><li><code>kubectl explain pods</code> 查看说明</li><li><code>kubectl explain pods.metadata</code> 查看某个字段说明(可一直通过.字符进行描述)</li><li><code>kubectl get pods nginx-deploy-75875cf46f-kbjck -o yaml</code> 查看使用案例</li></ul></li><li><code>ServiceAccount</code>(sa) k8s包括用户账号和服务账号，此时服务账号是附加某个pod上供其访问apiserver<ul><li>当创建 pod 的时候，如果没有指定一个 service account，系统会自动在与该 pod 相同的 namespace 下为其指派一个default service account。而pod和apiserver之间进行通信的账号，称为serviceAccountName</li><li>每一个命名空间下都有一个<code>default-token-xxxx</code>(查看<code>kubectl get secret</code>)用于该空间下pod的默认secret</li><li>每创建一个ServiceAccount就会自动创建一个Secret与之关联。如<code>kubectl create serviceaccount sa-admin</code>会产生一个<code>sa-admin-token-xxxx</code>的Secret(如此Secret中保存的token信息可用于登录Dashboard)</li><li>如将拉取容器镜像的Secret附加到ServiceAccount(sa)上，然后将sa定义到此容器上即可访问私有镜像(也可直接将Secret附加在容器上)</li></ul></li></ul><h4 id="资源配置文件字段说明"><a href="#资源配置文件字段说明" class="headerlink" title="资源配置文件字段说明"></a>资源配置文件字段说明</h4><ul><li><code>apiVersion</code> 查看支持API资源列表<code>kubectl api-versions</code>。查看某个一个资源对应API版本<code>kubectl explain pods</code>中的VERSION字段：创建Pod/Service使用v1，创建RS用apps/v1</li><li><code>kind</code> 资源类别：Pod、ReplicaSet、Deployment、DaemonSet等</li><li><code>metadata</code> 元数据<ul><li><code>name</code> 同以类别下名称需要唯一</li><li><code>namespace</code> 命名空间。基于文件apply时，会将资源创建到此处定义的命名空间中；如果不定义可以通过<code>--namespace=dev</code>传入；如果定义了，则–namespace参数无法对其进行覆盖</li><li><code>labels</code> 标签(限制长度)<ul><li>常见的标签键名：app、tier(frontend/backend)、version、profile、env</li></ul></li><li><code>annotations</code> 资源注解(不限长度)，与lables不同的是不能用于挑选资源对象。一般是提供一些配置表示，如<code>nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</code></li><li><code>selfLink</code> 每个资源引用PATH格式：<code>/apo/GROUP/VERSION/namespaces/NAMESPACE/TYPE/NAME</code></li></ul></li><li><code>spec</code> 期望状态(disired state)<ul><li><code>containers</code> 描述容器(&lt;[]Object&gt;)<ul><li><code>name</code> 容器名</li><li><code>image</code> 容器镜像地址。如：quay.io/coreos/kube-rbac-proxy:v0.4.1、prom/node-exporter(此时省略host，则docker默认host)</li><li><code>imagePullPolicy</code> Always(永远重新拉取镜像，镜像latest默认)、Never、IfNotPresent(如果本地有则不拉取镜像，其他默认)。创建Pod后无法修改此字段</li><li><code>ports</code>(&lt;[]Object&gt;)<ul><li><code>containerPort</code> 将此容器中的某个端口暴露到pod中</li><li><code>name</code> 如：http/https/myhttp</li><li><code>protocol</code> 如：TCP</li></ul></li><li><code>command</code> 对应ENTRYPOINT，可类似docker-compose使用<code>[]</code><ul><li>command/args不能强依赖于<code>lifecycle.postStart</code>的执行结果。此处command是在lifecycle.postStart之前执行的</li></ul></li><li><code>args</code> 对应CMD(<code>&lt;[]Object&gt;</code>)。<a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell" target="_blank" rel="noopener">与command对应关系</a></li><li><code>env</code> 环境变量信息(<code>&lt;[]Object&gt;</code>)<ul><li><code>name</code> 变量名</li><li><code>value</code> 变量值(建议用双引号包裹)</li><li><code>valueFrom</code> 从其他地方获取环境变量。第一次创建然容器时读取了数据后就不会再同步新数据，如果需要同步；可以挂载ConfigMap/Secret存储卷到pod上<ul><li><code>configMapKeyRef</code> 从ConfigMap中获取环境变量<ul><li><code>name</code> ConfigMap资源名称</li><li><code>key</code> 变量名</li></ul></li><li><code>secretKeyRef</code> 从SecretKey中获取(类似ConfigMap)</li><li><code>fieldRef</code> <a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/" target="_blank" rel="noopener">从该pod的yaml信息中读取信息</a><ul><li><code>fieldPath</code> 如：spec.nodeName、metadata.namespace、status.podIP</li></ul></li><li><code>resourceFieldRef</code> 从资源占用中获取信息<ul><li><code>containerName</code></li><li><code>resource</code> 如：limits.cpu、requests.memory</li></ul></li></ul></li></ul></li><li><code>volumeMounts</code> 挂载目录(<code>&lt;[]Object&gt;</code>)<ul><li><code>name</code> 外部存储空间名称，需要和volumes属性配合使用(一个容器可以在同一存储空间上挂载多个目录，可基于subPath)，如：node1-nfs存在空间的根目录/data</li><li><code>mountPath</code> 容器需要挂载到外部的路径，如：/usr/share/nginx/html/</li><li><code>subPath</code> 在存储空间创建子目录来映射容器的路径，需要相对路径。如：此时为www/，则表示将/usr/share/nginx/html/映射到/data/www/目录</li></ul></li><li><code>securityContext</code> <a href="https://www.kubernetes.org.cn/security-context-psp" target="_blank" rel="noopener">参考</a>。此时仅影响容器级别(Container-level Security Context仅应用到指定的容器上，并且不会影响Volume)<ul><li><code>privileged</code> true(设置容器运行在特权模式)</li><li><code>runAsUser</code> 启动容器用户。0代表root用户</li></ul></li><li><code>serviceAccountName</code> pod内部访问其他资源的账号名称</li><li><code>resources</code> 资源配置<ul><li><code>requests</code> 资源请求</li><li><code>limits</code> 资源限制</li></ul></li><li><code>livenessProbe</code> 存活性探测(如果多次存活探测失败，则会重启此pod)<ul><li><code>exec</code> 基于执行命令探测<ul><li><code>command</code> 执行的探测命令</li></ul></li><li><code>tcpSocket</code> 基于tcpSocket探测</li><li><code>httpGet</code> 基于httpGet探测<ul><li><code>scheme</code> 连接使用的 schema，默认HTTP</li><li><code>host</code> 连接的主机名，默认连接到 pod 的 IP</li><li><code>port</code></li><li><code>path</code> eg: /index.html</li><li><code>httpHeaders</code> 自定义请求的 header</li></ul></li><li><code>initialDelaySeconds</code> 初始化探测延时时间(修改Deployment有效，修改ReplicaSet无效)</li><li><code>periodSeconds</code> 探测周期(默认为10s)</li><li><code>timeoutSeconds</code> 探测超时时间。默认1秒，最小1秒</li><li><code>successThreshold</code> 探测失败后，最少连续探测成功多少次才被认定为成功。默认是 1，对于 liveness 必须是 1。最小值是 1。</li><li><code>failureThreshold</code> 探测成功后，最少连续探测失败多少次才被认定为失败。默认是 3。最小值是 1</li></ul></li><li><code>readinessProbe</code> 就绪性探测(子标签类似livenessProbe)。在readiness探测失败之后，Pod和容器并不会被删除，而是会被标记成特殊状态，进入这个状态之后，如果这个Pod是在某个serice的endpoint列表里面的话，则会被从这个列表里面清除，以保证外部请求不会被转发到这个Pod上；等Pod恢复成正常状态，则会被加回到endpoint的列表里面，继续对外服务</li></ul></li><li><code>securityContext</code> 影响整个pod级别。参考上文spec.containers.securityContext</li><li><code>nodeSelector</code> 节点标签选择器，如果定义则pod只会运行在有此标签的节点上。如：<code>nodeSelector: kubernetes.io/hostname: node1</code></li><li><code>nodeName</code> 直接运行在此节点上</li><li><code>restartPolicy</code> 重启策略：Always(默认)、OnFailure、Never</li><li><code>lifecycle</code> 生命周期<ul><li><code>postStart</code> 主pod容器被创建后调用(子标签类似livenessProbe)</li><li><code>preStop</code> 主pod容器被退出前调用(子标签类似livenessProbe)</li></ul></li><li><code>hostIPC</code> pod共享节点的ipc namespace</li><li><code>hostNetwork</code> pod共享节点的network namespace(此时则无需暴露端口，一般用于DaemonSet中)</li><li><code>hostPID</code> pod共享节点的pid namespace</li><li><code>volumes</code> 存储设置，<a href="#存储卷">见下文</a></li><li><code>imagePullSecrets</code> 拉取镜像使用的Secret资源</li></ul><hr><ul><li><code>replicas</code> ReplicaSet 维持pod数量</li><li><code>selector</code> ReplicaSet 选择pod的选择器</li><li><code>template</code> ReplicaSet 创建pod的模板<ul><li><code>metadata</code> 类似kind=Pod的</li><li><code>spec</code> 类似kind=Pod的</li></ul></li><li><code>strategy</code> Deployment创建pod的策略(如更新pod配置时)<ul><li><code>type</code> 取值：Recreate、RollingUpdate(默认。滚动更新：在新Pod进入readiness就绪之前，仍然由旧Pod提供服务；当新Pod就绪后，则移除就Pod)</li><li><code>rollingUpdate</code><ul><li><code>maxSurge</code> 操作pod时，可控制的最大数量，如修改配置后可能创建新版本pod和依次删除历史pod同时进行。(DaemonSet无，如果等于0则类似Recreate)</li><li><code>maxUnavailable</code> 更新配置时，不可用的最大数量</li></ul></li></ul></li><li><code>updateStrategy</code> DaemonSet更新pod策略(类似strategy)</li></ul><hr><ul><li><code>selector</code></li><li><code>type</code> Service类型：ClusterIP(默认，k8s集群内访问)、NodePort(k8s集群外可访问)、LoadBalancer(在NodePort的基础上，基于负载均衡，将请求转发到NodeIP:NodePort)、ExternalName(将k8s外部服务映射到集群)</li><li><code>clusterIP</code> Service服务集群IP(ExternalName类型无需)。eg：不定义则自动生成类似10.66.66.66、None(无头服务)</li><li><code>ports</code> 服务端口(暴露pod的服务端口)<ul><li><code>port</code> 暴露的服务端口</li><li><code>targetPort</code> 被暴露的容器端口</li><li><code>nodePort</code> 仅type=NodePort/LoadBalancer时，使用Node的端口映射服务端口(确保Node端口可用)，不指定则随机。NodePort默认端口范围为30000~32768</li></ul></li><li><code>externalName</code> 仅用于type=ExternalName，取值应该是一个外部域名，CNAME记录。CNAME -&gt; FQDN</li><li><code>externalIPs</code> 可配合<code>IPVS</code>实现将外部流量引入到集群内部，同时实现负载均衡，即用来定义VIP(直接填写一个和节点同一网段没使用过的IP即可，无需创建VIP)；可以和任一类型的Service一起使用(如LoadBalancer)</li><li><code>externalTrafficPolicy</code> 取值：Cluster(默认。隐藏源IP，可能会导致第二跳进行转发，负载可用性好)、Local(保留客户端源 IP 地址，不会尝试转发)。如果服务需要将外部流量路由到本地节点或者集群级别的端点，即service type 为LoadBalancer或NodePort，那么需要指明该参数<ul><li>当取值Cluster时，kube-proxy会在所有节点监听对应的nodePort，且可访问任意节点IP+nodePort访问到应用(可能会进行第二跳转发，且源IP会丢失)；当取值Local时，kube-proxy仅会在pod对应节点监听端口，且此时只能根据该节点IP访问到应用</li></ul></li><li><code>sessionAffinity</code> 是否session感知的：ClientIP(同一个客户永远访问的是同一个pod)、None(默认)</li></ul></li><li><code>status</code> 当前状态(current state)。由K8s进行维护，用户无需修改</li></ul><h4 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sq-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-pod</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">sq-test1</span></span><br><span class="line"><span class="attr">    tier:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-nginx</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">nginx:1.14-alpine</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-busybox</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"sleep 1h"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sq-dploy.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># 生成的pod名称会随机加一个字符串，如：sq-dploy-bjrgc</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-dploy</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 维持pod数量</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="comment"># 控制器选择pod的选择器。有可能选择的pod不是来自一个控制器，主要根据选择器来的</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">sq-test2</span></span><br><span class="line"><span class="attr">      tier:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="comment"># 创建pod的模板</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line">      <span class="comment"># 此name不会作为pod的名称，最终pod的名称为`RS名-xxxx`，如：sq-dploy-bjrgc</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">sq-dploy-pod</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">sq-test2</span></span><br><span class="line"><span class="attr">        tier:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">        profile:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">sq-nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">nginx:1.14-alpine</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><ul><li>基于yaml资源文件操作资源</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于配置文件创建pod</span></span><br><span class="line">kubectl create -f sq-pod.yaml</span><br><span class="line">kubectl get pods</span><br><span class="line"><span class="comment"># 基于配置文件删除资源</span></span><br><span class="line">kubectl delete -f sq-pod.yaml</span><br></pre></td></tr></table></figure><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><ul><li><p>Pod生命周期</p><p> <img src="/data/images/devops/k8s-pod-lifecycle.png" alt="k8s-pod-lifecycle"></p><ul><li>还有一个<code>Unknown</code>状态</li></ul></li><li><p>Pod生命周期中的重要行为</p><ul><li>运行主容器之前可以运行初始化容器</li><li>主容器运行成功后和退出前可以执行钩子：<code>post start</code>、<code>pre stop</code></li><li><p>主容器运行过程中可进行探测：<code>liveness</code>(存活状态检测)、<code>readiness</code>(就绪状态检测)</p><ul><li>探针类型：exec、tcpSocket、httpGet</li></ul><p><img src="/data/images/devops/k8s-pod-action.png" alt="k8s-pod-action"></p></li></ul></li><li>Pod状态(STATUS)<ul><li><code>Pending</code> 准备中，可能为：正在处理、没有合适的运行节点</li><li><code>Running</code> 正常运行</li><li><code>Terminating</code> 中断中</li><li><code>CrashLoopBackOff</code> 容器退出，kubelet正在将它重启</li><li><code>InvalidImageName</code> 无法解析镜像名称</li><li><code>ImageInspectError</code> 无法校验镜像</li><li><code>ErrImageNeverPull</code> 策略禁止拉取镜像</li><li><code>ImagePullBackOff</code> 正在重试拉取</li><li><code>RegistryUnavailable</code> 连接不到镜像中心</li><li><code>ErrImagePull</code> 通用的拉取镜像出错</li><li><code>CreateContainerConfigError</code> 不能创建kubelet使用的容器配置</li><li><code>CreateContainerError</code> 创建容器失败</li><li><code>ContainerCreating</code> 容器创建中</li><li><code>ContainersNotReady</code> 容器没有准备完毕</li><li><code>ContainersNotInitialized</code> 容器没有初始化完毕</li><li><code>RunContainerError</code> 启动容器失败</li><li><code>m.internalLifecycle.PreStartContainer</code> 执行hook报错</li><li><code>PostStartHookError</code> 执行hook报错</li><li><code>PodInitializing</code> pod初始化中</li><li><code>DockerDaemonNotReady</code> docker还没有完全启动</li><li><code>NetworkPluginNotReady</code> 网络插件还没有完全启动</li></ul></li><li>Pod条件(Conditions)<ul><li>Pod有一个PodStatus，它有一个PodConditions 数组。PodCondition数组的每个元素都有六个可能的字段<ul><li><code>type</code> 一个包含以下可能值的字符串<ul><li><code>PodScheduled</code> Pod已被安排到一个节点</li><li><code>Read</code> Pod能够提供请求，应该添加到所有匹配服务的负载平衡池中</li><li><code>Initialized</code> 所有init容器都已成功启动</li><li><code>Unschedulable</code> 调度程序现在无法调度Pod，例如由于缺少资源或其他限制</li><li><code>ContainersReady</code> Pod中的所有容器都已准备就绪</li></ul></li><li><code>status</code> 一个字符串，可能的值为True/False/Unknown</li><li><code>lastProbeTime</code> 提供上次探测Pod条件的时间戳</li><li><code>lastTransitionTime</code> 提供Pod最后从一个状态转换到另一个状态的时间戳</li><li><code>reason</code> 该条件最后一次转换的唯一，CamelCase原因</li><li><code>message</code> 指示有关转换的详细信息</li></ul></li></ul></li><li>Deployment是构建于RS之上的，可能出现一类Pod由不同的RS控制</li><li>重启Pod：直接删掉该Pod，会自动重新创建一个新的Pod</li><li><p>容器</p><ul><li><p>拉取私有仓库镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置所有节点，加入下列内容(docker客户端默认使用https访问仓库)</span></span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line">&#123;<span class="string">"insecure-registries"</span>: [<span class="string">"192.168.17.196:5000"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建docker-registry类型的secret(k8s必须创建secret才可拉取镜像，在节点机器上提前login也无法拉取)</span></span><br><span class="line">kubectl create secret docker-registry harbor-secret-ops --docker-server=192.168.17.196:5000 --docker-username=smalle --docker-password=Hello666 -n <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 并给对应Pod配置以下伪代码：imagePullSecrets[0].name=harbor-secret-ops</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="控制器"><a href="#控制器" class="headerlink" title="控制器"></a>控制器</h3><h4 id="Deployment-amp-ReplicaSet-amp-ReplicationController"><a href="#Deployment-amp-ReplicaSet-amp-ReplicationController" class="headerlink" title="Deployment &amp; ReplicaSet &amp; ReplicationController"></a>Deployment &amp; ReplicaSet &amp; ReplicationController</h4><ul><li>Deployment 与 ReplicaSet<ul><li>Deployment是构建于RS之上的，可能出现一类Pod由不同的RS控制</li><li>当创建了 Deployment 之后，实际上也创建了 ReplicaSet，所以说 Deployment 管理着 ReplicaSet</li><li>如果直接伸缩 ReplicaSet，但 Deployment 不会相应发生伸缩。如果ReplicaSet全部删除了，Deployment会自动创建一个新的副本集</li></ul></li><li>ReplicationController(RC) 每个Pod保存一个副本，早先K8s版本资源</li></ul><h4 id="StatefulSet-管理有状态副本集"><a href="#StatefulSet-管理有状态副本集" class="headerlink" title="StatefulSet(管理有状态副本集)"></a>StatefulSet(管理有状态副本集)</h4><ul><li>三个组件：headless service、StatefulSet、volumeClaimTemplate</li><li>会有序的创建pod，并逆序的移除pod</li></ul><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><ul><li><p>Service网络工作模式：userspace(1.1之前)、iptables(1.1默认)、ipvs(1.1之后)</p><ul><li><p>userspace：较慢，每次请求都需要kube-proxy转发</p><p> <img src="/data/images/devops/k8s-userspace.png" alt="k8s-userspace"></p><ul><li>apiserver提交修改(pod变更等) -&gt; kube-proxy -&gt; 修改iptables规则</li><li>client-pod -&gt; iptables -&gt; kube-proxy -&gt; server-pod</li></ul></li><li><p>iptablse/ipvs</p><ul><li>iptablse和ipvs工作流程一直，如下图。k8s配置成ipvs时，如果内核不支持，则会自动使用iptables</li><li><p>ipvs(IP Virtual Server)实现了传输层负载均衡，也就是常说的4层LAN交换，作为 Linux 内核的一部分。是运行在LVS(Linux Virtual Server)下的提供负载平衡功能的一种技术 <a href="https://www.qikqiak.com/post/how-to-use-ipvs-in-kubernetes/" target="_blank" rel="noopener">^6</a></p><p><img src="/data/images/devops/k8s-ipvs.png" alt="k8s-ipvs"></p></li><li>apiserver提交修改(pod变更等) -&gt; kube-proxy -&gt; 修改ipvs规则<ul><li>API Server修改了配置，被kube-proxy监视(watch)到，然后转换成iptables/ipvs规则(有延迟)</li></ul></li><li>client-pod -&gt; ipvs -&gt; server-pod</li></ul></li></ul></li><li>服务类型(type)<ul><li>ClusterIP(默认，仅k8s集群内访问)</li><li>NodePort(k8s集群外可访问，k8s边界服务)<ul><li>client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:containerPort</li></ul></li><li>LoadBalancer(负载均衡器，实现了流量经过前端负载均衡器分发到各个Node节点暴露出的端口，再通过ipvs/iptables进行一次负载均衡，最终分发到实际的Pod上这个过程。可通过<code>externalIPs</code>配合<code>IPVS</code>实现将外部流量引入到集群内部，同时实现负载均衡)<ul><li>LoadBalancer 是基于 NodePort 和云服务供应商提供的外部负载均衡器，通过这个外部负载均衡器将外部请求转发到各个 NodeIP:NodePort 以实现对外暴露服务</li></ul></li><li>ExternalName(将k8s外部服务映射到集群)<ul><li>CNAME -&gt; FQDN(将外部服务映射到内部，通过内部DNS服务进行访问)</li></ul></li><li>特殊服务：无头服务(headless services)<ul><li>此时<code>clusterIP=None</code>，且未定义type，即headless不分配clusterIP</li><li>kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。DNS 如何实现自动配置，依赖于 Service 是否定义了 selector</li><li>通过ServiceName获取PodIP<ul><li>headless service可以通过解析service的DNS，返回所有Pod的地址和DNS(statefulSet部署的Pod才有DNS)</li><li>普通的service只能通过解析service的DNS返回service的ClusterIP</li></ul></li></ul></li></ul></li><li><strong>Service通过观测(watch) API Server，每当Pod变化，API Server就会通知Service进行变更选择的Pod</strong></li><li>集群内部源Pod通过Service地址访问目标Pod时，可使用Service的ip/hostname:port，其中hostname格式为<code>service_name.namespace_name.svc</code>，如访问：<a href="http://prometheus-k8s.monitoring.svc:9090" target="_blank" rel="noopener">http://prometheus-k8s.monitoring.svc:9090</a></li></ul><h3 id="Ingress-amp-Ingress-Control-3"><a href="#Ingress-amp-Ingress-Control-3" class="headerlink" title="Ingress &amp; Ingress Control ^3"></a>Ingress &amp; Ingress Control <a href="https://www.cnblogs.com/linuxk/p/9706720.html" title="Ingress和Ingress Controller" target="_blank" rel="noopener">^3</a></h3><ul><li>Ingress Control背景<ul><li>特殊的控制器，不同于普通Control Manager，<strong>主要是提供集群访问入口(边界节点)，外部请求至Ingress Control(对应的NodePort/LoadBalancer类型的Service)，然后Ingress Control调用最终的Pod</strong><ul><li>集群提供外网访问：Pod直接定义hostNetwork共享节点网络；定义NodePort/LoadBalancer类型的Service代理访问到相应Pod</li></ul></li><li>如果使用https访问，则只需要在Ingress Control进行配置证书(否则所有的Pod都需要配置证书)，k8s内部Pod无需处理，内部仍然使用http明文调用(此时相当于再进行一次反向代理)<ul><li>客户域名进行访问，此时是访问到NodePort Service，而Service调度到Pod是第四层转换，而Https是工作在第七层。要建立Https连接必须要和最终主机(Pod)完成，因此就需要所有Pod配置https证书(K8s最终选择Ingress解决Https问题)</li></ul></li></ul></li><li><code>Ingress</code> 简单的理解就是你原来需要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，可以用 yaml 创建，每次不要去改 Nginx 了，直接改 yaml 然后创建/更新就行了；那么问题来了：”nginx 该怎么处理？”</li><li><p><code>Ingress Controller</code> 就是解决 “Nginx 的处理方式”的，Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod(Ingress-nginx) 里，最后 reload 一下，工作流程如下图</p><p> <img src="/data/images/devops/k8s-ingress.png" alt="k8s-ingress"></p><ul><li><strong>Pod变化，会反映到对应的Service；Ingress通过管理这些Service和应用Host的关系，得知某个Host最终可以访问那些Pod，并将相关配置注入到Ingress Controller；Client客户端请求Host到达Ingress Controller后，便可直接代理到最终的Pod</strong></li><li>实际上Ingress也是Kubernetes API的标准资源类型之一，它其实就是一组基于DNS名称（host）或URL路径把请求转发到指定的Service资源的规则，用于将集群外部的请求流量转发到集群内部完成的服务发布。Ingress资源自身不能进行”流量穿透”，仅仅是一组规则的集合，这些集合规则还需要其他功能的辅助，比如监听某套接字，然后根据这些规则的匹配进行路由转发，这些能够为Ingress资源监听套接字并将流量转发的组件就是Ingress Controller</li><li>此时使用NodePort暴露Ingress Controller；也可以(使Node)直接访问到Ingress Controller，不经过前面的Service(NodePort)。需要将Ingress Controller设置成DaemonSet，且共享Node的IP和端口</li></ul></li><li>Ingress的资源类型：单Service资源型Ingress、基于URL路径进行流量转发、基于主机名称的虚拟主机、TLS类型的Ingress资源</li><li>K8s中Ingress Control支持类型<ul><li>传统的七层负载均衡，如Nginx，HAproxy，开发了适应微服务应用的插件，具有成熟，高性能等优点</li><li>新型微服务负载均衡，如Traefik(基于go开放，和k8s融合更紧密)，Envoy，Istio，专门适用于微服务+容器化应用场景，具有动态更新特点</li></ul></li><li>ingress-nginx支持的代理类型<ul><li>http、tcp、udp，具体参考<a href="/_posts/devops/helm.md#ingress-nginx">ingress-nginx</a></li><li>ingress仅支持http代理，如果是tcp、udp代理需要额外开放端口</li></ul></li></ul><h4 id="Ingress-Controller部署-以ingress-nginx为例"><a href="#Ingress-Controller部署-以ingress-nginx为例" class="headerlink" title="Ingress Controller部署(以ingress-nginx为例)"></a>Ingress Controller部署(以ingress-nginx为例)</h4><ul><li>Ingress Control不直接运行为kube-controller-manager的一部分，它仅仅是Kubernetes集群的一个附件，类似于CoreDNS，需要在集群上单独部署</li><li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">ingress-nginx</a>，此为kubernetes维护的开源组件；还有一个类似的是nginx维护的(nginx-ingress)</li><li>手动安装如下，可以<a href="/_posts/devops/helm.md#ingress-nginx">基于helm安装</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 部署Ingress Controller，此时是Ingress Controller部署为Deployment。如果只执行此部署，则只能在集群内部访问，还需下文暴露成如Nodeport</span></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.25.1/deploy/static/mandatory.yaml</span><br><span class="line"><span class="comment"># 修改镜像地址(quay.io有时候会很慢)</span></span><br><span class="line">sed -i <span class="string">'s#quay.io/kubernetes-ingress-controller#registry.cn-hangzhou.aliyuncs.com/google_containers#g'</span> mandatory.yaml</span><br><span class="line">kubectl apply -f mandatory.yaml</span><br><span class="line">kubectl get deployment -n ingress-nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">## (生产环境一般使用 LoadBalancer) 使用Nodeport暴露Ingress Controller，如需修改暴露的节点端口，可添加 nodePort: 30080 和 nodePort: 30443 来指定</span></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.25.1/deploy/static/provider/baremetal/service-nodeport.yaml</span><br><span class="line">kubectl apply -f service-nodeport.yaml</span><br><span class="line"><span class="comment"># kubectl expose deployment nginx-ingress-controller --port 80 --external-ip 192.168.6.132 # 或者基于 LoadBalancer + externalIPs 来暴露服务</span></span><br><span class="line">kubectl get svc -n ingress-nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">## 取消HSTS配置：https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/</span></span><br><span class="line"><span class="comment"># 修改配置，具体见下文nginx-configuration配置。修改后可能得半分钟左右生效</span></span><br><span class="line">kubectl edit configmap nginx-configuration -n ingress-nginx <span class="comment"># 如果基于helm安装则是 nginx-ingress-controller</span></span><br><span class="line"><span class="comment">## 查看 ingress-controller 对应pod的nginx配置</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it nginx-ingress-controller-74c6b9c45c-9qm54 -n ingress-nginx cat /etc/nginx/nginx.conf</span><br><span class="line"><span class="comment"># 查看日志(cat /var/log/nginx/access.log 卡死)</span></span><br><span class="line">kubectl logs nginx-ingress-controller-74c6b9c45c-9qm54 -n ingress-nginx</span><br></pre></td></tr></table></figure><ul><li>nginx-configuration配置(configmap)</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="comment"># 取消hsts，配置成功后查看nginx配置时则无`Strict-Transport-Security`相关代码</span></span><br><span class="line"><span class="attr">  hsts:</span> <span class="string">"false"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure><ul><li>Ingress资源注解说明<ul><li><code>nginx.ingress.kubernetes.io/whitelist-source-range: 192.168.6.0/24,192.168.1.100</code> 访问白名单</li><li><code>nginx.ingress.kubernetes.io/canary: true</code> <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary" target="_blank" rel="noopener">金丝雀(canary)/灰度发布功能</a></li></ul></li></ul><h4 id="创建Ingress示例"><a href="#创建Ingress示例" class="headerlink" title="创建Ingress示例"></a>创建Ingress示例</h4><ul><li>查看定义<code>kubectl explain ingress</code>，此处以<code>ingress.aezocn.local</code>(外部测试需要在hosts中加入对应节点IP)为例</li><li>先创建一个测试服务</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sq-ingress.yaml</span></span><br><span class="line"><span class="comment"># 创建测试service为sq-ingress</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># service名称</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">sq-ingress</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">http</span></span><br><span class="line">    <span class="comment"># 服务端口</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8080</span></span><br><span class="line">    <span class="comment"># 容器端口</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ajp</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8009</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8009</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 创建后端服务的pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-ingress-backend-pod</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">sq-ingress</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">sq-ingress</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">tomcat:8.5.43-jdk8</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">ajp</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">8009</span></span><br></pre></td></tr></table></figure><ul><li>编写ingress的配置清单<ul><li>Ingress必须基于域名进行设置，测试可将域名-IP对应关系写到本地hosts中</li><li>如果有多个应用，可以创建多个Ingress</li></ul></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ingress-sq-ingress.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ingress-sq-ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">"nginx"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 定义后端转发的规则</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">ingress.aezocn.local</span></span><br><span class="line">    <span class="comment"># ingress只能代理http端口，如果需要代理tcp/udp可参考[ingress-nginx](/_posts/devops/helm.md#ingress-nginx)</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line">      <span class="comment"># 配置后端服务</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">sq-ingress</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br><span class="line">        <span class="comment"># 配置访问路径，如果通过url进行转发，需要修改；空默认为访问的路径为"/"</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">"/"</span></span><br><span class="line">  <span class="comment"># 配置TLS站点才需要(结合下文构建TLS站点示例)</span></span><br><span class="line"><span class="attr">  tls:</span></span><br><span class="line"><span class="attr">  - secretName:</span> <span class="string">sq-ingress-secret</span></span><br><span class="line"><span class="attr">    hosts:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ingress.aezocn.local</span></span><br><span class="line">    <span class="comment"># 指定 secret 名称(可通过TLS证书创建，见下文)</span></span><br></pre></td></tr></table></figure><ul><li><code>kubectl get ingress</code> 查看ingress配置</li><li>可在集群外访问 <code>http://ingress.aezocn.local</code> 会显示tomcat主页<ul><li>下文TSL站点则访问<code>https://ingress.aezocn.local</code>。如果配置了TSL，则访问http时，默认会跳转到https(443)</li><li>如果修改ingress-nginx的service-nodeport.yaml中节点端口，此处测试访问的域名应该加上相应端口</li></ul></li><li>常见问题<ul><li>访问https地址时，点击高级不显示<code>继续前往ingress.aezocn.local（不安全）</code>，而是显示<code>您目前无法访问 ingress.aezocn.local，因为此网站使用了 HSTS。网络错误和攻击通常是暂时的，因此，此网页稍后可能会恢复正常</code><ul><li>原因：ingress-nginx默认模式是以HSTS访问，如果证书有问题，可能直接导致浏览器无法忽略警告继续访问</li><li>可访问<code>chrome://net-internals/#hsts</code>进行清除当前域名的hsts设置(测试没有清除成功)</li><li>或者去掉HSTS后，重新更换域名</li></ul></li></ul></li></ul><h4 id="构建TLS站点示例"><a href="#构建TLS站点示例" class="headerlink" title="构建TLS站点示例"></a>构建TLS站点示例</h4><ul><li>手动创建证书</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建证书</span></span><br><span class="line"><span class="built_in">cd</span> ~/.certs/</span><br><span class="line">openssl genrsa -out aezocn.key 2048</span><br><span class="line"><span class="comment"># 注意签名中的域名(表示只有此域名以https访问证书才有效)</span></span><br><span class="line">openssl req -new -x509 -key aezocn.key -out aezocn.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=ingress.aezocn.local</span><br><span class="line"><span class="comment">## 生成secret(类型为tls，名称为sq-ingress-secret)</span></span><br><span class="line">kubectl create secret tls sq-ingress-secret --cert=aezocn.crt --key=aezocn.key</span><br><span class="line"><span class="comment">## 启动ingress-nginx后，可通过容器日志看到证书加载日志 Adding Secret "default/sq-ingress-secret" to the local store；如果secret无效，则会使用系统默认证书Kubernetes Ingress Controller Fake Certificate</span></span><br></pre></td></tr></table></figure><ul><li>安装Let’s Encrypt免费SSL证书(Let’s Encrypt提供90天的证书有效期，可安装自动续期服务)</li></ul><h3 id="存储卷"><a href="#存储卷" class="headerlink" title="存储卷"></a>存储卷</h3><ul><li>csi(Container Storage Interface)[<a href="https://github.com/container-storage-interface/spec]" target="_blank" rel="noopener">https://github.com/container-storage-interface/spec]</a></li><li>常用分类<ul><li><code>emptyDir</code> 临时目录</li><li><code>hostPath</code> 宿主机目录映射</li><li>PVC持久化存储<ul><li>本地存储 <code>SAN</code>(<code>iSCSI</code>、<code>FC</code>)、<code>NAS</code>(<code>nfs</code>、<code>cifs</code>、<code>http</code>)</li><li>分布式存储 <code>glusterfs</code>、<code>rbd</code>、<code>ceph</code>/<code>rook</code></li><li>云存储 <code>EBS</code>、<code>Azure Disk</code></li></ul></li><li>存储选型：私有云可考虑使用<code>Rook</code>/<code>Ceph</code> <a href="https://blog.fleeto.us/post/kubernetes-storage-performance-comparison/" title="Kubernetes 存储性能对比" target="_blank" rel="noopener">^8</a></li></ul></li><li>存储卷挂载过程<ul><li>provision，卷分配成功，这个操作由PVController完成</li><li>attach，卷挂载在对应worker node，这个操作为AttachDetachController完成</li><li>mount，卷挂载为文件系统并且映射给对应Pod，这个操作为VolumeManager完成</li></ul></li><li>存储卷卸载过程<ul><li>umount，卷已经和对应worker node解除映射，且已经从文件系统umount</li><li>detach，卷已经从worker node卸载</li><li>recycle，卷被回收</li></ul></li><li><code>kubectl explain pod.spec.volumes</code> 查看k8s支持的存储类型及配置<ul><li><code>emptyDir</code> 临时目录存储，取值<code>{}</code>时，则子字段为默认值。<strong>Pod删除，数据也会丢失</strong>，容器的crashing事件并不会导致emptyDir中的数据被删除</li><li><code>hostPath</code> 宿主机目录存储，重新创建Pod后数据还在，但各节点目录不共享。<a href="https://kubernetes.io/docs/concepts/storage/volumes#hostpath" target="_blank" rel="noopener">doc</a><ul><li><code>type</code> 存储类型。默认””，取值：DirectoryOrCreate(可自动创建存储目录)/DirectoryFileOrCreate/File/Socket/CharDevice/BlockDevice</li><li><code>path</code> 数据存储在Node节点上存储目录</li></ul></li><li><code>nfs</code> 基于nfs的网络存储，各节点可共享。注：此时各Node节点需要可驱动nfs，可在各节点安装<code>nfs-utils</code><ul><li><code>server</code> nfs服务器地址，IP或者hostname</li><li><code>path</code></li></ul></li><li><code>persistentVolumeClaim</code> PVC(存储卷创建申请)<ul><li><code>claimName</code> 对应PVC资源名称</li></ul></li><li><code>configMap</code><ul><li><code>name</code> ConfigMap资源名</li></ul></li><li><code>secret</code><ul><li><code>secretName</code></li></ul></li></ul></li><li><code>kubectl explain pv.spec</code> 查看PersistentVolume(pv)配置<ul><li><code>accessModes</code> 定义访问模型，可定义多个。取值：ReadWriteOnce(RWO，单节点读写)、ReadWriteMany(RWX，多节点读写)、ReadOnlyMany(ROX，多节点只读)。<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes" target="_blank" rel="noopener">支持的存储模型</a></li><li><code>capacity</code> 定义PV空间的大小<ul><li><code>storage</code> eg：5G(1000换算)、5Gi(1024换算)，Ki | Mi | Gi | Ti | Pi | Ei等</li></ul></li><li><code>nfs</code> 基于nfs配置pv。还可通过其他方式如分布式存储、云存在进行配置</li><li><code>persistentVolumeReclaimPolicy</code> 回收pv策略。取值：Retain(保留，需手动删除，默认值)、Recycle(回收，只有 NFS 和 HostPath 支持)、Delete(关联的存储资产如EBS、Azure Disk等将被删除)</li></ul></li><li><code>kubectl explain pvc.spec</code> 查看PersistentVolumeClaim(pvc)配置<ul><li><code>accessModes</code> 定义访问模式，必须是PV的访问模式的子集。一般如<code>ReadWriteOnce</code></li><li><code>resources</code> 定义申请资源的大小<ul><li><code>requests</code><ul><li><code>storage</code> 定义大小，eg：2Gi</li></ul></li></ul></li><li><code>storageClassName</code> 可自动根据PVC创建PV</li></ul></li><li><code>kubectl explain sc</code> 查看StorageClass(sc)配置。使用参考<a href="/_posts/devops/ceph.md#k8s使用ceph存储">ceph.md#k8s使用ceph存储</a><ul><li><code>provisioner</code> 存储提供者，如<code>rook-ceph.rbd.csi.ceph.com</code>(基于rook-ceph的存储方案)</li><li><code>parameters</code> 相关参数</li><li><code>reclaimPolicy</code> 类似pv的persistentVolumeReclaimPolicy参数取值：Retain、Recycle、Delete(默认)，创建后无法修改</li><li><code>allowVolumeExpansion</code> 是否允许扩容(true允许)</li></ul></li><li>PVC、PV、SC<ul><li>存储管理员提前创建不同存储服务(nfs、glusterfs等)，K8s集群管理根据不同的持久化卷类型配置存储卷映射(PV，集群公共资源)，用户基于存储卷创建定义PVC</li><li><code>PV</code>状态：<code>Available</code>(可用) -&gt; <code>Bound</code>(绑定) -&gt; <code>Released</code>(释放) -&gt; Failed(失败。该卷的自动回收失败)<ul><li>Released说明：声明被删除，但是资源还未被集群重新声明。当pv回收策略为Retain时，删除了对应pvc(此pv之前绑定的)后，此时pod中数据得到了保留，但其 PV 状态会一直处于 Released，不能被其他 PVC 申请。为了重新使用存储资源，可以删除PV并重新创建该PV(<strong>删除 PV 操作只是删除了 PV 对象，即k8s-pv与存储介质之间的对应关系，存储空间中的数据并不会被删除</strong>)</li></ul></li><li><code>PVC</code>状态：<code>Pending</code>(准备中) -&gt; <code>Bound</code>(绑定)<ul><li>PVC一直处于Pending状态，而PV却处于Bound状态，可能情况：如使用的NFS服务器关闭了；定义的PV大小、读写类型不符合PVC的要求</li><li>PV一直处于Released状态：如果确认此PV不再使用(对应的数据文件目录)，可删除此PV重新创建PV</li></ul></li><li><code>SC</code>资源配置，参考：<a href="/_posts/devops/rook.md#简单使用">http://blog.aezo.cn/2019/06/22/devops/rook-ceph/</a><ul><li>在pvc申请存储空间时，未必就有现成的pv符合pvc申请的需求。当用户突然需要使用PVC时，可通过restful发送请求StorageClass，继而SC让存储空间创建相应的存储image，之后在集群中定义对应的PV供给当前的PVC作为挂载使用。因此存储系统必须支持restful接口，比如ceph分布式存储，而glusterfs则需要借助第三方接口完成这样的请求</li></ul></li><li>PV和PVC创建无需先后顺序</li></ul></li><li><code>ConfigMap</code>和<code>Secret</code>为一种特殊的存储卷</li></ul><h4 id="emptyDir案例"><a href="#emptyDir案例" class="headerlink" title="emptyDir案例"></a>emptyDir案例</h4><ul><li>下例中，busybox与nginx使用的存储卷相同，因此可看成是同一个目录<ul><li>busybox修改容器中/data/index.hmtl文件 -&gt; 相当于busybox挂载的存储卷html下index.html被修改 -&gt; 相当于nginx容器/usr/share/nginx/html目录下的index.html文件被修改。实际是同一个文件</li><li><code>kubectl get pods -o wide</code>查看pod对应IP，再使用<code>curl 10.244.1.22</code>访问即可看到网页变化</li></ul></li><li>测试案例配置</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-volumes</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">sq-volumes</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-nginx</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">nginx:1.14-alpine</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line">    <span class="comment"># 对应此pod定义的存储卷名</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">html</span></span><br><span class="line">      <span class="comment"># 挂载容器的目录</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-busybox</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="comment"># pod中多个容器必须都要挂载才能访问存储券</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line">    <span class="comment"># 对应此pod定义的存储卷名</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">html</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/data/</span></span><br><span class="line">    <span class="comment"># 此时busybox容器会定时往index.hmtl中加数据</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">['/bin/sh',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'while true; do echo $(date) &gt;&gt; /data/index.html; sleep 2; done'</span><span class="string">]</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">html</span></span><br><span class="line">    <span class="comment"># &#123;&#125; 表示使用默认配置</span></span><br><span class="line"><span class="attr">    emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><h4 id="PVC、PV、NFS配合使用案例"><a href="#PVC、PV、NFS配合使用案例" class="headerlink" title="PVC、PV、NFS配合使用案例"></a>PVC、PV、NFS配合使用案例</h4><ul><li>配置NFS：此处使用192.168.6.10(store1)作为NFS存储服务器，此服务器和所有的Node节点必须安装NFS(<code>yum install -y nfs-utils</code>)<ul><li>store1节点配置NFS，参考：<a href="/_posts/linux/CentOS服务器使用说明.md#NFS">NFS</a></li></ul></li><li>创建pv、pvc、pod</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## sq-pv.yml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pv001</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">pv001</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.10</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/volumes/v1</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteMany",</span> <span class="string">"ReadWriteOnce"</span><span class="string">]</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pv002</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">pv002</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="string">store1</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/volumes/v2</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteMany"]</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## sq-pvc.yml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># storageClassName: rook-ceph-block # 使用 StorageClass 动态创建PV时需要</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteMany"]</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-volumes-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-nginx</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">nginx:1.14-alpine</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">html</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">html</span></span><br><span class="line"><span class="attr">    persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">      claimName:</span> <span class="string">sq-pvc</span></span><br></pre></td></tr></table></figure><ul><li>测试<ul><li>找到pod绑定的pv(<code>kubectl get pvc</code>)，从而得知pv对应的nfs目录</li><li>进入到store1对应目录创建主页 <code>echo &quot;welcome smalle!&quot; &gt; index.html</code></li><li>访问此pod，如<code>curl 10.244.1.22</code></li></ul></li></ul><h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><ul><li>K8s网络类型：节点网络、Service网络(10.xx，生成虚拟的IP)、Pod网络(默认10.244.0.0/16)<ul><li>节点一：cni0网桥(10.244.0.1/24)、flannel.1网卡(10.244.0.1/32)；其他节点：10.244.1.x, … , 10.244.x.x</li><li><code>--pod-network-cidr=10.244.0.0/16</code> 初始master节点参数，则规定pod网络为此参数设定的。运行pod后会产生一个<code>cni0</code>的网桥，pod网络只能在K8s集群内部使用</li></ul></li><li><p>通信方式 <a href="https://www.jianshu.com/p/3f2401d14c78" title="K8s网络模型" target="_blank" rel="noopener">^7</a></p><p> <img src="/data/images/devops/k8s-network.png" alt="k8s-network"></p><p> <img src="/data/images/devops/k8s-network2.webp" alt="k8s-network2"></p><ul><li>同一个Pod内多个容器之间通信：Pod本地<ul><li>一个Pod内多个容器共享同一个网络命名空间，每个Docker容器拥有与Pod相同的IP和port地址空间，可以通过localhost相互访问。本质是是使用Docker的<code>-net=container</code>网络模型</li></ul></li><li>各Pod之间通信：物理网桥、Overlay叠加网络(常用)<ul><li>同一Node上的两个Pod通过veth对链接到root网络命名空间(宿主机)，并且通过网桥(宿主机的docker0)进行通信</li><li>不同Node上的Pod通信(如上图k8s-network2：Node-vm1上的Pod1与Node-vm2上Pod4之间进行交互)<ul><li>首先pod1通过自己的以太网设备eth0把数据包发送到关联到root命名空间的veth0上，然后数据包被Node1上的网桥设备cbr0(docker0)接受到，网桥查找转发表发现找不到pod4的Mac地址，则会把包转发到默认路由(root命名空间的eth0设备)，然后数据包经过eth0就离开了Node1，被发送到网络</li><li>数据包到达Node2后，首先会被root命名空间的eth0设备，然后通过网桥cbr0把数据路由到虚拟设备veth1,最终数据表会被流转到与veth1配对的另外一端(pod4的eth0)</li></ul></li><li>Overlay网络(VxLan)参考：<a href="/_posts/devops/docker.md#docker网络">http://blog.aezo.cn/2017/06/25/devops/docker/</a></li><li>Flannel(见下文CNI插件)致力于给k8s集群中的nodes提供一个3层网络，他并不控制node中的容器是如何进行组网的，仅仅关心流量如何在node之间流转<ul><li>上例中流量从Node1上的网桥设备cbr0到达宿主机eth0时，中间会经过flannel0，并有flanneld进程进行封包才到达eth0；相反从Node2的eth0到达网桥前也会经过flannel0由flanneld进程解包</li></ul></li></ul></li><li>Pod与Service之间通信：Kube-proxy(运行在Node上的守护进程)，参考上文Service</li></ul></li><li>集群内部通信是点对点通信(Https通信)，需CA证书的S/C类型<ul><li><code>etcd - etcd</code></li><li><code>etcd - API Server</code></li><li><code>API Server - CLI</code></li><li><code>API Server - Node(Kublet)</code></li><li><code>API Server - Node(Kube-proxy)</code></li></ul></li><li><code>CNI</code> K8s基于CNI网络接口进行通信，只要实现了CNI接口都可用于K8s上的通信<ul><li>解决方案：虚拟网桥、多路复用(MacVLAN)、硬件交换(SR-IOV)</li></ul></li><li><p>常用CNI插件(/etc/cni/net.d)</p><ul><li><code>flannel</code> 支持网络配置<ul><li>会运行在所有的<code>kubelet</code>上，每个节点会运行一个相应的pod(DaemonSet守护进程)</li><li>对应ConfigMap参数(<code>kubectl get configmap kube-flannel-cfg -o yaml -n kube-system</code>)<ul><li><code>Network</code> flannel使用的CIDR格式的网络地址，用于为Pod配置网络功能<ul><li>示例一：集群pod网络为<code>10.244.0.0/16</code>(可容纳256个节点，默认)：master(此节点上的pod网络为10.244.0.0/24)、node01(10.244.1.0/24)、…、node255(10.244.255.0/24)<ul><li>此时可容纳256个节点，每个节点还可以部署256个容器。理论上一个节点不会部署太多容器，因此可适当调节子网掩码从而扩大节点个数</li></ul></li><li>示例二：<code>10.0.0.0/8</code>(可容纳2^16=65536个节点)：10.0.0.0/24、…、10.255.255.0/24(默认第2-3段为子网，第4段为节点内部使用)</li></ul></li><li><code>SubnetLen</code> 把Network切分子网供各节点使用时，使用的掩码切分长度节点网络，默认24位(则剩余8位可为主机号，即一个节点上可运行的pod数量为256)掩码</li><li><code>SubnetMin</code> 最小的子网地址。eg：10.244.0.0/24</li><li><code>SubnetMax</code> eg：10.244.255.0/24</li><li><code>Backend</code> 支持后端类型<code>Type</code>取值：<code>vxlan</code>、<code>host-gw</code>、<code>udp</code><ul><li><code>VxLAN</code><ul><li>Node处于同一网段可使用Directouting，处于不同网段则必须使用VxLAN。VxLAN使用叠加网络，损耗会比Directouting高</li><li>默认没有开启<code>Directouting</code>，需要编辑配置文件添加<code>Directouting: true</code></li><li>VxLan模式流量走向：cn0 -&gt; flannel.1 -&gt; ens33；Directouting则为：cn0 -&gt; ens33(中间通过路由转换了)</li></ul></li><li><code>Host Gateway</code> 要求各节点必须在一个网段</li></ul></li></ul></li></ul></li><li><code>calico</code> 支持网络配置、网络策略(功能强大，但较flannel复杂)</li><li><p><code>canel</code> 上述二者合并(推荐)</p><ul><li>Calico可以独立地为Kubernetes提供网络解决方案和网络策略，也可以和flannel相结合，由flannel提供网络解决方案，Calico仅用于提供网络策略，此时将Calico称为Canal</li><li><p>安装(安装canel则无需单独再安装flannel)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://docs.projectcalico.org/v3.8/getting-started/kubernetes/installation/flannel</span></span><br><span class="line"><span class="comment"># canal内部会安装flannel镜像</span></span><br><span class="line">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/canal.yaml</span><br><span class="line">kubectl get pods -n kube-system -o wide | grep canal</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>网络策略(NetworkPolicy，netpol)资源配置</p></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">allow-sq-demo-ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">## 表示所有pod</span></span><br><span class="line">  <span class="comment">#podSelector: &#123;&#125;</span></span><br><span class="line"><span class="attr">  podSelector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">sq-demo</span></span><br><span class="line">  <span class="comment"># NetworkPolicy类型，可以是Ingress，Egress，或者两者共存</span></span><br><span class="line"><span class="attr">  policyTypes:</span> <span class="string">["Ingress"]</span></span><br><span class="line">  <span class="comment">## 允许所有网络可访问 </span></span><br><span class="line">  <span class="comment">#ingress:</span></span><br><span class="line">  <span class="comment">#- &#123;&#125;</span></span><br><span class="line">  <span class="comment">## 不定义ingress，则禁止所有网络访问(入站)</span></span><br><span class="line">  <span class="comment">## 定义入站详细规则</span></span><br><span class="line"><span class="attr">  ingress:</span></span><br><span class="line">  <span class="comment"># 如果不写from则表示所有网段可访问</span></span><br><span class="line"><span class="attr">  - from:</span></span><br><span class="line">    <span class="comment"># 定义可以访问的网段</span></span><br><span class="line"><span class="attr">    - ipBlock:</span></span><br><span class="line"><span class="attr">        cidr:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">        <span class="comment"># 排除的网段</span></span><br><span class="line"><span class="attr">        except:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="number">10.244</span><span class="number">.3</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line"><span class="attr">    - podSelector:</span></span><br><span class="line">        <span class="comment"># 选定当前dev名称空间下标签为app:sq-demo的pod可以被访问</span></span><br><span class="line"><span class="attr">        matchLabels:</span></span><br><span class="line"><span class="attr">          app:</span> <span class="string">sq-demo</span></span><br><span class="line">    <span class="comment"># 开放的协议和端口定义</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br><span class="line">   <span class="comment"># 定义出站规则，与入站类似。不定义则为禁止访问外部网络</span></span><br><span class="line">   <span class="comment">#engress:</span></span><br></pre></td></tr></table></figure><h4 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h4><ul><li>ks8常见DNS插件：kube-dns 和 CoreDNS(k8s 1.11默认)</li><li><code>kubectl edit configmap coredns -n kube-system</code> 编辑coredns对应的配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    health</span><br><span class="line">    kubernetes cluster.local <span class="keyword">in</span>-addr.arpa ip6.arpa &#123;</span><br><span class="line">       pods insecure</span><br><span class="line">       upstream                             <span class="comment"># upstream 用于解析指向外部主机的服务</span></span><br><span class="line">       <span class="comment"># upstream 172.16.0.1</span></span><br><span class="line">       fallthrough <span class="keyword">in</span>-addr.arpa ip6.arpa</span><br><span class="line">       ttl 30</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 自定义域名解析</span></span><br><span class="line">    <span class="comment"># hosts &#123;</span></span><br><span class="line">    <span class="comment">#   192.168.6.131  k8s.aezocn.1.com</span></span><br><span class="line">    <span class="comment">#   192.168.6.132  k8s.aezocn.1.com</span></span><br><span class="line">    <span class="comment">#   fallthrough</span></span><br><span class="line">    <span class="comment"># &#125;</span></span><br><span class="line">    prometheus :9153            <span class="comment"># CoreDNS的度量标准</span></span><br><span class="line">    forward . /etc/resolv.conf</span><br><span class="line">    <span class="comment"># proxy . /etc/resolv.conf  # 任何不在Kubernetes集群域内的查询都将转发到预定义的解析器(/etc/resolv.conf)</span></span><br><span class="line">    <span class="comment"># proxy . 172.16.0.1</span></span><br><span class="line">    cache 30                    <span class="comment"># 这将启用前端缓存</span></span><br><span class="line">    loop                        <span class="comment"># 检测简单的转发循环，如果找到循环则停止CoreDNS进程</span></span><br><span class="line">    reload                      <span class="comment"># 允许自动重新加载已更改的Corefile。编辑ConfigMap配置后，请等待两分钟以使更改生效</span></span><br><span class="line">    loadbalance                 <span class="comment"># 这是一个循环DNS负载均衡器，可以随机化A，AAAA和MX记录的顺序</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>查看pod容器dns解析配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 在某pod容器中运行 `cat /etc/resolv.conf` 打印如下</span></span><br><span class="line">nameserver 10.96.0.10 <span class="comment"># dns服务器地址(CoreDNS运行在pod中，此ip为pod暴露成服务的ip)</span></span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local <span class="comment"># svc.cluster.local为service在k8s集群中的名称；default为命名空间，此时为默认命令空间</span></span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># yum install bind-utils # 安装dig</span></span><br><span class="line"><span class="comment">## 向10.96.0.10的DNS服务器查询default空间下nginx服务的ip(可以在Node节点上运行)</span></span><br><span class="line">dig -t A nginx.default.svc.cluster.local @10.96.0.10</span><br><span class="line"><span class="comment"># 查询pod，格式：pod_name.service_name.namespace_name.svc.cluster.local</span></span><br><span class="line">dig -t A sq-nginx-75875cf46f-829nm.sq-nginx.default.svc.cluster.local @10.96.0.10</span><br></pre></td></tr></table></figure><ul><li>其他说明<ul><li>服务重新创建后，服务会重新生成虚拟IP，且会反馈到CoreDNS中。其他pod仍然可以使用此服务名称进行访问</li></ul></li></ul><h3 id="访问API-Server认证"><a href="#访问API-Server认证" class="headerlink" title="访问API Server认证"></a>访问API Server认证</h3><ul><li><code>RBAC</code> 基于角色的访问控制<ul><li>k8s基于RBAC进行权限控制</li><li><code>Role</code> 对象的作用范围是命名空间(namespace)内，<code>ClusterRole</code> 对象的作用范围是k8s集群范围<ul><li><code>Role</code>(<code>kubectl explain Role</code>)<ul><li><code>rules</code><ul><li><code>apiGroups</code> 限定的api列表，不限定则可以加一个<code>- &quot;&quot;</code>子元素</li><li><code>resources</code> 资源列表，如：<code>[&quot;nodes&quot;, &quot;pods&quot;, &quot;deployments&quot;, &quot;namespaces&quot;]</code></li><li><code>verbs</code> 可执行动作列表，如：<code>[&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</code> 或者 <code>[&#39;*&#39;]</code></li></ul></li></ul></li><li><code>ClusterRole</code> 集群角色(无namespace概念)</li></ul></li><li><code>RoleBinding</code> 和 <code>ClusterRoleBinding</code> 则是(某个命名空间/集群)角色和用户的绑定关系</li></ul></li><li>创建访问API Server账号<ul><li>如果是创建Dashboard访问账号(或在pod中需要访问api server)，则是创建ServiceAccount，参考下文手动安装Dashboard</li><li>创建sa账号后会自动在同一命名空间创建一个xxx-token-xxx的secret；删除sa账号时也会同步删除</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建证书</span></span><br><span class="line"><span class="built_in">cd</span> /root/.certs</span><br><span class="line">(<span class="built_in">umask</span> 077; openssl genrsa -out aezo.key 2048) <span class="comment"># 生成证书</span></span><br><span class="line">openssl req -new -key aezo.key -out aezo.csr -subj <span class="string">"/CN=aezo"</span> <span class="comment"># 证书签署请求</span></span><br><span class="line">openssl x509 -req -<span class="keyword">in</span> aezo.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out aezo.crt -days 365 <span class="comment"># 添加到用户认证，有效期365天</span></span><br><span class="line">openssl x509 -<span class="keyword">in</span> aezo.crt -text -noout <span class="comment"># 查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加用户到当前证书</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials aezo --client-certificate=./aezo.crt --client-key=./aezo.key --embed-certs=<span class="literal">true</span> <span class="comment"># 创建用户（如何删除？？？）</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-context aezo@kubernetes --cluster=kubernetes --user=aezo <span class="comment"># 创建上下文</span></span><br><span class="line"><span class="comment"># kubectl config delete-context aezo@kubernetes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看用户配置：会多出一个context(定义用户可以访问的集群)和user</span></span><br><span class="line">kubectl config view</span><br><span class="line"><span class="comment"># 切换用户context，以aezo用户权限访问集群API Server(view中current-context会变成当前用户context)</span></span><br><span class="line">kubectl config use-context aezo@kubernetes</span><br><span class="line"><span class="comment"># 提示：Error from server (Forbidden): pods is forbidden: User "aezo" cannot list resource "pods" in API group "" in the namespace "default"。由于aezo用户无管理集群的权限，所以在获取pods资源信息时，会提示Forrbidden</span></span><br><span class="line"><span class="comment"># 对此用户赋予权限参考下文</span></span><br><span class="line"><span class="comment"># kubectl get pods --context=aezo@kubernetes # 或者通过设定context查询</span></span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><ul><li>创建角色和绑定关系示例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建角色</span></span><br><span class="line"><span class="comment"># 干跑模式查看role的定义，不会产生实际操作，创建clusterrole同理。--verb操作权限定义，--resource资源定义</span></span><br><span class="line">kubectl create role my-pods-reader --verb=get,list,watch --resource=pods --dry-run -o yaml</span><br><span class="line"><span class="comment"># 干跑模式生成创建role配置文件</span></span><br><span class="line">kubectl create role my-pods-reader --verb=get,list,watch --resource=pods --dry-run -o yaml &gt; role-demo.yaml</span><br><span class="line">kubectl apply -f role-demo.yaml</span><br><span class="line">kubectl describe role my-pods-reader</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建角色绑定</span></span><br><span class="line">kubectl create rolebinding aezo-read-pods --role=my-pods-reader --user=aezo --dry-run -o yaml &gt; rolebinding-demo.yaml</span><br><span class="line">kubectl apply -f rolebinding-demo.yaml</span><br><span class="line">kubectl describe rolebinding aezo-read-pods</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用上述创建的aezo账号测试</span></span><br><span class="line">kubectl config use-context aezo@kubernetes</span><br><span class="line">kubectl get pods <span class="comment"># 此时不会显示Forrbidden</span></span><br></pre></td></tr></table></figure><ul><li>创建新集群示例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --kubeconfig 设置集群配置文件，默认文件为 `$&#123;HOME&#125;/.kube/config`(默认集群文件)</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster my-cluster --kubeconfig=/tmp/test.conf --server=<span class="string">"https://192.168.6.131:6443"</span> --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 查看集群</span></span><br><span class="line">kubectl config view --kubeconfig=/tmp/test.conf</span><br></pre></td></tr></table></figure><ul><li>创建某命名空间的SA管理用户用于登录Dashboard</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl create namespace aezo-test</span><br><span class="line">kubectl create serviceaccount sa-aezo-admin -n aezo-test</span><br><span class="line"><span class="comment"># test:sa-aezo-admin为绑定名称(可随便取)；--serviceaccount=SA命名空间:SA；cluster-admin是k8s内置的集群角色，也可自己创建集群角色；此处-n aezo-test则代表rolebinding属于此命名空间，则相当于赋予此SA账户clusterrole=cluster-admin所拥有的aezo-test命名空间的部分权限</span></span><br><span class="line">kubectl create rolebinding <span class="built_in">test</span>:sa-aezo-admin --clusterrole=cluster-admin --serviceaccount=aezo-test:sa-aezo-admin -n aezo-test</span><br><span class="line"><span class="comment"># 获取ServiceAccount关联的secret(xxx-token-xxx)</span></span><br><span class="line">kubectl get secret $(kubectl get secret -n aezo-test|grep sa-aezo-admin-token|awk <span class="string">'&#123;print $1&#125;'</span>) -n aezo-test -o jsonpath=&#123;.data.token&#125;|base64 -d |xargs <span class="built_in">echo</span></span><br><span class="line"><span class="comment"># 问题：登录后页面默认显示的是default命名空间，需要手动输入或者访问带上命名空间：https://192.168.6.131:30000/#!/overview?namespace=java-test</span></span><br></pre></td></tr></table></figure><h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><ul><li>API Server在接受客户端提交Pod对象创建请求后，然后是通过调度器（kube-schedule）从集群中选择一个可用的最佳节点来创建并运行Pod。而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选(过滤不符合条件的节点)、节点优选(对预选出的节点进行优先级排序)、节点选定(符合条件且优选级相同的则随机选择需要的数量)，从而筛选出最佳的节点</li><li>常用的预选策略(<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/algorithm/predicates/predicates.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/algorithm/predicates/predicates.go</a>)<ul><li>CheckNodeCondition：检查是否可以在节点报告磁盘、网络不可用或未准备好的情况下将Pod对象调度其上</li><li>GeneralPredicates<ul><li>HostName：如果Pod对象拥有spec.hostname属性，则检查节点名称字符串是否和该属性值匹配</li><li>PodFitsHostPorts：如果Pod对象定义了ports.hostPort属性，则检查Pod指定的端口是否已经被节点上的其他容器或服务占用</li><li>MatchNodeSelector：如果Pod对象定义了spec.nodeSelector属性，则检查节点标签是否和该属性匹配</li><li>PodFitsResources：检查节点上的资源(CPU、内存)可用性是否满足Pod对象的运行需求</li></ul></li><li>NoDiskConflict：检查Pod对象请求的存储卷在该节点上可用</li><li>PodToleratesNodeTaints：如果Pod对象中定义了spec.tolerations属性，则需要检查该属性值是否可以接纳节点定义的污点(taints)</li><li>PodToleratesNodeNoExecuteTaints：如果Pod对象定义了spec.tolerations属性，检查该属性是否接纳节点的NoExecute类型的污点</li><li>CheckNodeLabelPresence：仅检查节点上指定的所有标签的存在性，要检查的标签以及其可否存在取决于用户的定义</li><li>CheckServiceAffinity：根据当前Pod对象所属的Service已有其他Pod对象所运行的节点调度，目前是将相同的Service的Pod对象放在同一个或同一类节点上</li><li>CheckVolumeBinding：检查节点上已绑定和未绑定的PVC是否满足Pod对象的存储卷需求</li><li>NoVolumeZoneConflct：在给定了区域限制的前提下，检查在该节点上部署Pod对象是否存在存储卷冲突</li><li>CheckNodeMemoryPressure：在给定了节点已经上报了存在内存资源压力过大的状态，则需要检查该Pod是否可以调度到该节点上</li><li>CheckNodePIDPressure：如果给定的节点已经报告了存在PID资源压力过大的状态，则需要检查该Pod是否可以调度到该节点上</li><li>CheckNodeDiskPressure：如果给定的节点存在磁盘资源压力过大，则检查该Pod对象是否可以调度到该节点上</li><li>MatchInterPodAffinity：检查给定的节点能否可以满足Pod对象的亲和性和反亲和性条件，用来实现Pod亲和性调度或反亲和性调度</li><li>MaxEBSVolumeCount/MaxGCEPDVolumeCount/MaxAzureDiskVolumeCount：云计算存储卷检查</li></ul></li><li>优选算法(<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/algorithm/priorities" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/algorithm/priorities</a>)</li></ul><h4 id="亲和性"><a href="#亲和性" class="headerlink" title="亲和性"></a>亲和性</h4><ul><li>亲和性/反亲和性<ul><li>在出于高效通信的需求，有时需要将一些Pod调度到相近甚至是同一区域位置(比如同一节点、机房、区域)等等，比如业务的前端Pod和后端Pod，此时这些Pod对象之间的关系可以叫做<code>亲和性</code>(<code>affinity</code>)。最终会代替nodeSelector</li><li>同时出于安全性的考虑，也会把一些Pod之间进行隔离，此时这些Pod对象之间的关系叫做<code>反亲和性</code>(<code>anti-affinity</code>)</li></ul></li><li><code>kubectl explain pods.spec.affinity</code></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sq-affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  affinity:</span></span><br><span class="line">    <span class="comment"># 节点亲和性</span></span><br><span class="line"><span class="attr">    nodeAffinity:</span></span><br><span class="line">      <span class="comment"># 硬亲和性：必须满足以下条件的k8s节点才能被调度</span></span><br><span class="line"><span class="attr">      requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">        nodeSelectorTerms:</span> <span class="comment"># 只需满足一个nodeSelectorTerms</span></span><br><span class="line"><span class="attr">        - matchExpressions:</span> <span class="comment"># 必须满足所有matchExpressions(此时匹配节点labels)</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">&#123;key:</span> <span class="string">zone,</span> <span class="attr">operator:</span> <span class="string">In,</span> <span class="attr">values:</span> <span class="string">["sh"]&#125;</span> <span class="comment"># operator：In, NotIn, Exists, DoesNotExist, Gt, Lt</span></span><br><span class="line">      <span class="comment"># 节点软亲和性</span></span><br><span class="line"><span class="attr">      preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">      - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        preference:</span></span><br><span class="line"><span class="attr">          matchExpressions:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">&#123;key:</span> <span class="string">"zone"</span><span class="string">,</span> <span class="attr">operator:</span> <span class="string">In,</span> <span class="attr">values:</span> <span class="string">["cn"]&#125;</span></span><br><span class="line">    <span class="comment"># Pod亲和性</span></span><br><span class="line"><span class="attr">    podAffinity:</span></span><br><span class="line">      <span class="comment"># 硬亲和性</span></span><br><span class="line"><span class="attr">      requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">      - labelSelector:</span></span><br><span class="line"><span class="attr">          matchExpression:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">&#123;key:</span> <span class="string">app,</span> <span class="attr">operator:</span> <span class="string">In,</span> <span class="attr">values:</span> <span class="string">["tomcat"]&#125;</span></span><br><span class="line">        <span class="comment"># 使用哪个键来判断pod的位置信息。此时pods labels key=kubernetes.io/hostname则会按照节点的hostname去判定是否在同一位置区域</span></span><br><span class="line"><span class="attr">        topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">      <span class="comment"># 软亲和性</span></span><br><span class="line"><span class="attr">      preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">      - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        podAffinityTerm:</span></span><br><span class="line"><span class="attr">          labelSelector:</span></span><br><span class="line"><span class="attr">            matchExpressions:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">&#123;key:</span> <span class="string">app,</span> <span class="attr">operator:</span> <span class="string">In,</span> <span class="attr">values:</span> <span class="string">["cache"]&#125;</span></span><br><span class="line"><span class="attr">          topologyKey:</span> <span class="string">zone</span></span><br><span class="line"><span class="attr">      - weight:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">        podAffinityTerm:</span></span><br><span class="line"><span class="attr">          labelSelector:</span></span><br><span class="line"><span class="attr">            matchExpressions:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">&#123;key:</span> <span class="string">app,</span> <span class="attr">operator:</span> <span class="string">In,</span> <span class="attr">values:</span> <span class="string">["db"]&#125;</span></span><br><span class="line"><span class="attr">          topologyKey:</span> <span class="string">zone</span></span><br><span class="line">    <span class="comment"># Pod反亲和性(同上)</span></span><br><span class="line">    <span class="comment">#podAntiAffinity:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-nginx</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">nginx:1.14-alpine</span></span><br></pre></td></tr></table></figure><h4 id="污点-作用于节点上"><a href="#污点-作用于节点上" class="headerlink" title="污点(作用于节点上)"></a>污点(作用于节点上)</h4><ul><li>污点类型<ul><li>NoSchedule：不能容忍此类污点的新Pod对象不能调度到该节点上。<strong>强制约束，节点历史存在的Pod对象不受影响</strong></li><li>PreferNoSchedule：即不能容忍此污点的Pod对象尽量不要调度到该节点，不过无其他节点可以调度时也可以允许接受调度。柔性约束，节点历史存在的Pod对象不受影响</li><li>NoExecute：不能容忍此类污点的新Pod对象不能运行在该节点上。<strong>强制约束，会影响历史存在的Pod</strong></li></ul></li><li><p>命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看示例</span></span><br><span class="line">kubectl describe node node1 <span class="comment"># 查看node1节点污点(Taints)</span></span><br><span class="line">kubectl get nodes node1 -o go-template=&#123;&#123;.spec.taints&#125;&#125; <span class="comment"># 查看污点</span></span><br><span class="line">kubectl describe pods kubernetes-dashboard-5dc4c54b55-ft4xh -n kube-system <span class="comment"># 查看pod容忍污点(Tolerations)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加污点语法：kubectl taint nodes &lt;nodename&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</span></span><br><span class="line"><span class="comment"># 给node1添加污点</span></span><br><span class="line">kubectl taint nodes node1 profile=prod:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment">## 删除语法：kubectl taint nodes &lt;node-name&gt; &lt;key&gt;[: &lt;effect&gt;]-</span></span><br><span class="line">kubectl taint nodes node1 profile:NoSchedule- <span class="comment"># 删除profile键名的NoSchedule类型污点</span></span><br><span class="line">kubectl taint nodes node1 profile- <span class="comment"># 删除指定键名的所有污点</span></span><br></pre></td></tr></table></figure></li><li><p>常见污点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装的master节点对应污点</span></span><br><span class="line">node-role.kubernetes.io/master:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment">## pod容忍的污点</span></span><br><span class="line"><span class="comment"># 污点意思：如果节点包含`node.kubernetes.io/not-ready`污点(节点未准备就绪)，则pod不能在此节点上运行</span></span><br><span class="line"><span class="comment"># 而此时pod容忍此污点，则相当于就算节点未准备就绪，pod也可以在此节点上运行(系统不会到其他节点重新创建pod)，且此忍耐时间为300s(即300s之后节点仍然未就绪，则此k8s会将此pod调度到其他节点)</span></span><br><span class="line">node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 300s <span class="comment"># not-ready为node尚未准备就绪污点</span></span><br><span class="line">node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 300s <span class="comment"># unreachable为node尚不可达污点(如节点kubelet程序挂掉，则会自动加上此污点)</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="pod的容忍度"><a href="#pod的容忍度" class="headerlink" title="pod的容忍度"></a>pod的容忍度</h4><ul><li>查看<code>kubectl explain pods.spec.tolerations</code><ul><li><code>operator</code> 包含<code>Equal</code>和<code>Exists</code>两种类型。如果操作符为Exists，那么value属性可省略，如果不指定operator，则默认为Equal</li><li><code>effect</code> 为上述污点类型</li><li><code>tolerationSeconds</code> 容忍时间(默认是永久性容忍)</li></ul></li><li>添加容忍度配置的pod表示此pod可以接受节点上的相应污点</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...省略pods其他配置</span></span><br><span class="line"><span class="comment"># Equal</span></span><br><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"key1"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Equal"</span></span><br><span class="line"><span class="attr">  value:</span> <span class="string">"value1"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="comment"># Exists：表示此pod可以容忍存在NoExecute类型的node.kubernetes.io/not-ready污点</span></span><br><span class="line"><span class="comment"># 显示成 Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s</span></span><br><span class="line"><span class="attr">- effect:</span> <span class="string">NoExecute</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">node.kubernetes.io/not-ready</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">Exists</span></span><br><span class="line">  <span class="comment"># 如果运行此Pod的Node，被设置了具有NoExecute效果的污点，这个Pod将在存活300s后才被驱逐；如果没有设置tolerationSeconds字段，将永久运行</span></span><br><span class="line"><span class="attr">  tolerationSeconds:</span> <span class="number">300</span></span><br></pre></td></tr></table></figure><ul><li>相关调度失败显示</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># eg: kubectl describe pods my-dev-mysql-6fc6466d86-v5mxd</span></span><br><span class="line"><span class="comment"># 此时调度失败，pod处于 Pending 状态：总共3个有效节点，1个节点存在不可容忍污点，另外2个几点内存不足</span></span><br><span class="line">Warning  FailedScheduling  4m (x145 over 174m)  default-scheduler  0/3 nodes are available: 1 node(s) had taints that the pod didn<span class="string">'t tolerate, 2 Insufficient memory</span></span><br></pre></td></tr></table></figure><h3 id="资源限制及监控"><a href="#资源限制及监控" class="headerlink" title="资源限制及监控"></a>资源限制及监控</h3><h4 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h4><ul><li>资源需求及限制配置</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时 Pod 的服务质量等级是 Burstable</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sq-pod</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line">      <span class="comment"># 此容器的资源需求</span></span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line">        <span class="comment"># 1逻辑CPU=1000millicores，500m=0.5CPU</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"500m"</span></span><br><span class="line">        <span class="comment"># E/P/T/G/M/K(1000); Ei/Pi/...(1024换算)</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"256Mi"</span></span><br><span class="line">      <span class="comment"># 对此容器的资源限制</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"1"</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"512Mi"</span></span><br></pre></td></tr></table></figure><ul><li>QoS(服务质量等级，<code>kubectl describe pods sq-test</code>中的QoS Class自动归类显示)<ul><li><code>Guranteed</code> 所有容器同时设置了CPU和内存的requests和limits，且请求和限制相同，高优先级</li><li><code>Burstable</code> 至少有一个容器设置了CPU或内存的requests属性或limits，中优先级</li><li><code>BestEffort</code> 没有任何容器设置了requests和limits，低优先级(有可能会吞掉太多资源，因此当资源不足时优先关闭此类Pod)</li></ul></li><li>基于HeapSter监控(弃用)<ul><li>Heapster作为kubernetes安装过程中默认安装的一个插件，同时在Horizontal Pod Autoscaling中也用到了，HPA将Heapster作为Resource Metrics API</li><li>Heapster可以收集Node节点上的cAdvisor数据，还可以按照kubernetes的资源类型来集合资源，比如Pod、Namespace域，可以分别获取它们的CPU、内存、网络和磁盘的metric。默认的metric数据聚合时间间隔是1分钟；也可将这些信息存储到InfluxDB，然后结合Grafana进行显示</li><li>Kubernetes 1.11 不建议使用 Heapster，推荐使用metrics-server</li></ul></li><li>推荐监控<ul><li>核心指标流水线：由kubelet、metrics-server以及API server提供的api组成；CPU累计使用率、内存实时占用率、Pod资源占用率、容器磁盘占用率</li><li>监控流水线：用于从系统(Prometheus)收集各种指标数据并提供终端用户、存储系统以及HPA，包含核心指标及许多非核心指标(不能被k8s所解析)</li></ul></li><li><p>metrics-server</p><ul><li>API Server提供了一套api资源，而Mertrics Server也通过了一套api(/apis/metrics.k8s.io/v1beta1)，此时需要将这两套api聚合成一个即kube-aggregator提供用户统一访问</li><li><p>安装metrics-server</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">mkdir metrics-server</span><br><span class="line"><span class="comment"># 下载配置文件</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml ; <span class="keyword">do</span> wget https://github.com/kubernetes/kubernetes/raw/release-1.15/cluster/addons/metrics-server/<span class="variable">$file</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看并更换无法访问镜像地址</span></span><br><span class="line">grep <span class="string">'image: k8s.gcr.io'</span> *</span><br><span class="line">sed -i <span class="string">'s/image: k8s.gcr.io/image: registry.aliyuncs.com\/google_containers/g'</span> *</span><br><span class="line"><span class="comment"># 修改 metrics-server-deployment.yaml 中&#123;&#123;&#125;&#125;信息</span></span><br><span class="line">sed -i <span class="string">'s/--cpu=&#123;&#123; base_metrics_server_cpu &#125;&#125;/--cpu=80m/g'</span> metrics-server-deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/--memory=&#123;&#123; base_metrics_server_memory &#125;&#125;/--memory=80Mi/g'</span> metrics-server-deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/--extra-memory=&#123;&#123; metrics_server_memory_per_node &#125;&#125;Mi/--extra-memory=8Mi/g'</span> metrics-server-deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125;/#- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125;/g'</span> metrics-server-deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/- --kubelet-port=10255/#- --kubelet-port=10255/g'</span> metrics-server-deployment.yaml <span class="comment"># 关闭http的访问，使用tls</span></span><br><span class="line">sed -i <span class="string">'s/- --deprecated-kubelet-completely-insecure=true/#- --deprecated-kubelet-completely-insecure=true/g'</span> metrics-server-deployment.yaml</span><br><span class="line"><span class="comment"># 在 `- --metric-resolution=30s` 后加一行 `- --kubelet-insecure-tls`</span></span><br><span class="line"><span class="comment"># 修改 resource-reader.yaml，在 `- namespaces` 后加一行 `- nodes/stats`</span></span><br><span class="line"><span class="comment"># 修改 resource-reader.yaml，加入 subjectaccessreviews 资源的操作. 参考：https://github.com/kubernetes-incubator/metrics-server/issues/277#issuecomment-514961241</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用所有的配置文件</span></span><br><span class="line">kubectl apply -f .</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get pods -n kube-system <span class="comment"># metrics-server-v0.3.3-867b87bf58-2thdd</span></span><br><span class="line">kubectl api-versions <span class="comment"># 查看API列表，会增加一个 metrics.k8s.io/v1beta1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于top命令查看监控指标</span></span><br><span class="line">kubectl top pods</span><br><span class="line">kubectl top nodes</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h4><p>参考 <a href="/_posts/devops/prometheus.md">Prometheus</a></p><h3 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h3><ul><li>HPA(HorizontalPodAutoscaler，弹性伸缩)目前有<code>v1</code>和<code>v2</code>版。<code>v1</code>版只支持核心指标的缩放，核心指标包括CPU和内存(不可压缩性资源，因此不支持弹性伸缩)</li><li><p>测试案例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建deployment</span></span><br><span class="line">kubectl run sq-hpa --image=nginx:1.14-alpine --replicas=1 --requests=<span class="string">'cpu=50m,memory=256Mi'</span> --limits=<span class="string">'cpu=50m,memory=256Mi'</span> --labels=<span class="string">'app=sq-hpa'</span> --expose --port=80</span><br><span class="line"><span class="comment"># 设置deployment伸缩(hpa)。此处当cpu使用率达到20%则进行扩展</span></span><br><span class="line">kubectl autoscale deployment sq-hpa --min=1 --max=8 --cpu-percent=20</span><br><span class="line"><span class="comment"># 查看hpa设置</span></span><br><span class="line">kubectl get hpa -w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 压力测试</span></span><br><span class="line">kubectl get svc -o wide</span><br><span class="line">yum install httpd-tools</span><br><span class="line">ab -c 100 -n 50000 http://10.106.200.196/</span><br><span class="line"><span class="comment"># 会看到pods个数会增加</span></span><br><span class="line">kubectl describe hpa sq-hpa</span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><ul><li>出现问题 <code>subjectaccessreviews.authorization.k8s.io is forbidden: User \&quot;system:serviceaccount:kube-system:metrics-server\&quot; cannot create resource \&quot;subjectaccessreviews\&quot; in API group \&quot;authorization.k8s.io\&quot; at the cluster scope&quot;</code><ul><li>解决方案：上文<code>metrics-server</code>安装时需要在<code>resource-reader.yaml</code>中加入<code>subjectaccessreviews</code>资源的操作</li></ul></li></ul></li></ul><h2 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h2><h3 id="Pod-Preset"><a href="#Pod-Preset" class="headerlink" title="Pod Preset"></a>Pod Preset</h3><ul><li>参考：<a href="https://k8smeetup.github.io/docs/tasks/inject-data-application/podpreset/" target="_blank" rel="noopener">https://k8smeetup.github.io/docs/tasks/inject-data-application/podpreset/</a></li><li>Pod Preset<ul><li>是一种 API 资源，在 pod 创建时，用户可以用它将额外的运行时需求信息注入 pod</li><li>使用标签选择器(label selector)来指定 Pod Preset 所适用的 pod</li><li>如果Pod Preset注入出错，pod还是正常启动</li><li>中途创建一个Pod Preset，历史运行的pod不会产生变化；但是如果重新创建此pod，则会应用Pod Preset</li><li>删除 Pod Preset，历史注入Pod的配置不会变化；但是如果重新创建此pod，则之前Pod Preset中的配置会丢失?</li></ul></li><li>启用Pod Preset功能(Pod Preset功能处于Alpha阶段)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 修改kube-apiserver启动参数配置，然后重新创建kube-apiserver对应pod</span></span><br><span class="line">vi /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line"><span class="comment"># 修改`- --enable-admission-plugins=NodeRestriction` 为 `- --enable-admission-plugins=NodeRestriction,PodPreset`</span></span><br><span class="line"><span class="comment"># 添加`- --runtime-config=settings.k8s.io/v1alpha1=true`</span></span><br><span class="line"><span class="comment"># 重新启动(会重启coredns。假设之前的为kube-apiserver-node1，则此pod会被修改，并且会新创建一个kube-apiserver，可直接删掉新创建的kube-apiserver)</span></span><br><span class="line">kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line"><span class="comment"># 如果应用失败，导致kubectl get pods等命令无法使用，可重新修改kube-apiserver.yaml并apply</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看资源是否启用</span></span><br><span class="line">kubectl get podpreset <span class="comment"># 提示No resources found.则为正常</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 案例：配置设置时区的Pod Preset，配置文件参考下文</span></span><br><span class="line">vi preset-tz-env.yaml</span><br><span class="line">kubectl apply -f allow-tz-env.yaml</span><br><span class="line"><span class="comment"># for name in $(kubectl get namespace | awk '&#123;print $1&#125;' | grep -v NAME); do kubectl apply -f allow-tz-env.yaml -n $name; done # 在所有命名空间创建</span></span><br><span class="line">kubectl get podpreset</span><br></pre></td></tr></table></figure><ul><li>allow-tz-env.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">settings.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodPreset</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># 被注入的pod会增加一个Annotation如：podpreset.admission.kubernetes.io/podpreset-allow-tz-env: 4735967</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">allow-tz-env</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line">    <span class="comment"># 空表示匹配改namespace下所有pod</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">  env:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">TZ</span></span><br><span class="line"><span class="attr">    value:</span> <span class="string">Asia/Shanghai</span></span><br></pre></td></tr></table></figure><ul><li>如果不希望 pod 被 Pod Preset 所改动，可以在 pod.spec 中添加形如 <code>podpreset.admission.kubernetes.io/exclude: &quot;true&quot;</code> 的注解</li></ul><h3 id="Admission-Controller-准入控制"><a href="#Admission-Controller-准入控制" class="headerlink" title="Admission Controller 准入控制"></a>Admission Controller 准入控制</h3><ul><li>使用<code>--enable-admission-plugins</code>启用<code>Admission Controller</code>(之前k8s版本使用<code>--admission-control</code>)，案例参考<code>Pod Preset</code></li></ul><h3 id="Admission-Webhook"><a href="#Admission-Webhook" class="headerlink" title="Admission Webhook"></a>Admission Webhook</h3><h2 id="高级知识"><a href="#高级知识" class="headerlink" title="高级知识"></a>高级知识</h2><h3 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h3><ul><li>启动配置参数：<a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/</a></li></ul><h2 id="小知识点"><a href="#小知识点" class="headerlink" title="小知识点"></a>小知识点</h2><h3 id="k8s时区问题"><a href="#k8s时区问题" class="headerlink" title="k8s时区问题"></a>k8s时区问题</h3><ul><li>基于Docker方式(每个pod都需要设置) <a href="https://zhuanlan.zhihu.com/p/44269163" target="_blank" rel="noopener">^9</a><ul><li>设置容器的时区环境变量</li><li>挂载主机的时区文件到容器中</li></ul></li><li>通过K8s资源PodPreset进行预设置，参考<a href="#Pod%20Preset">Pod Preset</a></li></ul><h2 id="辅助组件使用"><a href="#辅助组件使用" class="headerlink" title="辅助组件使用"></a>辅助组件使用</h2><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li><p>kubectl 命令自动补全</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bash-completion</span><br><span class="line"># locate bash_completion/usr/share/bash-completion/bash_completion</span><br><span class="line">source /usr/share/bash-completion/bash_completion</span><br><span class="line">source &lt;(kubectl completion bash)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://github.com/c-bata/kube-prompt" target="_blank" rel="noopener">kube-prompt</a> 交互式 Kubernetes 客户端</p><ul><li><p>不必键入kubectl来为每个命令添加前缀，并为每个命令提供自动完成功能以及上下文信息，且有自动提示小窗口。相同的工具如<code>kube-shell</code>(需要升级python)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/c-bata/kube-prompt/releases/download/v1.0.3/kube-prompt_v1.0.3_linux_amd64.zip</span><br><span class="line"><span class="comment"># yum -y install unzip</span></span><br><span class="line">unzip kube-prompt_v1.0.3_linux_amd64.zip <span class="comment"># 就一个可执行文件</span></span><br><span class="line"><span class="comment"># 给 kube-prompt 加上执行权限并移动常用的可搜索路径。</span></span><br><span class="line">chmod +x kube-prompt</span><br><span class="line">sudo mv ./kube-prompt /usr/<span class="built_in">local</span>/bin/kube-prompt</span><br><span class="line"><span class="comment"># 进入kube-prompt命令行</span></span><br><span class="line">kube-prompt</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>Kubectl Aliases</code> 是一个通过编程方式生成的 Kubectl 别名脚本</p></li></ul><h3 id="Helm-参考"><a href="#Helm-参考" class="headerlink" title="Helm 参考"></a>Helm 参考</h3><ul><li><a href="/_posts/devops/helm.md">http://blog.aezo.cn/2019/06/22/devops/helm/</a></li></ul><h3 id="手动安装Dashboard"><a href="#手动安装Dashboard" class="headerlink" title="手动安装Dashboard"></a>手动安装Dashboard</h3><ul><li>推荐使用helm安装，具体参考<a href="/_posts/devops/helm.md#dashboard">helm.md</a></li><li><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">github</a></li></ul><h4 id="Dashboard界面说明"><a href="#Dashboard界面说明" class="headerlink" title="Dashboard界面说明"></a>Dashboard界面说明</h4><ul><li>部署(Deployment)<ul><li>伸缩：如果将伸缩值设置为0，则会移除所有Pod，此时不提供服务；当将伸缩设置为1，会自动重新创建Pod并对外提供服务</li></ul></li><li>容器组(Pod)<ul><li>容器组命令行界面中可执行的命令比<code>kubectl exec</code>进入Pod可执行的命令要多，如<code>ll</code>一般Pod中都无此命令，但是Dashboard可执行</li></ul></li><li>副本集(ReplicaSet)</li><li>服务(Service)</li><li>配置与存储(Secret)</li><li><strong>有时候存在界面上资源的状态显示和命令行不一致</strong></li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 安装</span></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span><br><span class="line"><span class="comment"># 修改Dashboard Service配置为NodePort，伪代码如：添加spec.ports[0].nodePort=30000，添加spec.type=NodePort。一般可通过Ingress暴露出来</span></span><br><span class="line"><span class="comment"># 修改容器镜像 `k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1` 为 `registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1`</span></span><br><span class="line"><span class="comment"># 修改token过期时间，默认是15分钟(900秒)：在 `- --auto-generate-certificates` 下加一行参数 `- --token-ttl=31536000‬` (1年有效)</span></span><br><span class="line">vi kubernetes-dashboard.yaml</span><br><span class="line"><span class="comment"># 创建资源(pod运行在master节点上)</span></span><br><span class="line">kubectl apply -f kubernetes-dashboard.yaml</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get pods -n kube-system</span><br><span class="line"><span class="comment"># 访问 https://192.168.6.131:30000/ 可显示登录页面(仅火狐浏览器支持，下文可解决)，登录秘钥获取见下文</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置https证书</span></span><br><span class="line"><span class="comment"># 生成私钥和证书签名</span></span><br><span class="line">openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048</span><br><span class="line">openssl rsa -passin pass:x -<span class="keyword">in</span> dashboard.pass.key -out dashboard.key</span><br><span class="line">rm dashboard.pass.key</span><br><span class="line">openssl req -new -key dashboard.key -out dashboard.csr <span class="comment"># 一路回车即可</span></span><br><span class="line">openssl x509 -req -sha256 -days 365 -<span class="keyword">in</span> dashboard.csr -signkey dashboard.key -out dashboard.crt <span class="comment"># 生成SSL证书</span></span><br><span class="line"><span class="comment"># mkdir /var/share/certs/ -p &amp;&amp; cp dashboard.key dashboard.crt /var/share/certs/ # 可对私钥和密码进行保存</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 重新创建dashboard默认证书(默认证书导致只能通过火狐浏览器访问，此步骤可解决此问题)</span></span><br><span class="line">kubectl delete secret kubernetes-dashboard-certs -n kube-system <span class="comment"># 删除原有的证书secret</span></span><br><span class="line">kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key=./dashboard.key --from-file=./dashboard.crt -n kube-system <span class="comment"># 创建新的证书secret</span></span><br><span class="line">kubectl get pod -n kube-system <span class="comment"># 查看pod</span></span><br><span class="line">kubectl delete pod kubernetes-dashboard-5dc4c54b55-ft4xh -n kube-system <span class="comment"># 按情况删除pod(会自动重新创建)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建 ServiceAccount 和 ClusterRoleBinding</span></span><br><span class="line"><span class="comment"># 创建serviceaccount</span></span><br><span class="line">kubectl create serviceaccount sa-admin -n kube-system</span><br><span class="line"><span class="comment"># service account账户绑定到集群角色admin，dev:cluster-sa-admin为绑定名称(可随便取)</span></span><br><span class="line">kubectl create clusterrolebinding dev:cluster-sa-admin --clusterrole=cluster-admin --serviceaccount=kube-system:sa-admin</span><br><span class="line"></span><br><span class="line"><span class="comment">## 法一：基于token登录</span></span><br><span class="line"><span class="comment"># 查看 ServiceAccount 对应的 Secret token。复制此token以令牌形式登录 https://192.168.6.131:30000/ 即可</span></span><br><span class="line">kubectl get secret $(kubectl get secret -n kube-system|grep sa-admin-token|awk <span class="string">'&#123;print $1&#125;'</span>) -n kube-system -o jsonpath=&#123;.data.token&#125;|base64 -d |xargs <span class="built_in">echo</span></span><br><span class="line"><span class="comment">## 法二：基于kubeconfig文件登录</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --server=<span class="string">"https://192.168.6.131:6443"</span> --embed-certs=<span class="literal">true</span> --kubeconfig=./cluster-sa-admin.conf</span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials sa-admin --token=$(kubectl get secret $(kubectl get secret -n kube-system|grep sa-admin-token|awk <span class="string">'&#123;print $1&#125;'</span>) -n kube-system -o jsonpath=&#123;.data.token&#125;|base64 -d |xargs <span class="built_in">echo</span>) --kubeconfig=./cluster-sa-admin.conf</span><br><span class="line">kubectl config <span class="built_in">set</span>-context sa-admin@kubernetes --cluster=kubernetes --user=sa-admin --kubeconfig=./cluster-sa-admin.conf</span><br><span class="line">kubectl config use-context sa-admin@kubernetes --kubeconfig=./cluster-sa-admin.conf</span><br><span class="line"><span class="comment"># 下载 ./cluster-sa-admin.conf 文件到宿主机，登录选择此文件即可</span></span><br></pre></td></tr></table></figure><h2 id="运维案例"><a href="#运维案例" class="headerlink" title="运维案例"></a>运维案例</h2><ul><li>扩容PVC，参考<a href="/_posts/devops/ceph.md#镜像扩容缩容(rbd-images">ceph.md#镜像扩容缩容(rbd-images)</a>)</li><li>k8s证书过期<ul><li>由 kubeadm 生成的客户端证书在 1 年后到期。<code>kubeadm alpha certs check-expiration</code> 查看所有证书过期时间</li><li>手动更新证书，参考：<a href="https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired/56334732#56334732" target="_blank" rel="noopener">https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired/56334732#56334732</a> 、 <a href="https://feisky.gitbooks.io/kubernetes/content/practice/certificate-rotation.html" target="_blank" rel="noopener">https://feisky.gitbooks.io/kubernetes/content/practice/certificate-rotation.html</a></li><li>或者升级K8S则会自动更新证书</li><li>可设置证书有效期</li></ul></li></ul><h2 id="常见问题-1"><a href="#常见问题-1" class="headerlink" title="常见问题"></a>常见问题</h2><ul><li>日志查看<ul><li><code>sudo journalctl -u kubelet -f -n 100</code> <strong>查看对应节点kubelet日志</strong></li><li><code>sudo journalctl -u docker -f -n 100</code> <strong>查看对应节点docker日志</strong></li></ul></li><li>相关目录<ul><li><code>/var/lib/kubelet/pods/</code> 节点中pod存放位置，里面基于pod-id存放，此id有时会出现在journalctl日志中</li></ul></li></ul><h3 id="nodes"><a href="#nodes" class="headerlink" title="nodes"></a>nodes</h3><ul><li><code>kubectl get nodes</code> 显示Node状态为NotReady<ul><li>查看对应节点的网络插件(Pod)是否正常启动</li><li>查看对应节点服务状态<code>systemctl status kubelet/docker</code></li><li><code>sudo journalctl -u kubelet -f -n 100</code> 查看对应节点kubelet日志</li></ul></li><li><p>Kubernetes报错<code>Failed to get system container stats for &quot;/system.slice/kubelet.service&quot;</code>。解决如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参考：https://stackoverflow.com/questions/46726216/kubelet-fails-to-get-cgroup-stats-for-docker-and-kubelet-services</span></span><br><span class="line">vi /etc/sysconfig/kubelet</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">DAEMON_ARGS=--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure></li><li><p>kubelet报错<code>orphaned pod &quot;501454ff-c11c-4fd0-8ca0-5c89263399de&quot; found, but volume paths are still present on disk</code>(对整体使用影响不大) <a href="https://www.jianshu.com/p/a67316ee0288" target="_blank" rel="noopener">^11</a></p><ul><li>解决：root执行 <code>bash &lt;(curl -L https://raw.githubusercontent.com/oldinaction/scripts/master/k8s/prod/kubelet-issues-solution.sh)</code></li><li>查看所有问题podid <code>cat /var/log/messages|grep &#39;orphaned pod&#39;|awk -F &#39;&quot;&#39; &#39;{print $2}&#39;|uniq</code></li></ul></li><li><p>node节点磁盘<code>/var/lib/docker/overlay2</code>目录占用较高，导致node状态为<code>NotReady</code> <a href="https://www.cnblogs.com/snooker/p/10963377.html" target="_blank" rel="noopener">^14</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker system df <span class="comment"># 查看docker磁盘使用情况，RECLAIMABLE 列为可收回的</span></span><br><span class="line">docker system prune <span class="comment"># 清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)</span></span><br><span class="line">docker system prune -a <span class="comment"># 慎用。命令清理得更加彻底，会把没有开启的容器，以及暂时没有用到的Docker镜像都删掉了</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h3><ul><li><p><strong>pod常见日志顺序</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pod has unbound immediate PersistentVolumeClaims (repeated 3 times) # 有时可能出现几次此种报错，如果一直不打印下一行assigned日志则确实有问题</span></span><br><span class="line"></span><br><span class="line">Successfully assigned devops/mongodb-devops-7d556f9578-4gjmz to dev2-1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unable to mount volumes for pod "mysql-devops-5b7d797b9b-q5jmv_devops(110dbfea-067b-4477-a6d5-e723ba6d5adb)": timeout expired waiting for volumes to attach or mount for pod "devops"/"mysql-devops-5b7d797b9b-q5jmv". list of unmounted volumes=[data]. list of unattached volumes=[configurations migrations data default-token-bs8fb] # 报错日志，尝试删除所有历史副本集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MountVolume.WaitForAttach failed for volume "pvc-bc0366a4-56d4-4a42-a610-e3d7075499b1" : rbd image kube/kubernetes-dynamic-pvc-216c730c-2555-11ea-93a3-8ab700667926 is still being used # 报错日志</span></span><br><span class="line"></span><br><span class="line">AttachVolume.Attach succeeded <span class="keyword">for</span> volume <span class="string">"pvc-b9668e8c-6564-4084-9192-d431393ff201"</span></span><br><span class="line"></span><br><span class="line">Pulling image <span class="string">"docker.io/bitnami/mongodb:4.2.6"</span></span><br><span class="line"></span><br><span class="line">Successfully pulled image <span class="string">"docker.io/bitnami/mongodb:4.2.6"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Container image "docker.io/bitnami/mongodb:4.2.6" already present on machine # 如果镜像原本存在则打印此行</span></span><br><span class="line"></span><br><span class="line">Created container mongodb-devops</span><br><span class="line"></span><br><span class="line">Started container mongodb-devops</span><br></pre></td></tr></table></figure></li><li><p>一直CrashLoopBackOff，且describe显示<code>Back-off restarting failed container</code></p><ul><li>可查看对应pod的日志</li></ul></li><li>报错<code>Back-off restarting failed container</code><ul><li>可在Deploy中(实际是Pod)覆盖镜像的command，即加<code>command: [ &quot;/bin/sh&quot;, &quot;-ce&quot;, &quot;sleep 1h&quot; ]</code>(-c参数中命令可以使用<code>\n</code>进行换行)从而先进入容器，然后手动启动，并查看日志</li></ul></li><li>报错<code>Multi-Attach error for volume &quot;pvc-bc0366a4-56d4-4a42-a610-e3d7075499b1&quot; Volume is already exclusively attached to one node and can&#39;t be attached to another</code> <a href="https://fengxsong.github.io/2018/05/30/%E8%8A%82%E7%82%B9%E5%A5%94%E6%BA%83%E9%87%8D%E5%90%AF%E5%90%8E%E9%83%A8%E5%88%86pvc%E4%B8%8D%E8%83%BD%E6%AD%A3%E5%B8%B8%E6%8C%82%E8%BD%BD/" target="_blank" rel="noopener">^10</a><ul><li>手动移除rbd image watcher，参考：<a href="/_posts/devops/ceph.md#常见问题">ceph.md#常见问题(无法删除镜像)</a></li><li>将原rbd观察者加入到黑名单后，新的观察者即可自动添加(历史数据不会丢失)</li><li>有时候无watcher，但是提示被其他节点占用，后来发现改副本集包含一个卡在Running状态的pod，因此重新创建pod一致失败。此时可以考虑将副本集删除掉会自动创建新副本集</li><li>有时候删掉副本集之后，还是无法创建成功，反而会出现两个Pod（老的一个Pod在Dashboard上显示Running，在命令行显示Terminating）。此时可尝试在命令行强制删除老的Pod，之后可考虑重新创建副本集</li><li>对于ReadWriteOnce类似的PVC，可设置Deployment的strategy=Recreate，或设置strategy=RollingUpdate和strategy.rollingUpdate.maxSurge=0(表示滚动更新时不创建额外的pod，其实就是禁止滚动更新)</li></ul></li><li>报错<code>MountVolume.WaitForAttach failed for volume &quot;pvc-bc0366a4-56d4-4a42-a610-e3d7075499b1&quot; : rbd image kube/kubernetes-dynamic-pvc-216c730c-2555-11ea-93a3-8ab700667926 is still being used</code>。解决方案同上</li><li>报错<code>Unable to attach or mount volumes: unmounted volumes</code>。解决方案同上</li><li>报错<code>pod has unbound immediate PersistentVolumeClaims</code><ul><li>情况一：此时pod日志显示<code>AttachVolume.Attach succeeded for volume &quot;pvc-b9668e8c-6564-4084-9192-d431393ff201&quot;</code>，且PV和PVC均正常显示Bound(PV也符合PVC的要求)。后发现pod日志只显示<code>Pulling image &quot;docker.io/bitnami/mongodb:4.2.6&quot;</code>，并未显示<code>Successfully pulled image &quot;docker.io/bitnami/mongodb:4.2.6&quot;</code>，后发现对接节点确实没有相应镜像，由此推断镜像获取失败导致</li><li>情况二：一直卡在<code>pod has unbound immediate PersistentVolumeClaims</code>，无后续日志 <a href="https://blog.csdn.net/pencc/article/details/84333315" target="_blank" rel="noopener">^12</a> <a href="https://fengxsong.github.io/2018/05/30/%E8%8A%82%E7%82%B9%E5%A5%94%E6%BA%83%E9%87%8D%E5%90%AF%E5%90%8E%E9%83%A8%E5%88%86pvc%E4%B8%8D%E8%83%BD%E6%AD%A3%E5%B8%B8%E6%8C%82%E8%BD%BD/" target="_blank" rel="noopener">^10</a><ul><li>出现场景：pod之前正常运行，但是某时刻该节点震荡，导致此pod不可用，并自动创建了新pod；且pv是基于StorageClass从ceph请求存储空间；且创建pod时pvc申请策略为ReadWriteOnce</li><li>原因分析：由于ceph只支持ReadWriteOnce模式，便将pvc设置成了ReadWriteOnce；而此时滚动更新时会产生多一个pod，而ReadWriteOnce的访问模式又不允许两个pod挂载同一个volume</li><li>解决：RollingUpdate模式下设置strategy.rollingUpdate.maxSurge=0</li></ul></li></ul></li><li>报错<code>MountVolume.MountDevice failed for volume &quot;pvc-bc0366a4-56d4-4a42-a610-e3d7075499b1&quot; : rbd: failed to mount device /dev/rbd5 at /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kube-image-kubernetes-dynamic-pvc-216c730c-2555-11ea-93a3-8ab700667926 (fstype: ext4), error &#39;fsck&#39; found errors on device /dev/rbd5 but could not correct them: fsck from util-linux 2.23.2 /dev/rbd5: recovering journal /dev/rbd5 contains a file system with errors, check forced. /dev/rbd5: Unconnected directory inode 131149 (/???) /dev/rbd5: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY. (i.e., without -a or -p options)</code><ul><li>进入到对应节点，手动执行<code>fsck /dev/rbd5</code>进行磁盘检查。注意进行数据备份</li></ul></li></ul><h3 id="pv-pvc"><a href="#pv-pvc" class="headerlink" title="pv/pvc"></a>pv/pvc</h3><ul><li><p>pv/pvc一直无法删除，force也无法删除。解决办法如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch pv my-pv -p <span class="string">'&#123;"metadata":&#123;"finalizers":null&#125;&#125;'</span></span><br><span class="line">kubectl patch pvc my-pvc -p <span class="string">'&#123;"metadata":&#123;"finalizers": []&#125;&#125;'</span> --<span class="built_in">type</span>=merge -n default</span><br></pre></td></tr></table></figure></li></ul><hr><p>参考文章</p></div><div></div><div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> smalle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="http://blog.aezo.cn/2019/06/01/devops/kubernetes/" title="Kubernetes">http://blog.aezo.cn/2019/06/01/devops/kubernetes/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/k8s/" rel="tag"># k8s</a> <a href="/tags/docker/" rel="tag"># docker</a> <a href="/tags/cncf/" rel="tag"># cncf</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/05/30/devops/virtual-server-cloud/" rel="next" title="虚拟化服务器搭建 | 私有云"><i class="fa fa-chevron-left"></i> 虚拟化服务器搭建 | 私有云</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2019/06/20/linux/network/" rel="prev" title="网络">网络<i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400" alt="smalle"><p class="site-author-name" itemprop="name">smalle</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">155</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">140</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#背景"><span class="nav-number">1.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#概念"><span class="nav-number">1.2.</span> <span class="nav-text">概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K8s集群安装"><span class="nav-number">2.</span> <span class="nav-text">K8s集群安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于kubeadm安装k8s"><span class="nav-number">2.1.</span> <span class="nav-text">基于kubeadm安装k8s</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kubelet-说明"><span class="nav-number">2.2.</span> <span class="nav-text">kubelet 说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见问题"><span class="nav-number">2.3.</span> <span class="nav-text">常见问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#命令使用"><span class="nav-number">3.</span> <span class="nav-text">命令使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kubectl"><span class="nav-number">3.1.</span> <span class="nav-text">kubectl</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基础知识"><span class="nav-number">4.</span> <span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#资源"><span class="nav-number">4.1.</span> <span class="nav-text">资源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#资源配置文件字段说明"><span class="nav-number">4.1.1.</span> <span class="nav-text">资源配置文件字段说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简单示例"><span class="nav-number">4.1.2.</span> <span class="nav-text">简单示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod"><span class="nav-number">4.2.</span> <span class="nav-text">Pod</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#控制器"><span class="nav-number">4.3.</span> <span class="nav-text">控制器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deployment-amp-ReplicaSet-amp-ReplicationController"><span class="nav-number">4.3.1.</span> <span class="nav-text">Deployment &amp; ReplicaSet &amp; ReplicationController</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#StatefulSet-管理有状态副本集"><span class="nav-number">4.3.2.</span> <span class="nav-text">StatefulSet(管理有状态副本集)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Service"><span class="nav-number">4.4.</span> <span class="nav-text">Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ingress-amp-Ingress-Control-3"><span class="nav-number">4.5.</span> <span class="nav-text">Ingress &amp; Ingress Control ^3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ingress-Controller部署-以ingress-nginx为例"><span class="nav-number">4.5.1.</span> <span class="nav-text">Ingress Controller部署(以ingress-nginx为例)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建Ingress示例"><span class="nav-number">4.5.2.</span> <span class="nav-text">创建Ingress示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#构建TLS站点示例"><span class="nav-number">4.5.3.</span> <span class="nav-text">构建TLS站点示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#存储卷"><span class="nav-number">4.6.</span> <span class="nav-text">存储卷</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#emptyDir案例"><span class="nav-number">4.6.1.</span> <span class="nav-text">emptyDir案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PVC、PV、NFS配合使用案例"><span class="nav-number">4.6.2.</span> <span class="nav-text">PVC、PV、NFS配合使用案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络"><span class="nav-number">4.7.</span> <span class="nav-text">网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DNS"><span class="nav-number">4.7.1.</span> <span class="nav-text">DNS</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#访问API-Server认证"><span class="nav-number">4.8.</span> <span class="nav-text">访问API Server认证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调度器"><span class="nav-number">4.9.</span> <span class="nav-text">调度器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#亲和性"><span class="nav-number">4.9.1.</span> <span class="nav-text">亲和性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#污点-作用于节点上"><span class="nav-number">4.9.2.</span> <span class="nav-text">污点(作用于节点上)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pod的容忍度"><span class="nav-number">4.9.3.</span> <span class="nav-text">pod的容忍度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#资源限制及监控"><span class="nav-number">4.10.</span> <span class="nav-text">资源限制及监控</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#资源限制"><span class="nav-number">4.10.1.</span> <span class="nav-text">资源限制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prometheus"><span class="nav-number">4.10.2.</span> <span class="nav-text">Prometheus</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HPA"><span class="nav-number">4.11.</span> <span class="nav-text">HPA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#进阶知识"><span class="nav-number">5.</span> <span class="nav-text">进阶知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pod-Preset"><span class="nav-number">5.1.</span> <span class="nav-text">Pod Preset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Admission-Controller-准入控制"><span class="nav-number">5.2.</span> <span class="nav-text">Admission Controller 准入控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Admission-Webhook"><span class="nav-number">5.3.</span> <span class="nav-text">Admission Webhook</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高级知识"><span class="nav-number">6.</span> <span class="nav-text">高级知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#API-Server"><span class="nav-number">6.1.</span> <span class="nav-text">API Server</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小知识点"><span class="nav-number">7.</span> <span class="nav-text">小知识点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#k8s时区问题"><span class="nav-number">7.1.</span> <span class="nav-text">k8s时区问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#辅助组件使用"><span class="nav-number">8.</span> <span class="nav-text">辅助组件使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips"><span class="nav-number">8.1.</span> <span class="nav-text">Tips</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Helm-参考"><span class="nav-number">8.2.</span> <span class="nav-text">Helm 参考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#手动安装Dashboard"><span class="nav-number">8.3.</span> <span class="nav-text">手动安装Dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dashboard界面说明"><span class="nav-number">8.3.1.</span> <span class="nav-text">Dashboard界面说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#安装"><span class="nav-number">8.3.2.</span> <span class="nav-text">安装</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运维案例"><span class="nav-number">9.</span> <span class="nav-text">运维案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常见问题-1"><span class="nav-number">10.</span> <span class="nav-text">常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nodes"><span class="nav-number">10.1.</span> <span class="nav-text">nodes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pod"><span class="nav-number">10.2.</span> <span class="nav-text">pod</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pv-pvc"><span class="nav-number">10.3.</span> <span class="nav-text">pv/pvc</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2016 - <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">smalle</span>&nbsp;&nbsp;&nbsp;&nbsp;<div class="powered-by"> 由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="powered-by"> 主题 - <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="theme-info cnzz" style="margin:0 0 -5px 10px"><script type="text/javascript">var cnzz_protocol="https:"==document.location.protocol?"https://":"http://";document.write(unescape("%3Cspan id='cnzz_stat_icon_1276691827'%3E%3C/span%3E%3Cscript src='"+cnzz_protocol+"s23.cnzz.com/z_stat.php%3Fid%3D1276691827%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"))</script></div></div><div class="ad"> <span style="font-weight:700">AD&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=oby5nolb">阿里云大礼包</a></div></div><div class="aezocn"> <span style="font-weight:700">@AEZO.CN&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="http://shop.aezo.cn/">杂货铺</a></div></div><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.1"></script></body></html>