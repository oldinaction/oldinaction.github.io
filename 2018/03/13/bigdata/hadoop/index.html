<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="hadoop,"><link rel="alternate" href="/atom.xml" title="Smalle's Blog | AEZOCN" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="简介 Hadoop([hædu:p])作者Doug cutting，名字来源于Doug Cutting儿子的玩具大象 模块 HDFS(Hadoop Distributed File System) 分布式存储系统 Hadoop MapReduce 分布式计算框架 Hadoop YARN 资源管理系统(Hadoop 2.x才有) Hadoop Common   网址 官网 r1.0.4中文文档 r2"><meta name="keywords" content="hadoop"><meta property="og:type" content="article"><meta property="og:title" content="Hadoop"><meta property="og:url" content="http://blog.aezo.cn/2018/03/13/bigdata/hadoop/index.html"><meta property="og:site_name" content="Smalle&#39;s Blog | AEZOCN"><meta property="og:description" content="简介 Hadoop([hædu:p])作者Doug cutting，名字来源于Doug Cutting儿子的玩具大象 模块 HDFS(Hadoop Distributed File System) 分布式存储系统 Hadoop MapReduce 分布式计算框架 Hadoop YARN 资源管理系统(Hadoop 2.x才有) Hadoop Common   网址 官网 r1.0.4中文文档 r2"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-hdfs-write.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-hdfs-read.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-ha.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-federation.gif"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-mp.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-mp-detail.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-mp-demo.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-jt-tt.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-yarn.png"><meta property="og:updated_time" content="2021-08-31T06:07:20.334Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Hadoop"><meta name="twitter:description" content="简介 Hadoop([hædu:p])作者Doug cutting，名字来源于Doug Cutting儿子的玩具大象 模块 HDFS(Hadoop Distributed File System) 分布式存储系统 Hadoop MapReduce 分布式计算框架 Hadoop YARN 资源管理系统(Hadoop 2.x才有) Hadoop Common   网址 官网 r1.0.4中文文档 r2"><meta name="twitter:image" content="http://blog.aezo.cn/data/images/bigdata/hadoop-hdfs-write.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"BWD6R9FA4K",apiKey:"3330f3cbaa099dfc30395de5f5b20151",indexName:"blog",hits:{per_page:10},labels:{input_placeholder:"请输入关键字",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}}}</script><link rel="canonical" href="http://blog.aezo.cn/2018/03/13/bigdata/hadoop/"><title>Hadoop | Smalle's Blog | AEZOCN</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div style="display:none"><script src="//s95.cnzz.com/z_stat.php?id=cnzz_stat_icon_1276691827&web_id=cnzz_stat_icon_1276691827" language="JavaScript"></script></div></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Smalle's Blog | AEZOCN</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Better Code, Better Life</h1></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br> 站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.aezo.cn/2018/03/13/bigdata/hadoop/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="smalle"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Smalle's Blog | AEZOCN"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Hadoop</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-13T20:31:00+08:00">2018-03-13</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><div></div><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li><code>Hadoop</code>([hædu:p])作者<code>Doug cutting</code>，名字来源于Doug Cutting儿子的玩具大象</li><li>模块<ul><li><code>HDFS</code>(Hadoop Distributed File System) 分布式存储系统</li><li><code>Hadoop MapReduce</code> 分布式计算框架</li><li><code>Hadoop YARN</code> 资源管理系统(Hadoop 2.x才有)</li><li><code>Hadoop Common</code></li></ul></li><li>网址<ul><li><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">官网</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/index.html" target="_blank" rel="noopener">r1.0.4中文文档</a></li><li><a href="http://hadoop.apache.org/docs/r2.10.1/" target="_blank" rel="noopener">r2.10.1文档</a></li></ul></li><li>谷歌论文(理论来源)<ul><li>《The Google File System》 2003年</li><li>《MapReduce: Simplified Data Processing on Large Clusters》 2004年</li><li>《Bigtable: A Distributed Storage System for Structured Data》 2006年</li></ul></li><li>版本：2016年10月hadoop-2.6.5，2017年12月hadoop-3.0.0</li><li><a href="http://www.cloudera.com" target="_blank" rel="noopener">大数据生态CDH提供商</a></li></ul><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="HDFS基础概念"><a href="#HDFS基础概念" class="headerlink" title="HDFS基础概念"></a>HDFS基础概念</h3><ul><li>HDFS优缺点<ul><li>优点：高容错性(自动保存副本，自动恢复)、适合批处理、适合大数据(TB/PB)处理、可构建在廉价机器上</li><li>缺点：占用内存大、修改文件成本过高</li></ul></li><li>存储模型<ul><li>文件线性按字节切割成块(block)，具有offset，id</li><li>文件与文件的block大小可以不一样</li><li>一个文件除最后一个block，其他block大小一致</li><li>block的大小依据硬件的I/O特性调整</li><li>block被分散存放在集群的节点中，具有location</li><li>block具有副本(replication)，没有主从概念，副本不能出现在同一个节点。副本是满足可靠性和性能的关键</li><li>文件上传可以指定block大小和副本数，上传后只能修改副本数</li><li>一次写入，多次读取，不支持修改</li><li>支持追加数据</li><li>数据块/数据存储单元(Block)说明<ul><li>文件被切分成固定大小的数据块，数据块默认大小为128MB(Hadoop 1.x默认为64M，后来发展为256M，可调整)。若文件大小不到128MB，则单独存成一个Block</li><li>一个文件存储方式：按大小被切分成若干个Block，存储到不同节点上。默认情况下每个Block都有三个副本</li><li>block大小和副本数通过Client端上传文件时设置，文件上传成功后副本数可以变更，Block Size不可变更</li></ul></li></ul></li><li>架构设计<ul><li>HDFS是一个主从(Master/Slaves)架构</li><li>由一个NameNode和一些DataNode组成</li><li>面向文件，包含：文件数据(data)和文件元数据(metadata)</li><li>NameNode负责存储和管理文件元数据，并维护了一个层次型的文件目录树</li><li>DataNode负责存储文件数据(block块)，并提供block的读写</li><li>DataNode与NameNode维持心跳，并汇报自己持有的block信息</li><li>Client和NameNode交互文件元数据和DataNode交互文件block数据</li></ul></li><li>角色功能<ul><li><code>NameNode</code>(NN)<ul><li>NameNode主要功能<ul><li><strong>完全基于内存存储文件<code>metadate</code>元数据、目录结构、文件block的映射</strong><ul><li>需要持久化方案保证数据可靠性，持久化方案为 fsimage 和 edits 结合(类似redis的镜像日志和普通日志)</li><li>提供副本放置策略</li></ul></li><li>接受客户端的读写服务</li></ul></li><li>NameNode的metadate信息说明<ul><li>元数据信息存储形式为 <strong><code>fsimage</code>和<code>edits</code></strong><ul><li><code>fsimage</code> metadata存储到磁盘文件名为fsimage(format格式化的时候产生)</li><li><code>edits</code> 记录对metadata的操作日志(客户端读写日志)，会自动合并到fsimage中</li><li>以上两个文件主要记录了文件包含哪些block，block保存在哪个DataNode(由DataNode启动时上报)</li></ul></li><li>NameNode的metadate信息在启动后会加载到内存</li><li>Block的位置信息不会保存到fsimage(NN将block位置存放到内存中)</li></ul></li></ul></li><li><code>DataNode</code>(DN)<ul><li>基于本地磁盘存储Block(文件的形式)</li><li>并保存block的校验和数据保证block的可靠性</li><li>与NameNode保持心跳，汇报block列表状态<ul><li>启动DN线程的时候会向NN汇报block信息(NN将block位置存放到内存中)</li><li>通过向NN发送心跳保持与其联系（3秒一次），如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN</li></ul></li></ul></li><li><code>SecondaryNameNode</code>(SNN, 非HA模式才有)<ul><li>它不是NN的备份，也不是HA，它的主要工作是帮助NN合并edits日志，减少NN启动时间</li><li>SNN执行合并时机：根据配置文件设置的时间间隔<code>fs.checkpoint.period</code>默认3600秒；根据配置文件设置edits日志大小 <code>fs.checkpoint.size</code>规定edits文件的最大值默认是64MB。超过时间或到达日志大小则合并</li></ul></li></ul></li><li>元数据持久化<ul><li>NameNode使用了FsImage(镜像) + EditLog(日志)整合的方案。类似redis的持久化方式<ul><li>HDFS搭建时会格式化，格式化操作会产生一个空的FsImage</li><li>当Namenode启动时，它从硬盘中读取Editlog和FsImage</li><li>将所有Editlog中的事务作用在内存中的FsImage上</li><li>并将这个新版本的FsImage从内存中保存到本地磁盘上</li><li>然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了</li></ul></li></ul></li><li>安全模式<ul><li>Namenode启动后会进入一个称为安全模式的特殊状态</li><li>处于安全模式的Namenode是不会进行数据块的复制的</li><li>Namenode从所有的 Datanode接收心跳信号和块状态报告</li><li>每当Namenode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的</li><li>在一定百分比（这个参数可配置）的数据块被Namenode检测确认是安全之后（加上一个额外的30秒等待时间），Namenode将退出安全模式状态</li><li>接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他Datanode上</li></ul></li><li>Block的副本放置策略<ul><li>第一个副本：放置在上传文件的DN；如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点</li><li>第二个副本：放置在与第一个副本不同机架的节点上</li><li>第三个副本：与第二个副本相同机架的节点</li><li>更多副本：随机节点</li></ul></li></ul><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><ul><li><p>写流程</p><p> <img src="/data/images/bigdata/hadoop-hdfs-write.png" alt="hadoop-hdfs-write"></p><ul><li>Client和NN连接创建文件元数据</li><li>NN判定元数据是否有效</li><li>NN触发副本放置策略，返回一个有序的DN列表</li><li>Client和DN建立Pipeline连接</li><li>Client将块切分成packet（64KB），并使用chunk（512B）+ chucksum（4B）填充</li><li><strong>基于流式传输数据，其实也是变种的并行计算</strong><ul><li>Client将packet放入发送队列dataqueue中，并向第一个DN发送</li><li>第一个DN收到packet后本地保存并发送给第二个DN，此时第一个DN可以接收下一个packet</li><li>第二个DN收到packet后本地保存并发送给第三个DN</li></ul></li><li>当block传输完成，DN们各自向NN汇报，同时client继续传输下一个block，client的传输和block的汇报也是并行的</li></ul></li><li><p>读流程</p><p> <img src="/data/images/bigdata/hadoop-hdfs-read.png" alt="hadoop-hdfs-read"></p><ul><li>为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本<ul><li>如果读取程序的本机上，则直接读取本机</li><li>如果在读取程序的同一个机架上有一个副本，那么就读取该副本</li><li>如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本</li></ul></li><li>语义：下载一个文件<ul><li>Client和NN交互文件元数据获取fileBlockLocation</li><li>NN会按距离策略排序返回</li><li>Client尝试下载block并校验数据完整性</li></ul></li><li><strong>语义：下载一个文件其实是获取文件的所有的block元数据，那么子集获取某些block应该成立</strong><ul><li>HDFS支持client给出文件的offset自定义连接哪些block的DN，自定义获取数据</li><li>这个是支持计算层的分治、并行计算的核心</li></ul></li></ul></li></ul><h3 id="HA及联邦"><a href="#HA及联邦" class="headerlink" title="HA及联邦"></a>HA及联邦</h3><ul><li>解决HDFS 1.0中单点故障和内存受限问题<ul><li>解决单点故障：HDFS HA(通过多个主备NameNode解决)<ul><li>如果主NameNode发生故障，则切换到备NameNode上(备NameNode会同步主NameNode元数据)</li><li>所有DataNode同时向两个NameNode汇报数据块信息</li></ul></li><li>解决内存受限问题：HDFS Federation(联邦)<ul><li>水平扩展，支持多个NameNode</li><li>每个NameNode分管一部分目录。直接在A NN中无法获取B NN的目录结构，可以在两个NN之上再创建一个统一目录管理，如/a目录保存到A NN，/b保存到B NN</li><li>所有NameNode共享所有DataNode存储资</li></ul></li></ul></li><li>基于Zookeeper自动切换方案(也可手动切换)<ul><li><code>Zookeeper Failover Controller</code>(ZKFC) 监控NameNode健康状态，并向Zookeeper注册NameNode。NameNode挂掉后，ZKFC为NameNode竞争锁，获得ZKFC锁的NameNode变为active</li></ul></li><li><p>HA架构图</p><p> <img src="/data/images/bigdata/hadoop-ha.png" alt="HA架构图"></p><ul><li><code>NameNode</code>分为<code>Active</code>(主)和<code>Standby</code>(备)。主备切换的条件是两天NN的元数据一致</li><li><code>NameNode(Active)</code>会将<code>edits</code>文件保存到<code>JournalNode</code>中(服务数&gt;=2)。<code>NameNode(Standby)</code>会同步<code>JournalNode</code>中的<code>edits</code>数据。初始化时，将其中一台进行<code>fsimage</code>格式化，然后将此<code>fsimage</code>复制到其他机器。确保元数据(fsimage + edits)一致</li><li>所有DataNode启动时同时向两个NameNode汇报数据块信息</li><li><code>Zookeeper Failover Controller</code>和<code>NameNode</code>是一一对应。作用：通过远程命令控制<code>NameNode</code>切换；对<code>NameNode</code>做健康检查，并汇报给<code>Zookeeper</code></li></ul></li><li><p>Federation架构图</p><p> <img src="/data/images/bigdata/hadoop-federation.gif" alt="Federation架构图"></p><ul><li>通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展。多个namenode相会独立</li><li>能把单个namenode的负载分散到多个节点中，在HDFS数据规模较大的时候不会也降低HDFS的性能。可以通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中</li></ul></li></ul><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><ul><li>Map-reduce的思想就是”分而治之”</li><li>主要分为<code>Map</code>和<code>Reduce</code>两个阶段<ul><li>Map和Reduce是阻塞的，必须先完成Map阶段才可进入Reduce阶段，这种计算方式属于批量计算（不同于流式计算）</li><li>Map负责映射、变换、过滤；Reduce负责分解、缩小、归纳；二者通过KV关联，并根据K分组</li><li>也可细分为<code>Split</code>、<code>Map</code>、<code>Shuffler(sort、copy、merge)</code>、<code>Reduce</code>几个阶段</li></ul></li><li><p>Map-Reduce整体架构图</p><p> <img src="/data/images/bigdata/hadoop-mp.png" alt="MapReduce架构图"></p><ul><li>从split块中以一条记录为单位，执行map方法映射成KV；相同的key为一组，这一组数据调用一次reduce方法，在方法内迭代计算这一组数据</li><li>数据按照一定的切割规律提交到每个Map进程中<ul><li>block是物理切割，split是逻辑切割(可根据资源情况，设置切割大小)，默认对应Block大小</li><li>有说切割大小为 <code>max(min.split, min(max.split, block))</code> 其中默认min.split=10M，max.split=100M，block=128M</li></ul></li><li>reduce计算使用迭代器模式：数据集较大时，使用迭代的计算方式，可有效节省内存(只需要获取一条数据到内存中进行计算)</li></ul></li><li><p>MapTask-ReduceTask执行原理</p><p> <img src="/data/images/bigdata/hadoop-mp-detail.png" alt="MapReduce执行原理"></p><ul><li>MapTask流程<ul><li>split切片会格式化出记录(如以换行符或开闭标签等方式来进行切割)，以记录为单位调用map方法</li><li>map的输出映射成KV，KV会参与分区计算，拿着key算出分区号P(如按照reduce个数取模)，最终为K,V,P</li><li>将上述单条记录处理好后保存到buffer中，buffer满后按照分区P和键K进行快速排序，然后按照归并排序写入到一个本地文件中(最终得到的文件是先按分区排，分区内按照K排)<ul><li>使用buffer是为了减少IO，如果每次将单条记录写入到文件中会频繁产生IO(系统调用)</li><li>按分区排好序是方便后面ReduceTask读取MapTask结果文件时只需读取一部分(分区部分)，不用扫描整个文件</li><li>按K排好序也是方便后面ReduceTask计算时使用迭代器模式，不用扫描整个分区片段</li></ul></li></ul></li><li>ReduceTask流程<ul><li>从不同MapTask中获取结果文件，读取当前ReduceTask处理的分区片段</li><li>将几个分区片段按照归并排序合并后提交到Reduce方法中进行迭代计算<ul><li>reduce的归并排序其实可以和reduce方法的计算同时发生</li><li>且无需将所有片段合并到文件再计算，从而减少IO，因为有迭代器模式的支持</li></ul></li><li>将计算结果输出</li></ul></li></ul></li><li><p>执行流程举例(查询文件中的重复行)</p><p> <img src="/data/images/bigdata/hadoop-mp-demo.png" alt="MapReduce执行流程举例"></p></li></ul><h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><ul><li>由于MapReduce注重计算向数据移动(将计算用到的算法程序拷贝到数据所在节点执行，减少数据IO)，而一份数据一般会有几个备份，且集群中可能同时运行着多个任务，因此需要资源管理(考量节点CPU/内存/网络等因素)来协调运行计算的节点</li><li>YARN是Haddop 2.x才出现的集群资源管理独立模块，只要是Hadoop生态均可使用；最早资源管理系统是集成在MapReduce中的</li></ul><h3 id="Haddop-1-x"><a href="#Haddop-1-x" class="headerlink" title="Haddop 1.x"></a>Haddop 1.x</h3><ul><li><p>MR资源管理</p><p> <img src="/data/images/bigdata/hadoop-jt-tt.png" alt="MR资源管理"></p></li><li>客户端<ul><li>需要从NN中获取到源数据保存的DN节点，从而才能将MR计算程序拷贝到相应节点运行(计算向数据移动)，实际的计算(执行M/R方法)发送在数据节点</li><li>而适合运行此计算任务的需要从JobTracker中获知</li><li>上传源数据的客户端和发起计算任务的客户端可能是不同的客户端</li></ul></li><li><code>JobTracker</code>(JT)主要负责管理集群资源和任务调度<ul><li>接受TaskTracker(TT)上报的资源数据，从而判断出适合运行MR程序的节点</li><li>将MR计算程序拷贝到相应节点执行计算</li><li>JobTracker以常服务(永久运行的)运行在MR中，YARN之后MR则无此常服务</li></ul></li><li><code>TaskTracker</code>(TT)主要负责当前DataNode资源上报，每个DataNode会运行一个TaskTracker</li><li>存在问题<ul><li>JobTracker单点问题(不稳定、负载不好)</li><li>JobTracker和MR耦合度太高，不利于整合对其他框架</li></ul></li></ul><h3 id="Haddop-2-x"><a href="#Haddop-2-x" class="headerlink" title="Haddop 2.x"></a>Haddop 2.x</h3><ul><li><p>YARN原理</p><p> <img src="/data/images/bigdata/hadoop-yarn.png" alt="YARN原理"></p></li><li>ResourceManager：类似JobTracker(只包含其资源管理，不包含其任务调度)<ul><li>接受NodeManager上报的状态信息</li><li>接受客户端发起的计算任务请求<ul><li>客户端会把计算程序(jar)上传到HDFS集群，最终会由AppMaster读取并分发到其他节点，计算完成后会清除此jar</li><li>ResourceManager会告诉NodeManager启动一个AppMaster(即JobTracker的任务调度功能)</li><li>AppMaster会向ResourceManager上报信息(监控挂掉可重新启动一个新的)，并提交请求的资源信息(需要的Container数)</li><li>ResourceManager会告诉NodeManager启动Container(用来执行M/R方法)，Container向AppMaster上报信息</li><li>每个请求任务有各自独立的AppMaster，从而减少了负载。计算任务完成，AppMaster和Container会停止</li></ul></li><li>集群单点有个节点运行ResourceManager</li><li>可基于ZK进行HA部署，但是和HDFS不同的是不需要ZKFC这个角色<ul><li>Hadoop 1.x的时候HDFS存在单点故障，因此在开发Haddop 2.x的时候增加了HA，但是为了向老版本兼容，不想过多修改1.x的程序，从而出现了ZKFC来控制HA的切换</li><li>而在Haddop 2.x的时候开发YARN是新的模块，从而切换功能直接在ResourceManager中</li></ul></li></ul></li><li>NodeManager：类似TaskTracker，运行在各个DN</li></ul><h2 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h2><ul><li>本文基于<code>hadoop-2.10.1</code>(jdk1.8)</li><li>HA模式安装：<a href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></li><li><a href="#单节点安装(不常用">单节点安装见下文</a>)</li></ul><h3 id="HDFS安装"><a href="#HDFS安装" class="headerlink" title="HDFS安装"></a>HDFS安装</h3><h4 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h4><table><thead><tr><th>服务器名</th><th>ip</th><th>NameNode</th><th>DataNode</th><th>JN</th><th>Zookeeper</th><th>ZKFC</th></tr></thead><tbody><tr><td>node01</td><td>192.168.6.131</td><td>Y</td><td></td><td></td><td>Y</td><td>Y</td></tr><tr><td>node02</td><td>192.168.6.132</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>node03</td><td>192.168.6.133</td><td></td><td>Y</td><td>Y</td><td>Y</td><td></td></tr><tr><td>node04</td><td>192.168.6.134</td><td></td><td>Y</td><td>Y</td><td></td></tr></tbody></table><ul><li><code>date</code> 检查4台机器的时间是否相差不大(30秒内)，并查看是否关闭防火墙</li><li>4台主机的<code>/etc/hosts</code>文件都需要包含一下内容</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.6.131	node01</span><br><span class="line">192.168.6.132	node02</span><br><span class="line">192.168.6.133	node03</span><br><span class="line">192.168.6.134	node04</span><br></pre></td></tr></table></figure><h4 id="免密登录"><a href="#免密登录" class="headerlink" title="免密登录"></a>免密登录</h4><ul><li>免密登录：使NN(node01和node02)可以免密码登录到自己和其他3台服务器<ul><li>启动start-dfs.sh脚本的机器，会去启动其他节点</li><li>在HA模式下，每一个NN所在节点会启动ZKFC，ZKFC会用免密的方式控制自己和其他NN节点的NN状态</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 设置node01</span></span><br><span class="line">su - <span class="built_in">test</span> <span class="comment"># 使用普通用户登录并安装</span></span><br><span class="line"><span class="comment">## 为4台机器都生成ssh密钥和公钥文件(可通过xshell的快速命令发送到全部会话)</span></span><br><span class="line">ssh-keygen -P <span class="string">''</span> <span class="comment"># -P设置密码</span></span><br><span class="line"><span class="comment">## 将node01和node02(NameNode)的公钥文件内容追加到自己和其他3台机器的的认证文件中</span></span><br><span class="line"><span class="comment">## 将本机器公钥文件内容追加到认证文件中。此时可以通过命令如`ssh node01`无需密码即可登录本地机器，记得要`exit`退出会话</span></span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub <span class="built_in">test</span>@node01 &amp;&amp; ssh-copy-id -i ~/.ssh/id_rsa.pub node02 &amp;&amp; ssh-copy-id -i ~/.ssh/id_rsa.pub node03 &amp;&amp; ssh-copy-id -i ~/.ssh/id_rsa.pub node04 <span class="comment"># 省略test@，则默认去当前用户</span></span><br><span class="line"><span class="comment"># 测试登录</span></span><br><span class="line">ssh <span class="built_in">test</span>@node03</span><br><span class="line"></span><br><span class="line"><span class="comment">## node02公钥复制同理</span></span><br></pre></td></tr></table></figure><h4 id="在node01上进行安装hadoop"><a href="#在node01上进行安装hadoop" class="headerlink" title="在node01上进行安装hadoop"></a>在node01上进行安装hadoop</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /opt/bigdata</span><br><span class="line"><span class="comment"># 生产环境一般不使用root，改成test用户，也方便后面测试权限</span></span><br><span class="line">sudo chown <span class="built_in">test</span>:<span class="built_in">test</span> /opt/bigdata</span><br><span class="line">mkdir -p /opt/bigdata/hadoop</span><br><span class="line">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.10.1/hadoop-2.10.1.tar.gz</span><br><span class="line"><span class="comment"># 解压`hadoop-2.10.1.tar.gz`(官方提供的是32位；32位的包可以运行在64位机器上，只是有警告；反之不行)</span></span><br><span class="line">tar -zxvf hadoop-2.10.1.tar.gz -C /opt/bigdata</span><br><span class="line"><span class="comment"># chown -R test:test /opt/bigdata/hadoop-2.10.1</span></span><br><span class="line"><span class="comment"># 数据目录</span></span><br><span class="line">mkdir -p /var/bigdata/hadoop</span><br><span class="line">chown -R <span class="built_in">test</span>:<span class="built_in">test</span> /var/bigdata/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量(必须配置 JAVA_HOME)</span></span><br><span class="line">    <span class="comment">#export JAVA_HOME=/usr/java/default</span></span><br><span class="line">    <span class="comment">#export HADOOP_HOME=/opt/bigdata/hadoop-2.10.1</span></span><br><span class="line">    <span class="comment">#export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span></span><br><span class="line">vi /etc/profile	</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改Hadoop脚本文件中的$&#123;JAVA_HOME&#125;</span></span><br><span class="line">    <span class="comment"># export JAVA_HOME=/usr/java/default</span></span><br><span class="line">vi <span class="variable">$HADOOP_HOME</span>/etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure><ul><li><code>vi $HADOOP_HOME/etc/hadoop/core-site.xml</code> 进行如下配置。<a href="http://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">配置项参考</a></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--配置NameNode的dfs.nameservices--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://aezocn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Zookeeper集群 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:2181,node02:2181,node03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 其他临时目录的基目录 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/bigdata/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">        可选。使用hiveserver2服务的时候需要修改hdfs的超级用户的管理权限（其中test为Hadoop启动用户）</span></span><br><span class="line"><span class="comment">        代理用户：如hiveserver2可以使用多个用户访问hive，这些用户实际操作hdfs时，对于hdfs而言看到的都是test这个用户，从而跳过hdfs的验证。此处*表示所有，多个可用逗号分割，如代理主机: node03,node04</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.test.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.test.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><code>vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml</code> 进行如下配置。<a href="http://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">配置项参考文档</a></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 一个hdfs实例的唯一标识 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>aezocn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- NameNode标识(dfs.ha.namenodes.[dfs.nameservices])，多个用逗号分割 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.aezocn<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- rpc协议用于hdfs文件上传和读取，zkfc会监控此端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.aezocn.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.aezocn.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- http协议用于后台监控 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.aezocn.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.aezocn.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定3台JournalNode服务地址。aezocn为JN的存储目录，且会自动新建，用于存放edits数据；这样多个Hadoop集群可以共用一个JN集群 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://node02:8485;node03:8485;node04:8485/aezocn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 启用Zookeeper Failover Controller自动切换 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- HA角色切换的代理类，帮助客户端查询一个活动的NameNode(Active)，固定为下面的类名；后面接集群名 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.aezocn<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 对NameNode进行远程切换时，此处通过ssh实现运行远程命令(还可通过shell脚本实现)。并设置ssh免密(由于使用test启动程序，因此需要test进行免密登录) --&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/test/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 副本个数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- fsimage文件会存放在此目录，默认是/tmp下，而此目录重启会清空，容易造成fsimage丢失。会自动创建文件夹 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/bigdata/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/bigdata/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 为JournalNode存放edits数据文件的根目录(会在此目录创建jndir)，会自动创建 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/bigdata/hadoop/dfs/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><code>vi $HADOOP_HOME/etc/hadoop/slaves</code> 配置<code>DataNode</code>主机名</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br><span class="line">node03</span><br><span class="line">node04</span><br></pre></td></tr></table></figure><h4 id="拷贝项目目录到其他3台机器"><a href="#拷贝项目目录到其他3台机器" class="headerlink" title="拷贝项目目录到其他3台机器"></a>拷贝项目目录到其他3台机器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提前在其他机器创建目录</span></span><br><span class="line">mkdir -p /opt/bigdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在node01上将文件复复制到其他节点</span></span><br><span class="line">scp -r /opt/bigdata/hadoop-2.10.1 root@node02:/opt/bigdata</span><br><span class="line">chown -R <span class="built_in">test</span>:<span class="built_in">test</span> /opt/bigdata/hadoop-2.10.1</span><br><span class="line"><span class="comment"># 配置另外3台机器的hadoop环境变量。如果几台机器的配置一致，可通过scp拷贝到其他节点</span></span><br><span class="line">scp /etc/profile root@node02:/etc/profile</span><br><span class="line"><span class="comment"># 在其他节点运行使配置生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><ul><li><strong>安装并启动Zookeeper</strong>。参考<a href="/_posts/arch/zookeeper.md">/_posts/arch/zookeeper.md</a>。<code>jps</code>会发现有一个<code>QuorumPeerMain</code>的进程</li><li>初始化(第一次安装才需要)<ul><li>切换test用户执行(则创建的数据目录所属用户为test)</li><li><strong>先启动JN</strong><ul><li><code>hadoop-daemon.sh start journalnode</code> 分别在三台JN节点上执行命令进行启动</li><li>可通过<code>jps</code>查看相应节点是否有<code>JournalNode</code>进程</li><li>或查看日志是否有错，如<code>tail -f /opt/bigdata/hadoop-2.10.1/logs/hadoop-root-journalnode-node02.log</code> <strong>不是.out文件</strong></li><li><strong>启动/重启程序时，都必须先启动JN</strong>，因为JN中存在最完整的数据；如果先启动了NN，则可能吧NN中不完整的数据覆盖掉了JN中的数据</li></ul></li><li><strong><code>hdfs namenode -format</code></strong> 初始化NN元数据<pre><code>- 选择一个NN 做格式化，第一次运行格式化hdfs获得元数据(不报错则成功，应该会有类似`Storage directory /var/bigdata/hadoop/dfs/name has been successfully formatted.`的日志)
- 只能在一台机器上执行一次，之后也不用执行。因为每次执行会生成一个唯一clusterID，如果两台机器都执行则会产生两个不同的clusterID
- 会创建上述`/opt/data/hadoop`文件夹及其name子文件夹(/var/bigdata/hadoop/dfs/name/current)
    - 并在里面创建`fsimage`文件和VERSION等文件
    - 且会将NN的数据同步到JN相应目录，如：/var/bigdata/hadoop/dfs/jn/aezocn/current/VERSION
</code></pre></li><li><strong><code>hadoop-daemon.sh start namenode</code></strong> 启动上述格式化的NN节点，以备另外一台同步。<code>jps</code>会出现一个<code>NameNode</code>的进程</li><li><strong><code>hdfs namenode -bootstrapStandby</code></strong> 在另外一台NN中以Standby模式初始化<ul><li>会自动同步元数据，且打印日志如<code>Storage directory /var/bigdata/hadoop/dfs/name has been successfully formatted.</code></li><li>或者手动拷贝上述fsimage到另外一台NameNode(node02)：<code>scp -r /var/bigdata/hadoop root@node02:/var/bigdata</code></li></ul></li><li><strong><code>hdfs zkfc -formatZK</code></strong> 在主NN中运行<ul><li>在某一台NameNode上初始化Zookeeper，只需在一台机器上执行一次，之后不用执行</li><li>本质是在ZK上创建一个hadoop目录(/hadoop-ha/aezocn)，之后会创建ZKFC的临时节点</li></ul></li></ul></li></ul><h3 id="YARN安装"><a href="#YARN安装" class="headerlink" title="YARN安装"></a>YARN安装</h3><ul><li>在node03-node04上运行ResourceManager；NodeManager运行在各个DN上，无需额外配置</li><li>安装启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只需要在安装好的hdfs配置上，增加几个配置文件。无需重启hdfs</span></span><br><span class="line"><span class="comment"># 在node01上修改配置文件，见下文</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">vi mapred-site.xml</span><br><span class="line">vi yarn-site.xml</span><br><span class="line">vi slaves <span class="comment"># 设置需要运行NodeManager的节点，可以不用设置，搭建hdfs时候已经设置了DN(和NM一一对应)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># **分发到node02-node04**</span></span><br><span class="line">scp mapred-site.xml yarn-site.xml node02:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><ul><li>启动参考<a href="#YARN启停">YARN启停</a></li><li>mapred-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- mapreduce on yarn，启动yarn后便可执行MapReduce程序 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启ha --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:2181,node03:2181,node04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- yarn监控资源的RM集群别名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>myyarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 集群部署节点 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node04<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 否则报错 MRClientService.getHttpPort NullPointerException --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node04:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="启动-停止-使用"><a href="#启动-停止-使用" class="headerlink" title="启动/停止/使用"></a>启动/停止/使用</h2><h3 id="HDFS启停"><a href="#HDFS启停" class="headerlink" title="HDFS启停"></a>HDFS启停</h3><ul><li><strong>需先确保Zookeeper集群已经启动</strong></li><li><strong><code>hadoop-daemon.sh start journalnode</code></strong> 用test用户(hadoop所属用户)分别在三台JN(node02-node04)节点上执行命令进行启动(<strong>必须在NN之前启动</strong>)<ul><li>自动启动：可在<code>/etc/rc.local</code>中增加代码<code>sudo -H -u test bash -c &#39;/opt/bigdata/hadoop-2.10.1/sbin/hadoop-daemon.sh start journalnode&#39;</code>自动启动(centos7需先<code>chmod +x /etc/rc.d/rc.local</code>激活此文件)</li></ul></li><li><p><strong><code>start-dfs.sh</code></strong> 在某一台NameNode上用test用户启动</p><ul><li><strong><code>start-all.sh</code></strong> 包含了 start-dfs.sh 和 start-yarn.sh</li><li><p>此时node01会通过免密码登录启动其他机器上的hadoop服务(NN、DN、ZKFC, JN已提前启动)</p><ul><li>journalnode在上述启动过，此处不会重新启动</li><li>启动后，则会自动同步数据到JN，如：edits_inprogress_0000000000000000001</li><li>启动后，查看ZK(<code>zkCli.sh -&gt; ls /hadoop-ha/aezocn</code>)会发现多出<code>ActiveBreadCrumb, ActiveStandbyElectorLock</code>两个节点(ActiveStandbyElectorLock记录了当前谁获得了锁，即那个节点是active)</li><li><p>需要保证数据目录<code>/var/bigdata/hadoop</code>为test用户有权读写(因为此处通过test用户启动)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 日志如下</span></span><br><span class="line">Starting namenodes on [node01 node02]</span><br><span class="line">node01: starting namenode, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-namenode-node01.out</span><br><span class="line">node02: starting namenode, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-namenode-node02.out</span><br><span class="line">node02: starting datanode, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-datanode-node02.out</span><br><span class="line">node03: starting datanode, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-datanode-node03.out</span><br><span class="line">node04: starting datanode, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-datanode-node04.out</span><br><span class="line">Starting journal nodes [node02 node03 node04]</span><br><span class="line">node02: journalnode running as process 16792. Stop it first.</span><br><span class="line">node03: journalnode running as process 15892. Stop it first.</span><br><span class="line">node04: journalnode running as process 15820. Stop it first.</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [node01 node02]</span><br><span class="line">node02: starting zkfc, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-zkfc-node02.out</span><br><span class="line">node01: starting zkfc, logging to /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-zkfc-node01.out</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>hadoop-daemon.sh start namenode</code> 在某节点运行，则单独启动一个NameNode</p></li><li><code>hadoop-daemon.sh start datanode</code> 在某节点运行，单独启动一个DataNode</li><li><code>hadoop-daemon.sh start journalnode</code> 在某节点运行，单独启动一个JN</li><li><code>hadoop-daemon.sh start zkfc</code> 在某节点运行，单独启动一个ZKFC</li></ul></li><li><strong>访问</strong><code>http://node01:50070</code>和<code>http://node02:50070</code> 查看NameNode监控。会发现一个为active，一个为standby<ul><li>关闭active对应的NameNode服务，可发现standby对应的服务变为active，实现了NameNode接管<ul><li>如node02无法切换为active，可查看对应ZKFC的日志 <code>tail -f /opt/bigdata/hadoop-2.10.1/logs/hadoop-test-zkfc-node02.log</code>。常见无法切换错误<ul><li>提示<code>Unable to fence NameNode</code>，可检查是否可进行免密码登录</li><li>提示<code>ssh: bash: fuser: 未找到命令</code>，可在NameNode上安装 <code>yum -y install psmisc</code></li></ul></li><li>手动切换nn2为active：<code>hdfs haadmin -transitionToActive nn2</code>(在未开启自动切换模式下才可使用)</li></ul></li><li>关闭active对应的ZKFC服务，可发现standby对应的服务变为active</li><li>关闭active对应节点的网络，此时会发现standby对应节点会抢到ZK锁，但是无法将自己升级为active(因为ZKFC无法与原active节点通信，因此无法确定对方节点状态)</li></ul></li><li><code>stop-dfs.sh</code> 停止所有hadoop服务</li><li>所有的启动日志均在<code>logs</code>目录，且Hadoop日志文件格式类似：<strong><code>hadoop-用户-角色-节点.log</code></strong></li></ul><h3 id="YARN启停"><a href="#YARN启停" class="headerlink" title="YARN启停"></a>YARN启停</h3><ul><li>前确保HDFS启动完成</li><li>启动NM和RM</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (root执行亦可) 在node01上执行，会在所有 DN 上启动 NM(NodeManager)。但是不能自动启动RM，需要手动启动；由于配置了 RM(ResourceManager) 不会在node01上启动，会看到执行此脚本后先在node01上启动RM，之后会自动退出(正常现象)</span></span><br><span class="line">start-yarn.sh</span><br><span class="line"><span class="comment"># stop-yarn.sh</span></span><br><span class="line"><span class="comment"># 在DN上使用jps查看可发现多出NodeManager的进程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (root执行亦可) **在node03-node04上分别执行，手动启动RM**</span></span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="comment"># zkCli.sh进入zk命令行，结果为 [ActiveBreadCrumb, ActiveStandbyElectorLock]，ActiveStandbyElectorLock中记录了主RM</span></span><br><span class="line"><span class="comment"># ls /yarn-leader-election/myyarn</span></span><br><span class="line"><span class="comment"># yarn-daemon.sh stop resourcemanager # 停止</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 RM 后台</span></span><br><span class="line">http://node03:8088</span><br><span class="line"><span class="comment"># 如果node04为从节点，直接访问会返回 `This is standby RM. Redirecting to the current active RM: http://node03:8088/`，并跳转到ndoe03</span></span><br><span class="line"><span class="comment"># 如果访问具体路径则不会跳转：http://node04:8088/cluster/cluster</span></span><br><span class="line">http://node04:8088</span><br></pre></td></tr></table></figure><h3 id="HDFS简单使用"><a href="#HDFS简单使用" class="headerlink" title="HDFS简单使用"></a>HDFS简单使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看dfs命令帮助. 客户端都可以执行，一些只要有集群配置，则可执行hdfs命令，还有一些必须要在NN上执行才可</span></span><br><span class="line">hdfs dfs</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入管理界面 - Utilities - Browse the file system 查看创建的目录</span></span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">hdfs dfs -mkdir /bigdata</span><br><span class="line"><span class="comment"># 创建test用户目录</span></span><br><span class="line">hdfs dfs -mkdir -p /user/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件（不指定目录，则默认上传到当前用户目录，即运行此命令的用户对应的HDFS用户目录）。当文件正在上传时，后台界面看到的是 `hadoop-2.10.1.tar.gz._COPYING_`</span></span><br><span class="line">hdfs dfs -put hadoop*.tar.gz</span><br><span class="line"><span class="comment"># 可以在管理界面查看到文件上传了node02、node03两个DN，找到一个DN查看上传文件，可以看到有`blk_xxxBlockID`的文件即为block(默认块大小为128M，此处上传会有两个block)</span></span><br><span class="line"><span class="comment"># cd /var/bigdata/hadoop/dfs/data/current/BP-2042046744-192.168.6.131-1621791288100/current/finalized/subdir0/subdir0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试设置块大小为1M</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `seq 100000`;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"hello hadoop <span class="variable">$i</span>"</span> &gt;&gt; data.txt ;<span class="keyword">done</span> <span class="comment"># 文件大小为1.9M</span></span><br><span class="line"><span class="comment"># 上传文件，指定块大小，上传目录为 /bigdata/data</span></span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576 -put data.txt /bigdata/data</span><br><span class="line"><span class="comment"># 会产生两个快，第一个的内容为`hello hadoop 1...hello hadoop 5`，第二个的内容为`5773...hello hadoop 100000`</span></span><br><span class="line"><span class="comment"># cd /var/bigdata/hadoop/dfs/data/current/BP-2042046744-192.168.6.131-1621791288100/current/finalized/subdir0/subdir0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载某个文件到当前目录</span></span><br><span class="line">hdfs dfs -get /data/wc/output/part-r-00000 ./</span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">hdfs dfs -rm /bigdata/data/data.txt</span><br></pre></td></tr></table></figure><h2 id="HDFS权限"><a href="#HDFS权限" class="headerlink" title="HDFS权限"></a>HDFS权限</h2><ul><li>hdfs是一个文件系统，类似linux有用户概念</li><li>hdfs没有相关命令和接口去创建用户<ul><li>默认情况使用操作系统提供的用户</li><li>也可扩展 LDAP/kerberos/继承第三方用户认证系统</li></ul></li><li>有超级用户的概念<ul><li>linux系统中超级用户：root</li><li>hdfs系统中超级用户：namenode进程的启动用户(如test启动，则test为hdfs的超级用户，其他包括root都为普通用户)</li></ul></li><li>有权限概念<ul><li>hdfs的权限是自己控制的，来自于hdfs的超级用户</li><li>默认hdfs依赖操作系统上的用户和组</li></ul></li><li>用户权限测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 设置目录权限，在node01(NN)上执行</span></span><br><span class="line">su - <span class="built_in">test</span></span><br><span class="line">hdfs dfs -mkdir /temp <span class="comment"># drwxr-xr-x test supergroup</span></span><br><span class="line">hdfs dfs -chown <span class="built_in">test</span>:aezo /temp <span class="comment"># drwxr-xr-x test aezo 尽管集群没有同步到aezo这个组名，仍然可以修改成功</span></span><br><span class="line">hdfs dfs -chmod 770 /temp <span class="comment"># drwxrwx--- test aezo 此时test用户在管理后台也无法进入改文件夹，提示Permission denied: user=dr.who, access=READ_EXECUTE, inode="/temp":test:aezo:drwxrwx---</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试上传，在node04(DN)上执行(随便找一台执行测试)</span></span><br><span class="line">su - root</span><br><span class="line"><span class="comment"># 创建用户和组，并关联</span></span><br><span class="line">useradd smalle &amp;&amp; groupadd aezo &amp;&amp; usermod -a -G aezo smalle &amp;&amp; id smalle <span class="comment"># uid=1001(smalle) gid=1001(smalle) groups=1001(smalle),1002(aezo)</span></span><br><span class="line">su - smalle</span><br><span class="line"><span class="comment"># 创建文件夹失败：因为hdfs已经启动了，不知道操作系统又创建了用户和组(解决见下文)</span></span><br><span class="line">hdfs dfs -mkdir /temp/abc <span class="comment"># mkdir: Permission denied: user=smalle, access=EXECUTE, inode="/temp":test:aezo:drwxrwx---</span></span><br><span class="line">hdfs groups <span class="comment"># smalle :</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 同步新的用户和组到集群，**必须在NN所在节点执行**</span></span><br><span class="line">su - root</span><br><span class="line">useradd smalle &amp;&amp; groupadd aezo &amp;&amp; usermod -a -G aezo smalle &amp;&amp; id smalle <span class="comment"># uid=1001(smalle) gid=1001(smalle) groups=1001(smalle),1002(aezo)</span></span><br><span class="line"><span class="comment"># 切换到集群管理员(因为使用test启动的集群)同步用户和组</span></span><br><span class="line">su - <span class="built_in">test</span></span><br><span class="line">hdfs dfsadmin -refreshUserToGroupsMappings</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重新测试上传，在node04(DN)上执行</span></span><br><span class="line">su - smalle</span><br><span class="line"><span class="comment"># 创建文件夹成功</span></span><br><span class="line">hdfs dfs -mkdir /temp/abc</span><br><span class="line">hdfs groups <span class="comment"># smalle : smalle aezo</span></span><br></pre></td></tr></table></figure><h2 id="API使用"><a href="#API使用" class="headerlink" title="API使用"></a>API使用</h2><ul><li>基于IDEA开发hadoop的client</li><li>在windows上创建<code>HADOOP_USER_NAME=test</code>的环境变量，用于Hadoop获取当前用户；设置JDK版本和Hadoop服务器版本一致</li><li>创建maven项目，引入客户端依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 和服务器版本一致 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在resources目录放置core-site.xml、hdfs-site.xml配置文件</li></ul><h3 id="HDFS测试代码"><a href="#HDFS测试代码" class="headerlink" title="HDFS测试代码"></a>HDFS测试代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestHDFS</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Configuration conf = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">public</span> FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//C/S</span></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">conn</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 读取当前classpath下的core-site.xml、hdfs-site.xml配置</span></span><br><span class="line">        conf = <span class="keyword">new</span> Configuration(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// fs = FileSystem.get(conf);</span></span><br><span class="line">        fs = FileSystem.get(URI.create(<span class="string">"hdfs://aezocn/"</span>), conf, <span class="string">"test"</span>); <span class="comment">// 也可基于配置文件覆盖配置</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Path dir = <span class="keyword">new</span> Path(<span class="string">"/idea-client"</span>);</span><br><span class="line">        <span class="keyword">if</span>(fs.exists(dir)) &#123;</span><br><span class="line">            fs.delete(dir,<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        fs.mkdirs(dir);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">upload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 本地文件流</span></span><br><span class="line">        BufferedInputStream input = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"./data/hello.txt"</span>)));</span><br><span class="line">        <span class="comment">// 目标文件流</span></span><br><span class="line">        Path outfile = <span class="keyword">new</span> Path(<span class="string">"/idea-client/hello-word.txt"</span>);</span><br><span class="line">        FSDataOutputStream output = fs.create(outfile);</span><br><span class="line">        IOUtils.copyBytes(input, output, conf,<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">blocks</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Path file = <span class="keyword">new</span> Path(<span class="string">"/bigdata/data"</span>); <span class="comment">// 一个1.8M的文件，并设定一个Block为1M。[参考上文测试文件](#启动/停止/使用)</span></span><br><span class="line">        FileStatus fss = fs.getFileStatus(file);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取文件块信息。第一个的内容为`hello hadoop 1...hello hadoop 5`，第二个的内容为`5773...hello hadoop 100000`</span></span><br><span class="line">        BlockLocation[] blks = fs.getFileBlockLocations(fss, <span class="number">0</span>, fss.getLen());</span><br><span class="line">        <span class="keyword">for</span> (BlockLocation b : blks) &#123;</span><br><span class="line">            <span class="comment">// 0,1048576,node02,node03</span></span><br><span class="line">            <span class="comment">// 1048576,840319,node02,node03</span></span><br><span class="line">            System.out.println(b);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        FSDataInputStream in = fs.open(file);</span><br><span class="line">        <span class="comment">// 设置偏移为1M，相当于从第二个Block开始读。因此多个客户端可设置不同的偏移来同时读取一个文件，最后合并</span></span><br><span class="line">        <span class="comment">//计算向数据移动后，期望的是分治，只读取自己关心（通过seek实现），同时具备距离的概念（优先和本地的DN获取数据--框架的默认机制）</span></span><br><span class="line">        in.seek(<span class="number">1048576</span>);</span><br><span class="line">        System.out.println((<span class="keyword">char</span>)in.readByte()); <span class="comment">// 5</span></span><br><span class="line">        System.out.println((<span class="keyword">char</span>)in.readByte()); <span class="comment">// 7</span></span><br><span class="line">        System.out.println((<span class="keyword">char</span>)in.readByte()); <span class="comment">// 7</span></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="MapReduce测试代码"><a href="#MapReduce测试代码" class="headerlink" title="MapReduce测试代码"></a>MapReduce测试代码</h3><ul><li>需要安装好HDFS和YARN</li><li>提交任务方式<ul><li>将my-mr.jar上传到集群中的某一个节点，再执行类似<code>hadoop jar my-mr.jar input output</code>的命令提交任务到YARN</li><li>嵌入集群方式，在linux/windows上开发程序并直接提交任务到YARN(计算发生在集群)。参考下文案例</li><li>local单机执行(计算发生在本机)<ul><li>在windows的系统中部署hadoop，并设置HADOOP_HOME</li><li>设置<code>mapreduce.framework.name=local</code>和<code>mapreduce.app-submission.cross-platform=true</code></li><li>额外下载相应版本的<code>hadoop.dll</code>、<code>winutils.exe</code>(参考：<a href="https://github.com/cdarlint/winutils)，分别放到C:\Windows\System32和%HADOOP_HOME%/bin目录" target="_blank" rel="noopener">https://github.com/cdarlint/winutils)，分别放到C:\Windows\System32和%HADOOP_HOME%/bin目录</a></li></ul></li></ul></li></ul><h4 id="测试hadoop提供的单词统计案例"><a href="#测试hadoop提供的单词统计案例" class="headerlink" title="测试hadoop提供的单词统计案例"></a>测试hadoop提供的单词统计案例</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建wc程序输入数据保存目录</span></span><br><span class="line">hdfs dfs -mkdir -p /data/wc/input</span><br><span class="line"><span class="comment"># 上传输入数据文件</span></span><br><span class="line"><span class="comment"># for i in `seq 100000`;do echo "hello hadoop $i" &gt;&gt; data.txt ;done</span></span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576 -put data.txt /data/wc/input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交MR任务。参数分别为：MR算法程序jar，wordcount为程序中定义的启动类(一般使用全类名)，输入数据目录(可为文件/目录/多个目录或文件)，最后一个为输出数据文件夹(一般要是一个不存在的目录)</span></span><br><span class="line"><span class="comment"># 21/05/28 22:09:30 INFO impl.YarnClientImpl: Submitted application application_1622210933585_0001</span></span><br><span class="line"><span class="comment"># 21/05/28 22:09:30 INFO mapreduce.Job: The url to track the job: http://node03:8088/proxy/application_1622210933585_0001/</span></span><br><span class="line"><span class="comment"># 21/05/28 22:09:30 INFO mapreduce.Job: Running job: job_1622210933585_0001</span></span><br><span class="line"><span class="comment"># 21/05/28 22:09:59 INFO mapreduce.Job: Job job_1622210933585_0001 running in uber mode : false</span></span><br><span class="line"><span class="comment"># 21/05/28 22:09:59 INFO mapreduce.Job:  map 0% reduce 0%</span></span><br><span class="line"><span class="comment"># 21/05/28 22:10:56 INFO mapreduce.Job:  map 100% reduce 0%</span></span><br><span class="line"><span class="comment"># 21/05/28 22:11:30 INFO mapreduce.Job:  map 100% reduce 100%</span></span><br><span class="line"><span class="comment"># 21/05/28 22:11:32 INFO mapreduce.Job: Job job_1622210933585_0001 completed successfully</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-2.10.1.jar wordcount /data/wc/input /data/wc/output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line"><span class="comment"># /data/wc/output/_SUCCESS # 标志成功的文件</span></span><br><span class="line"><span class="comment"># /data/wc/output/part-r-00000 # 返回结果文件(可能有多个)：m表示map运算后的结果(reduce个数=0时)，r表示reduce运算后的结果</span></span><br><span class="line">hdfs dfs -ls /data/wc/output</span><br><span class="line"><span class="comment"># 结果为</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 99998	1</span></span><br><span class="line"><span class="comment"># 99999	1</span></span><br><span class="line"><span class="comment"># hadoop	100000</span></span><br><span class="line"><span class="comment"># hello	100000</span></span><br><span class="line">hdfs dfs -cat /data/wc/output/part-r-00000</span><br><span class="line">hdfs dfs -get /data/wc/output/part-r-00000 ./ <span class="comment"># 下载结果文件到当前目录</span></span><br></pre></td></tr></table></figure><ul><li>案例：计算温度最高的前两天(数据源如下)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /data/twc/input</span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576 -put data.txt /data/wc/input</span><br></pre></td></tr></table></figure><h4 id="手写MR程序来完成单词统计"><a href="#手写MR程序来完成单词统计" class="headerlink" title="手写MR程序来完成单词统计"></a>手写MR程序来完成单词统计</h4><ul><li>操作流程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /data/twc/input</span><br><span class="line"><span class="comment"># for i in `seq 100000`;do echo "hello hadoop $i" &gt;&gt; data.txt ;done</span></span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576 -put data.txt /data/twc/input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用本地提交任务方式</span></span><br><span class="line"><span class="comment"># 编写相应代码，并打包成jar</span></span><br><span class="line"><span class="comment"># 在IDEA中设置Programe arguments=/data/twc/input /data/twc/output</span></span><br><span class="line"><span class="comment"># 执行main方法</span></span><br><span class="line"><span class="comment"># 到YARN后台查看执行结果</span></span><br><span class="line"><span class="comment"># 可以看到执行的任务列表，点击一个`application_xxx`进去可看到`appattempt_xxx`的执行任务尝试(其中的Node即为当前尝试时，AppMaster运行的节点)，点击Logs可查看日志(主要是syslog)</span></span><br><span class="line">http://node03:8088/cluster/apps</span><br></pre></td></tr></table></figure><ul><li>将<code>mapred-site.xml</code>和<code>yarn-site.xml</code>复制到项目的resources目录</li><li>算法程序如下</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TestWordCount.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration(<span class="keyword">true</span>); <span class="comment">// import org.apache.hadoop.conf.Configuration;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// hadoop command [genericOptions] [commandOptions]</span></span><br><span class="line">        <span class="comment">// eg: hadoop jar test.jar myTest -D name=test inpath outpath</span></span><br><span class="line">        <span class="comment">// args 包含2类参数: genericOptions commandOptions</span></span><br><span class="line">        <span class="comment">// 工具类会把-D类型的属性(genericOptions)直接set到conf，会留下commandOptions</span></span><br><span class="line">        GenericOptionsParser parser = <span class="keyword">new</span> GenericOptionsParser(conf, args);</span><br><span class="line">        String[] remainingArgs = parser.getRemainingArgs();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在本地运行MR程序，任务不会提交到YARN</span></span><br><span class="line">        <span class="comment">// conf.set("mapreduce.framework.name", "local");</span></span><br><span class="line">        <span class="comment">// windows上执行必须配置: 从而可得知文件分隔符</span></span><br><span class="line">        conf.set(<span class="string">"mapreduce.app-submission.cross-platform"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf); <span class="comment">// import org.apache.hadoop.mapreduce.Job;</span></span><br><span class="line">        <span class="comment">// 在本地提交任务到YARN上需要，否则无需</span></span><br><span class="line">        job.setJar(<span class="string">"D:\\gitwork\\smjava\\hadoop\\target\\hadoop-0.0.1-SNAPSHOT.jar"</span>);</span><br><span class="line">        job.setJarByClass(TestWordCount.class);</span><br><span class="line">        job.setJobName(<span class="string">"TestWordCount"</span>);</span><br><span class="line"></span><br><span class="line">        TextInputFormat.addInputPath(job, <span class="keyword">new</span> Path(remainingArgs[<span class="number">0</span>])); <span class="comment">// import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; 注意使用2.x api(lib包下)</span></span><br><span class="line">        Path outFile = <span class="keyword">new</span> Path(remainingArgs[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (outFile.getFileSystem(conf).exists(outFile)) &#123;</span><br><span class="line">            <span class="comment">// 如果存在此目录则删除</span></span><br><span class="line">            outFile.getFileSystem(conf).delete(outFile, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        TextOutputFormat.setOutputPath(job, outFile); <span class="comment">// import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设定Map方法类</span></span><br><span class="line">        job.setMapperClass(TestMapper.class);</span><br><span class="line">        <span class="comment">// 设置map方法执行完之后返回的KV类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">// 设置Reduce方法类</span></span><br><span class="line">        job.setReducerClass(TestReduce.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 默认Reduce个数为1；如果只做过滤，即只运行map方法，可设置为0</span></span><br><span class="line">        <span class="comment">// job.setNumReduceTasks(0);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交任务并等待</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TestMapper.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123; <span class="comment">// import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.io.Text; import org.apache.hadoop.io.IntWritable;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// hadoop有自己一套可以序列化、反序列化类. 如Test =&gt; String, IntWritable =&gt; Integer</span></span><br><span class="line">    <span class="comment">// 也可自己开发数据类型，但是必须实现：序列化接口、反序列化接口、比较器接口</span></span><br><span class="line">    <span class="comment">// 排序比较分为：字典序、数值顺序</span></span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// key: 是每一行字符串自己第一个字节面向源文件的偏移量</span></span><br><span class="line">    <span class="comment">// value: hello hadoop 1</span></span><br><span class="line">    <span class="comment">// value: hello hadoop 2</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">        <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 将k,v定义为成员变量的原因: 由于大量数据处理时，会重复调用此map方法很多次，如果频繁new对象，则会频繁触发GC，从而计算效率变慢</span></span><br><span class="line">            <span class="comment">// 每次进入map方法, k 中存的数据都会被刷走</span></span><br><span class="line">            k.set(itr.nextToken());</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TestReduce.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 相同的key为一组，这一组数据调用一次reduce</span></span><br><span class="line">    <span class="comment">// key value: hello 1</span></span><br><span class="line">    <span class="comment">// key value: hello 1</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="HDFS单节点安装-不常用"><a href="#HDFS单节点安装-不常用" class="headerlink" title="HDFS单节点安装(不常用)"></a>HDFS单节点安装(不常用)</h2><blockquote><p>单节点：<a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html</a></p></blockquote><ul><li>服务器配置</li></ul><table><thead><tr><th>服务器名</th><th>ip</th><th>角色</th></tr></thead><tbody><tr><td>node01</td><td>192.168.6.131</td><td>NameNode</td></tr><tr><td>node02</td><td>192.168.6.132</td><td>SecondaryNameNode、DataNode</td></tr><tr><td>node03</td><td>192.168.6.133</td><td>DataNode</td></tr><tr><td>node04</td><td>192.168.6.134</td><td>DataNode</td></tr></tbody></table><ul><li>检查date、修改hosts、免密码登录(参考上述HA安装)</li><li><p>在node01上进行安装hadoop(参考上述HA安装)</p><ul><li><p><code>vi etc/hadoop/core-site.xml</code> 进行如下配置(参考 <a href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/core-default.xml</a>)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置NameNode的主机名和数据传输端口(文件上传下载)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 默认保存在/tmp目录，容易丢失 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/data/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>vi /etc/hadoop/hdfs-site.xml</code> 进行如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置SecondaryNameNode的Http相关端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:50091<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>vi etc/hadoop/slaves</code> 配置<code>DataNode</code>主机名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br><span class="line">node03</span><br><span class="line">node04</span><br></pre></td></tr></table></figure></li><li><p><code>vi etc/hadoop/masters</code> 配置<code>SecondaryNameNode</code>主机名(默认无此文件。HA模式下无SecondaryNameNode，因此无效此步骤)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br></pre></td></tr></table></figure></li></ul></li><li><p>拷贝项目目录到其他3台机器并配置hadoop环境变量(参考上述HA安装)</p></li><li>在<code>node01(NameNode)</code>上运行<ul><li><code>hdfs namenode -format</code> 在<code>node01(NameNode)</code>上运行</li><li><code>start-dfs.sh</code> 在<code>node01(NameNode)</code>上执行</li><li><code>stop-dfs.sh</code> 停止所有hadoop服务</li></ul></li><li>访问<code>http://192.168.6.131:50070</code> 查看NameNode监控、<code>http://192.168.6.132:50090</code> 查看SecondaryNameNode监控</li></ul><hr><p>flume<br>ETL: kettle<br>sqoop、datax<br>埋点：初中高<br>oozie、azkanman</p><hr><p>参考文章</p><ul><li><a href="http://www.cnblogs.com/tgzhu/category/868038.html" target="_blank" rel="noopener">hdfs-HA原理及安装</a></li><li><a href="http://ambari.apache.org/" target="_blank" rel="noopener">Hadoop集群Web管理工具ambari</a></li></ul></div><div></div><div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> smalle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="http://blog.aezo.cn/2018/03/13/bigdata/hadoop/" title="Hadoop">http://blog.aezo.cn/2018/03/13/bigdata/hadoop/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/hadoop/" rel="tag"># hadoop</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/03/13/bigdata/solr/" rel="next" title="Solr"><i class="fa fa-chevron-left"></i> Solr</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2018/03/13/bigdata/lucene/" rel="prev" title="Lucene">Lucene<i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400" alt="smalle"><p class="site-author-name" itemprop="name">smalle</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">161</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">141</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS"><span class="nav-number">2.</span> <span class="nav-text">HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS基础概念"><span class="nav-number">2.1.</span> <span class="nav-text">HDFS基础概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读写流程"><span class="nav-number">2.2.</span> <span class="nav-text">读写流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HA及联邦"><span class="nav-number">2.3.</span> <span class="nav-text">HA及联邦</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce"><span class="nav-number">3.</span> <span class="nav-text">MapReduce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YARN"><span class="nav-number">4.</span> <span class="nav-text">YARN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Haddop-1-x"><span class="nav-number">4.1.</span> <span class="nav-text">Haddop 1.x</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Haddop-2-x"><span class="nav-number">4.2.</span> <span class="nav-text">Haddop 2.x</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop安装"><span class="nav-number">5.</span> <span class="nav-text">Hadoop安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS安装"><span class="nav-number">5.1.</span> <span class="nav-text">HDFS安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#服务器配置"><span class="nav-number">5.1.1.</span> <span class="nav-text">服务器配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#免密登录"><span class="nav-number">5.1.2.</span> <span class="nav-text">免密登录</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在node01上进行安装hadoop"><span class="nav-number">5.1.3.</span> <span class="nav-text">在node01上进行安装hadoop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拷贝项目目录到其他3台机器"><span class="nav-number">5.1.4.</span> <span class="nav-text">拷贝项目目录到其他3台机器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化"><span class="nav-number">5.1.5.</span> <span class="nav-text">初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YARN安装"><span class="nav-number">5.2.</span> <span class="nav-text">YARN安装</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动-停止-使用"><span class="nav-number">6.</span> <span class="nav-text">启动/停止/使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS启停"><span class="nav-number">6.1.</span> <span class="nav-text">HDFS启停</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YARN启停"><span class="nav-number">6.2.</span> <span class="nav-text">YARN启停</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS简单使用"><span class="nav-number">6.3.</span> <span class="nav-text">HDFS简单使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS权限"><span class="nav-number">7.</span> <span class="nav-text">HDFS权限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#API使用"><span class="nav-number">8.</span> <span class="nav-text">API使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS测试代码"><span class="nav-number">8.1.</span> <span class="nav-text">HDFS测试代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MapReduce测试代码"><span class="nav-number">8.2.</span> <span class="nav-text">MapReduce测试代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#测试hadoop提供的单词统计案例"><span class="nav-number">8.2.1.</span> <span class="nav-text">测试hadoop提供的单词统计案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#手写MR程序来完成单词统计"><span class="nav-number">8.2.2.</span> <span class="nav-text">手写MR程序来完成单词统计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS单节点安装-不常用"><span class="nav-number">9.</span> <span class="nav-text">HDFS单节点安装(不常用)</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2016 - <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">smalle</span>&nbsp;&nbsp;&nbsp;&nbsp;<div class="powered-by"> 由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="powered-by"> 主题 - <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="theme-info cnzz" style="margin:0 0 -5px 10px"><script type="text/javascript">var cnzz_protocol="https:"==document.location.protocol?"https://":"http://";document.write(unescape("%3Cspan id='cnzz_stat_icon_1276691827'%3E%3C/span%3E%3Cscript src='"+cnzz_protocol+"s23.cnzz.com/z_stat.php%3Fid%3D1276691827%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"))</script></div></div><div class="ad"> <span style="font-weight:700">AD&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=oby5nolb">阿里云大礼包</a></div></div><div class="aezocn"> <span style="font-weight:700">@AEZO.CN&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="http://shop.aezo.cn/">杂货铺</a></div></div><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.1"></script></body></html>