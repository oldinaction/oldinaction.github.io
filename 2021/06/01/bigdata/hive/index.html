<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="sql,hadoop,"><link rel="alternate" href="/atom.xml" title="Smalle's Blog | AEZOCN" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="简介 The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storag"><meta name="keywords" content="sql,hadoop"><meta property="og:type" content="article"><meta property="og:title" content="Hive"><meta property="og:url" content="http://blog.aezo.cn/2021/06/01/bigdata/hive/index.html"><meta property="og:site_name" content="Smalle&#39;s Blog | AEZOCN"><meta property="og:description" content="简介 The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storag"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hive-arch.png"><meta property="og:image" content="http://blog.aezo.cn/data/images/bigdata/hive-store-type.png"><meta property="og:updated_time" content="2021-08-31T06:07:20.336Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Hive"><meta name="twitter:description" content="简介 The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storag"><meta name="twitter:image" content="http://blog.aezo.cn/data/images/bigdata/hive-arch.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"BWD6R9FA4K",apiKey:"3330f3cbaa099dfc30395de5f5b20151",indexName:"blog",hits:{per_page:10},labels:{input_placeholder:"请输入关键字",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}}}</script><link rel="canonical" href="http://blog.aezo.cn/2021/06/01/bigdata/hive/"><title>Hive | Smalle's Blog | AEZOCN</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div style="display:none"><script src="//s95.cnzz.com/z_stat.php?id=cnzz_stat_icon_1276691827&web_id=cnzz_stat_icon_1276691827" language="JavaScript"></script></div></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Smalle's Blog | AEZOCN</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Better Code, Better Life</h1></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br> 站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.aezo.cn/2021/06/01/bigdata/hive/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="smalle"><meta itemprop="description" content=""><meta itemprop="image" content="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Smalle's Blog | AEZOCN"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Hive</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-06-01T19:58:00+08:00">2021-06-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><div></div><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><blockquote><p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p></blockquote><ul><li><a href="https://hive.apache.org/" target="_blank" rel="noopener">Hive官网</a>、<a href="https://mirrors.bfsu.edu.cn/apache/hive/" target="_blank" rel="noopener">下载</a>、<a href="https://github.com/apache/hive" target="_blank" rel="noopener">源码</a></li><li>Hive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制<ul><li>hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行</li><li><strong>hive基于hdfs做存储，基于mr进行计算(将sql语句转成mr程序)</strong></li></ul></li><li>Hive产生的原因<ul><li>方便对文件及数据的元数据进行管理，提供统一的元数据管理方式</li><li>提供更加简单的方式来访问大规模的数据集，使用SQL语言进行数据分析（无需写MapReduce程序，降低数据分析门槛）</li></ul></li><li><p>架构图</p><p> <img src="/data/images/bigdata/hive-arch.png" alt="hive-arch.png"></p><ul><li>用户访问接口<ul><li>CLI（Command Line Interface）：用户可以使用Hive自带的命令行接口执行Hive QL(有称HQL)、设置参数等功能</li><li>JDBC/ODBC：用户可以使用JDBC或者ODBC的方式在代码中操作Hive</li><li>Web GUI：浏览器接口，用户可以在浏览器中对Hive进行操作（2.2之后淘汰）</li></ul></li><li>Thrift Server：可使用Java、C++、Ruby等多种语言运行Thrift服务，通过编程的方式远程访问Hive</li><li>Driver：是Hive的核心，其中包含解释器、编译器、优化器等各个组件，完成从SQL语句到MapReduce任务的解析优化执行过程</li><li>Metastore：Hive的元数据存储服务，一般将数据存储在关系型数据库中<ul><li>HDFS中存有大量不同类型的数据，在做MR计算时，需要知道使用的数据集和一些数据特征(如数据分割符，分割后每个字段意思)，对应Hive中表结构，因此Hive将这些元数据(表结构)单独保存</li><li>Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（但是包含<em>的查询，比如select</em> from tbl不会生成MapRedcue任务）</li></ul></li></ul></li><li>HiveServer2模块(主要在是提供hive查询服务给远程用户)<ul><li>HiveServer2的实现，依托于Thrift RPC，它被设计用来提供更好的支持对于open API例如JDBC和ODBC</li><li>hiveserver2不用直接将hdfs和metastore暴露给用户</li><li>有HA机制，解决应用端的并发和负载问题</li><li>HiveServer2提供了一种新的命令行接口(Beeline)，可以提交执行SQL语句</li></ul></li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>元数据存储分类，参考：<a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration</a><ul><li>使用Hive自带的内存数据库Derby作为元数据存储(一般不使用)</li><li>使用远程数据库mysql作为元数据存储</li><li>使用本地/远程元数据服务模式安装Hive，可以基于Zookeeper对Thrift server进行HA配置(一般用于生产环境)</li></ul></li><li><code>v2.3.8</code>适用于<code>Hadoop 2.x</code>(本文使用版本)，<code>v3.x</code>适用于<code>Hadoop 3.x</code></li><li>安装</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## **启动hdfs和yarn集群**，参考[hadoop.md#启动/停止/使用](/_posts/bigdata/hadoop.md#启动/停止/使用)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在node01上安装mysql，略</span></span><br><span class="line">create database hive; <span class="comment"># 提前创建好元数据存储库</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在node02上安装Hive(当做Thrift server)</span></span><br><span class="line">wget https://mirrors.bfsu.edu.cn/apache/hive/hive-2.3.8/apache-hive-2.3.8-bin.tar.gz</span><br><span class="line">tar -zxvf apache-hive-2.3.8-bin.tar.gz</span><br><span class="line">mv apache-hive-2.3.8-bin hive-2.3.8</span><br><span class="line"><span class="comment"># 增加环境变量</span></span><br><span class="line">    <span class="comment"># HIVE_HOME=/opt/bigdata/hive-2.3.8</span></span><br><span class="line">    <span class="comment"># export PATH=$PATH:$HIVE_HOME/bin</span></span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 配置</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HIVE_HOME</span>/conf</span><br><span class="line"><span class="comment"># 先删除configuration中原默认配置，然后参考下文Thrift server上hive-site.xml配置</span></span><br><span class="line">cp hive-default.xml.template hive-site.xml</span><br><span class="line"><span class="comment"># 拷贝mysql-connector-java-5.1.49.jar到$HIVE_HOME/lib目录</span></span><br><span class="line">cp mysql-connector-java-5.1.49.jar <span class="variable">$HIVE_HOME</span>/lib</span><br><span class="line"><span class="comment"># ***启动Thrift server(使用root用户启动亦可，阻塞式窗口，卡住是正常现象)***。去mysql查看hive数据库已经自动创建了一些表</span></span><br><span class="line">hive --service metastore</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在node03上安装Hive(当做Driver)</span></span><br><span class="line">scp -r /opt/bigdata/hive-2.3.8 root@node03:/opt/bigdata</span><br><span class="line"><span class="comment"># 同上文一样增加环境变量</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HIVE_HOME</span>/conf</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="comment"># 配置，然后参考下文Driver上hive-site.xml配置</span></span><br><span class="line">vi hive-site.xml</span><br><span class="line"><span class="comment"># ***使用test用户(需要对hive.metastore.warehouse.dir有写入权限)执行，进入hive命令行***</span></span><br><span class="line">hive</span><br><span class="line"></span><br><span class="line"><span class="comment">## 日志所在目录</span></span><br><span class="line">cat /tmp/<span class="built_in">test</span>/hive.log</span><br></pre></td></tr></table></figure><ul><li>Thrift server上hive-site.xml配置</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 在hdfs中的根目录。无需提前创建，hive会自动创建 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node01:3306/hive?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>Hello1234!<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 不加会报错 MetaException(message:Version information not found in metastore. ) --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 自动创建表 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>Driver上hive-site.xml配置</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 在hdfs中的根目录。无需提前创建，hive会自动创建 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node02:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="使用HiveServer2组件"><a href="#使用HiveServer2组件" class="headerlink" title="使用HiveServer2组件"></a>使用HiveServer2组件</h2><ul><li><p>可选。使用共享metastore server的hiveserver2模式搭建</p><ul><li><p>需先在修改Hadoop配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改hdfs的超级用户的管理权限（其中test为Hadoop启动用户），否则报错：org.apache.hadoop.security.authorize.AuthorizationException --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.test.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span>	</span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.test.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span>	</span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>然后在所有NameNode上执行命令<code>hdfs dfsadmin -fs hdfs://node01:8020 -refreshSuperUserGroupsConfiguration</code>刷新配置<ul><li>node01:8020为各NN监听的rpc端口，<code>hdfs dfsadmin -fs hdfs://node02:8020 -refreshSuperUserGroupsConfiguration</code></li></ul></li></ul></li><li>在node02上(root亦可)执行<code>hive --service metastore</code>启动元数据服务</li><li>在node03上用test(会连接hdfs)执行<code>hive --service hiveserver2</code>或<code>hiveserver2</code>两个命令其中一个都可以(阻塞式命令行)<ul><li>会监听两个端口10000(接受HiveServer2客户端连接)、10002</li></ul></li><li>在任意一台包含beeline脚本(hive-2.3.8/bin/beeline)的虚拟机中执行<code>beeline</code>的命令进行连接</li></ul></li><li><code>beeline</code>命令行<ul><li><code>!connect jdbc:hive2://node03:10000/default test 123</code> 连接HiveServer2服务器，test对应密码123可随便输入<ul><li>进入后显示<code>0: jdbc:hive2://node03:10000/default&gt;</code>命令行</li><li>在beeline命令行下执行非hive sql语句需要使用<code>!</code></li><li>或者bash下直接连接 <code>beeline -u jdbc:hive2://192.168.6.133:10000/default -n test</code> -u表示url，-n表示登录用户(不用密码)，其中default为hive数据库<ul><li>和系统用户无关，只是一个标识(如把hdfs的/tmp目录设置成777, 则随便输入即可访问)</li><li>hive虽然不会保存一个实际的用户，但是hive会保存用户名和权限的关系，因此此处输入的用户名会在权限判断时用到</li></ul></li></ul></li><li><code>!help</code> 查看命令帮助</li><li><code>!quit</code> 退出命令行</li><li><code>show tables;</code> 链接上数据库后即可和hive cli一样执行SQL语句</li></ul></li><li>jdbc的访问方式：创建普通的java项目，将hive的jar包添加到classpath中，最精简的jar包如下</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">commons-lang-2.6.jar</span><br><span class="line">commons-logging-1.2.jar</span><br><span class="line">curator-client-2.7.1.jar</span><br><span class="line">curator-framework-2.7.1.jar</span><br><span class="line">guava-14.0.1.jar</span><br><span class="line">hive-exec-2.3.4.jar</span><br><span class="line">hive-jdbc-2.3.4.jar</span><br><span class="line">hive-jdbc-handler-2.3.4.jar</span><br><span class="line">hive-metastore-2.3.4.jar</span><br><span class="line">hive-service-2.3.4.jar</span><br><span class="line">hive-service-rpc-2.3.4.jar</span><br><span class="line">httpclient-4.4.jar</span><br><span class="line">httpcore-4.4.jar</span><br><span class="line">libfb303-0.9.3.jar</span><br><span class="line">libthrift-0.9.3.jar</span><br><span class="line">log4j-1.2-api-2.6.2.jar</span><br><span class="line">log4j-api-2.6.2.jar</span><br><span class="line">log4j-core-2.6.2.jar</span><br><span class="line">log4j-jul-2.5.jar</span><br><span class="line">log4j-slf4j-impl-2.6.2.jar</span><br><span class="line">log4j-web-2.6.2.jar</span><br><span class="line">zookeeper-3.4.6.jar</span><br></pre></td></tr></table></figure><h2 id="启停"><a href="#启停" class="headerlink" title="启停"></a>启停</h2><ul><li>启动hdfs和yarn集群，参考<a href="/_posts/bigdata/hadoop.md#启动/停止/使用">hadoop.md#启动/停止/使用</a></li><li>启动mysql (在node01上启动, 存放hive元数据)</li><li><code>hive --service metastore</code> 在node02上(root亦可)启动Thrift server，阻塞式窗口，卡住是正常现象</li><li><code>hive</code> 在node03上使用test用户执行，进入hive命令行即可执行增删改SQL<ul><li><code>quit;</code> 退出命令行</li></ul></li><li><code>hive --service hiveserver2</code> 在node03上用test启动Hiveserver2，参考<a href="#使用HiveServer2组件">使用HiveServer2组件</a></li><li><code>beeline</code> 连接到Hiveserver2，从而执行(查询)SQL，root亦可<ul><li><code>beeline -u jdbc:hive2://node03:10000/default -n test</code></li></ul></li></ul><h2 id="Hive命令使用"><a href="#Hive命令使用" class="headerlink" title="Hive命令使用"></a>Hive命令使用</h2><ul><li>hive运行方式分类<ul><li>命令行方式或者控制台模式</li><li>脚本运行方式（实际生产环境中用最多）</li><li>JDBC方式：hiveserver2</li><li>web GUI接口：hwi(hive v2.2以后已抛弃)、<a href="https://gethue.com/" target="_blank" rel="noopener">hue</a>等</li></ul></li></ul><h3 id="Hive-Cli"><a href="#Hive-Cli" class="headerlink" title="Hive Cli"></a>Hive Cli</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive --service cli <span class="comment"># 可简写为 `hive` 命令</span></span><br><span class="line">hive --service cli -h <span class="comment"># 查看 hive cli 命令帮助</span></span><br><span class="line"><span class="comment"># 帮助信息如下</span></span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    <span class="comment"># hive -d myid=1</span></span><br><span class="line">    <span class="comment"># select * from psn where id = $&#123;myid&#125;; # 使用上述定义的变量</span></span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from <span class="built_in">command</span> line</span><br><span class="line">    <span class="comment"># hive -e "select * from psn; show tables;" &gt; result.log # 可执行多个SQL，打印结果到文件(不会包含hive启动日志)，执行完后退出命令行</span></span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line">    <span class="comment"># hive -f ~/test.sql # 执行sql文件，完后会退出命令行</span></span><br><span class="line">    <span class="comment"># hive&gt; source test.sql; # 在hive命令行也可以执行本地sql文件，当前目录不能加~</span></span><br><span class="line"> -H,--<span class="built_in">help</span>                        Print <span class="built_in">help</span> information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value <span class="keyword">for</span> given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line">    <span class="comment"># hive -i ~/test.sql # 执行初始化sql文件，完后会停留在命令行</span></span><br><span class="line"> -S,--silent                      Silent mode <span class="keyword">in</span> interactive shell</span><br><span class="line">    <span class="comment"># hive -S # 静默模式，不会打印OK、Time taken等日志</span></span><br><span class="line"> -v,--verbose                     Verbose mode (<span class="built_in">echo</span> executed SQL to the console)</span><br></pre></td></tr></table></figure><ul><li>执行命令(hive命令行)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select * from psn; <span class="comment"># 执行Hive SQL</span></span><br><span class="line">dfs ls / <span class="comment"># 可以与HDFS交互</span></span><br><span class="line">! ls / <span class="comment"># 可以和linux交互</span></span><br><span class="line">quit; <span class="comment"># 退出hive命令行</span></span><br></pre></td></tr></table></figure><h3 id="参数操作"><a href="#参数操作" class="headerlink" title="参数操作"></a>参数操作</h3><ul><li>hive当中的参数、变量都是以命名空间开头的，详情如下表所示：</li></ul><table><thead><tr><th>命名空间</th><th>读写权限</th><th>含义</th></tr></thead><tbody><tr><td>hiveconf</td><td>可读写</td><td>hive-site.xml当中的各配置变量例：hive –hiveconf hive.cli.print.header=true</td></tr><tr><td>system</td><td>可读写</td><td>系统变量，包含JVM运行参数等例：system:user.name=root</td></tr><tr><td>env</td><td>只读</td><td>环境变量例：env：JAVA_HOME</td></tr><tr><td>hivevar</td><td>可读写</td><td>例：hive -d val=key。hive的变量可以通过<code>${}</code>方式进行引用，其中system、env下的变量必须以前缀开头</td></tr></tbody></table><ul><li>设置参数</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在启动hive cli时设置，此次会话生效。修改$&#123;HIVE_HOME&#125;/conf/hive-site.xml则永久生效</span></span><br><span class="line">hive --hiveconf hive.cli.print.header=<span class="literal">true</span> <span class="comment"># 打印表头</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在进入到cli之后，通过set命令设置</span></span><br><span class="line"><span class="built_in">set</span>; <span class="comment"># 查看所有参数, xxx=yyy、env:xxx=yyy、system:xxx=yyy</span></span><br><span class="line"><span class="built_in">set</span> hive.cli.print.header; <span class="comment"># 查看hive.cli.print.header的值</span></span><br><span class="line"><span class="built_in">set</span> hive.cli.print.header=<span class="literal">true</span>; <span class="comment"># 设值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hive参数初始化设置。当前用户每次进入hive cli的时候，都会加载.hiverc的文件，执行文件中的命令</span></span><br><span class="line">vi ~/.hiverc <span class="comment"># 在其中加入如`set hive.cli.print.header=true;`的参数配置</span></span><br><span class="line"><span class="comment"># cat ~/.hivehistory # 此文件中保存了hive cli中执行的所有命令</span></span><br></pre></td></tr></table></figure><h3 id="源码入口"><a href="#源码入口" class="headerlink" title="源码入口"></a>源码入口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.执行hive命令相当于执行了 hive --service cli</span></span><br><span class="line">hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.bin/etx/cli.sh: https://sourcegraph.com/github.com/apache/hive@rel/release-2.3.8/-/blob/bin/ext/cli.sh?L25</span></span><br><span class="line"><span class="function"><span class="title">updateCli</span></span>() &#123;</span><br><span class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$USE_DEPRECATED_CLI</span>"</span> == <span class="string">"true"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> HADOOP_CLIENT_OPTS=<span class="string">" -Dproc_hivecli <span class="variable">$HADOOP_CLIENT_OPTS</span> "</span></span><br><span class="line">    <span class="comment"># 主类</span></span><br><span class="line">    CLASS=org.apache.hadoop.hive.cli.CliDriver</span><br><span class="line">    JAR=hive-cli-*.jar</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">export</span> HADOOP_CLIENT_OPTS=<span class="string">" -Dproc_beeline <span class="variable">$HADOOP_CLIENT_OPTS</span> -Dlog4j.configurationFile=beeline-log4j2.properties"</span></span><br><span class="line">    CLASS=org.apache.hive.beeline.cli.HiveCli</span><br><span class="line">    JAR=hive-beeline-*.jar</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">cli</span></span> () &#123;</span><br><span class="line">  updateCli</span><br><span class="line">  execHiveCmd <span class="variable">$CLASS</span> <span class="variable">$JAR</span> <span class="string">"<span class="variable">$@</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.https://sourcegraph.com/github.com/apache/hive@rel/release-2.3.8/-/blob/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java?L685</span></span><br><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">    int ret = new CliDriver().run(args);</span><br><span class="line">    System.exit(ret);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SQL操作"><a href="#SQL操作" class="headerlink" title="SQL操作"></a>SQL操作</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener">LanguageManual</a></li><li>DDL参考：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a></li><li>DML参考：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML</a></li><li>数据库说明<ul><li>默认情况下，所有的表存在于default数据库，在hdfs上的展示形式是将此数据库的表保存在hive的默认路径下</li><li>如果创建了数据库，那么会在hive的默认路径下生成一个database_name.db的文件夹，此数据库的所有表会保存在database_name.db的目录下</li></ul></li><li>内部表跟外部表的区别<ul><li>hive内部表创建的时候数据存储在hive的默认存储目录中，外部表在创建的时候需要制定额外的目录(不会在hive默认数据目录创建数据库文件夹)</li><li>hive内部表删除的时候，会将元数据和数据都删除，而外部表只会删除元数据，不会删除数据</li><li>应用场景<ul><li>内部表: 需要先创建表，然后向表中添加数据，适合做中间表的存储</li><li>外部表: 可以先创建表，再添加数据，也可以先有数据，再创建表，本质上是将hdfs的某一个目录的数据跟hive的表关联映射起来，因此适合原始数据的存储，不会因为误操作将数据给删除掉</li></ul></li></ul></li><li>分区表<ul><li>hive默认将表的数据保存在某一个hdfs的存储目录下，当需要检索符合条件的某一部分数据的时候，需要全量遍历数据，io量比较大，效率比较低。因此可以采用分而治之的思想，将符合某些条件的数据放置在某一个目录，此时检索的时候只需要搜索指定目录即可，不需要全量遍历数据</li><li>注意项<ul><li>当创建完分区表之后，在保存数据的时候，会在hdfs目录中看到分区列会成为一个目录，多个分区以多级目录的形式存在</li><li>当创建多分区表之后，插入数据的时候不可以只添加一个分区列，需要将所有的分区列都添加值</li><li>多分区表在添加分区列的值得时候，与顺序无关，与分区表的分区列的名称相关，按照名称就行匹配</li><li>修改表时，添加分区列的值的时候，如果定义的是多分区表，那么必须给所有的分区列都赋值</li><li>修改表时，删除分区列的值的时候，无论是单分区表还是多分区表，都可以将指定的分区进行删除</li></ul></li><li>修复分区<ul><li><code>msck repair table my_table;</code></li><li>在使用hive外部表的时候，可以先将数据上传到hdfs的某一个目录中，然后再创建外部表建立映射关系，如果在上传数据的时候，参考分区表的形式也创建了多级目录，那么此时创建完表之后，是查询不到数据的，原因是分区的元数据没有保存在mysql中，因此需要修复分区，将元数据同步更新到mysql中，此时才可以查询到元数据</li></ul></li></ul></li></ul><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li>启动说明</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动hdfs和yarn集群，略</span></span><br><span class="line"><span class="comment"># 在node01上启动mysql数据库，略</span></span><br><span class="line"><span class="comment"># 在node02上启动Thrift server，启动后jps查看有一个RunJar的进程（如果是cli连接hive则可不用启动）</span></span><br><span class="line">hive --service metastore</span><br><span class="line"><span class="comment"># 进入hive命令行(类似mysql命令行)，在node03上使用test用户执行(需要对hive.metastore.warehouse.dir有写入权限). 参考：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli</span></span><br><span class="line">hive</span><br></pre></td></tr></table></figure><ul><li>简单操作</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看数据库</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 切换成test数据库</span></span><br><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建hive表. 会在hdfs中创建 /user/hive/warehouse/test.db/psn 目录(/user/hive/warehouse为hive默认数据根目录)</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    address <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看所有表</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="comment">-- 查看某个表</span></span><br><span class="line">desc psn;</span><br><span class="line"><span class="keyword">describe</span> formatted psn; <span class="comment">-- 查看详细信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入数据. 加载本地(local)数据(/home/test/data/psn_data)到hive表</span></span><br><span class="line"><span class="comment">-- /home/test/data/psn_data 文件内容为 `1^Asmalle^Agames^Bmusica^Addr1^Cshanghai`，其中 ^A 使用 `Control + V + A` 进行输入。且启动hive的用户需要对此文件有访问权限</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/test/data/psn_data'</span> <span class="keyword">into</span> <span class="keyword">table</span> psn;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询数据. 结果为：1	smalle	["games","music"]	&#123;"addr1":"shanghai"&#125;</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn;</span><br></pre></td></tr></table></figure><h3 id="数据库操作"><a href="#数据库操作" class="headerlink" title="数据库操作"></a>数据库操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 展示所有数据库，默认有一个 default 数据库</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="comment">-- 切换成test数据库</span></span><br><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="comment">-- 创建数据库		</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="comment">-- 删除数据库</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure><h3 id="DDL-表操作"><a href="#DDL-表操作" class="headerlink" title="DDL(表操作)"></a>DDL(表操作)</h3><h4 id="创建表语法"><a href="#创建表语法" class="headerlink" title="创建表语法"></a>创建表语法</h4><ul><li>参考：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTable" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTable</a></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name   <span class="comment">-- (<span class="doctag">Note:</span> TEMPORARY available in Hive 0.14.0 and later)</span></span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ... [constraint_specification])]</span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line"><span class="comment">-- 分区</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line"><span class="comment">-- 分桶</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[SKEWED <span class="keyword">BY</span> (col_name, col_name, ...)  <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.10.0 and later)]</span></span><br><span class="line"><span class="keyword">ON</span> ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES]</span><br><span class="line">[</span><br><span class="line">    [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">    | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]  <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.6.0 and later)</span></span><br><span class="line">]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]   <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.6.0 and later)</span></span><br><span class="line">[<span class="keyword">AS</span> select_statement];   <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.5.0 and later; not supported for external tables)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">    <span class="keyword">LIKE</span> existing_table_or_view_name</span><br><span class="line">[LOCATION hdfs_path];</span><br><span class="line"><span class="comment">-- 复杂数据类型</span></span><br><span class="line">data_type</span><br><span class="line">    : primitive_type</span><br><span class="line">    | array_type</span><br><span class="line">    | map_type</span><br><span class="line">    | struct_type</span><br><span class="line">    | union_type  <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.7.0 and later)</span></span><br><span class="line"><span class="comment">-- 基本数据类型</span></span><br><span class="line">primitive_type</span><br><span class="line">    : TINYINT</span><br><span class="line">    | SMALLINT</span><br><span class="line">    | INT</span><br><span class="line">    | BIGINT</span><br><span class="line">    | BOOLEAN</span><br><span class="line">    | FLOAT</span><br><span class="line">    | DOUBLE</span><br><span class="line">    | DOUBLE PRECISION <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 2.2.0 and later)</span></span><br><span class="line">    | STRING</span><br><span class="line">    | BINARY      <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.8.0 and later)</span></span><br><span class="line">    | TIMESTAMP   <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.8.0 and later)</span></span><br><span class="line">    | DECIMAL     <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.11.0 and later)</span></span><br><span class="line">    | DECIMAL(precision, scale)  <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.13.0 and later)</span></span><br><span class="line">    | DATE        <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.12.0 and later)</span></span><br><span class="line">    | VARCHAR     <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.12.0 and later)</span></span><br><span class="line">    | CHAR        <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.13.0 and later)</span></span><br><span class="line"></span><br><span class="line">array_type</span><br><span class="line">    : ARRAY &lt; data_type &gt;</span><br><span class="line"></span><br><span class="line">map_type</span><br><span class="line">    : MAP &lt; primitive_type, data_type &gt;</span><br><span class="line"></span><br><span class="line">struct_type</span><br><span class="line">    : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br><span class="line"></span><br><span class="line">union_type</span><br><span class="line">    : UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note: Available in Hive 0.7.0 and later)</span><br><span class="line"><span class="comment">-- 行格式规范</span></span><br><span class="line">row_format</span><br><span class="line">    : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS 				TERMINATED BY char]</span><br><span class="line">    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">    [NULL DEFINED AS char]   <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.13 and later)</span></span><br><span class="line">    | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, 				property_name=property_value, ...)]</span><br><span class="line"><span class="comment">-- 文件基本类型</span></span><br><span class="line">file_format:</span><br><span class="line">    : SEQUENCEFILE</span><br><span class="line">    | TEXTFILE    <span class="comment">-- (Default, depending on hive.default.fileformat configuration)</span></span><br><span class="line">    | RCFILE      <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.6.0 and later)</span></span><br><span class="line">    | ORC         <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.11.0 and later)</span></span><br><span class="line">    | PARQUET     <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.13.0 and later)</span></span><br><span class="line">    | AVRO        <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 0.14.0 and later)</span></span><br><span class="line">    | JSONFILE    <span class="comment">-- (<span class="doctag">Note:</span> Available in Hive 4.0.0 and later)</span></span><br><span class="line">    | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br><span class="line"><span class="comment">-- 表约束</span></span><br><span class="line">constraint_specification:</span><br><span class="line">    : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES 					table_name(col_name, ...) DISABLE NOVALIDATE</span><br></pre></td></tr></table></figure><h4 id="创建表案例"><a href="#创建表案例" class="headerlink" title="创建表案例"></a>创建表案例</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 外部表(需要添加external和location的关键字). 不会在hive默认数据目录创建文件夹</span></span><br><span class="line"><span class="comment">-- 分区表(partitioned)</span></span><br><span class="line"><span class="comment">-- 自定义分隔符</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> psn_part</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    address <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(gender <span class="keyword">string</span>, age <span class="built_in">int</span>) <span class="comment">-- 定义多个分区，注意分区字段和普通字段不能重复。如产生目录 /data/hive/psn_part/gender=man/age=10</span></span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="comment">-- 自定义分隔符。默认分隔符`^A(\001)、^B(\002)、^C(\003)`，其中 ^A 等为特殊分割符(cat文件时不可见)，需使用 `Control + V + A` 进行输入</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> <span class="comment">-- 字段分割符</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span> <span class="comment">-- 集合分隔符</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span> <span class="comment">-- map的key:value分隔符</span></span><br><span class="line">location <span class="string">'/data/hive/psn_part'</span>; <span class="comment">-- 数据保存的目录</span></span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除表(如果为外部表则不会删除dfs数据)</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> psn_part;</span><br></pre></td></tr></table></figure><h4 id="修改表结构案例"><a href="#修改表结构案例" class="headerlink" title="修改表结构案例"></a>修改表结构案例</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--给分区表添加分区列的值。添加分区列的值的时候，如果定义的是多分区表，那么必须给所有的分区列都赋值</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">partition</span>(col_name=col_value)</span><br><span class="line"><span class="comment">--删除分区列的值。删除分区列的值的时候，无论是单分区表还是多分区表，都可以将指定的分区进行删除</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">drop</span> <span class="keyword">partition</span>(col_name=col_value)</span><br></pre></td></tr></table></figure><h4 id="修复分区使用"><a href="#修复分区使用" class="headerlink" title="修复分区使用"></a>修复分区使用</h4><ul><li>一般是先有数据文件，后创建的hive表，需要用到修复分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在hdfs创建目录并上传文件</span></span><br><span class="line">hdfs dfs -mkdir /data</span><br><span class="line">hdfs dfs -mkdir /data/hive</span><br><span class="line">hdfs dfs -mkdir /data/hive/psn_part</span><br><span class="line">hdfs dfs -mkdir /data/hive/psn_part/age=10</span><br><span class="line">hdfs dfs -mkdir /data/hive/psn_part/age=20</span><br><span class="line"><span class="comment">-- 数据为：1,smalle,games-music,addr1:shanghai-add2:beijing</span></span><br><span class="line">hdfs dfs -put /home/test/data/psn_part_data_10 /data/hive/psn_part/age=10</span><br><span class="line"><span class="comment">-- 数据为</span></span><br><span class="line"><span class="comment">-- 2,test1,book-music1,addr1:guangzhou-add2:beijing</span></span><br><span class="line"><span class="comment">-- 3,test2,book-music2,addr1:guangzhou</span></span><br><span class="line">hdfs dfs -put /home/test/data/psn_part_data_20 /data/hive/psn_part/age=20</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在hive中创建外部分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> psn_part</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    address <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(age <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line">location <span class="string">'/data/hive/psn_part'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--查询结果（没有数据）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn_part;</span><br><span class="line"><span class="comment">--**修复分区**</span></span><br><span class="line">msck <span class="keyword">repair</span> <span class="keyword">table</span> psn_part;</span><br><span class="line"><span class="comment">--查询结果（有数据）</span></span><br><span class="line"><span class="comment">-- 1	smalle	["games","music"]	&#123;"addr1":"shanghai","add2":"beijing"&#125;	10</span></span><br><span class="line"><span class="comment">-- 2	test1	["book","music1"]	&#123;"addr1":"guangzhou","add2":"beijing"&#125;	20</span></span><br><span class="line"><span class="comment">-- 3	test2	["book","music2"]	&#123;"addr1":"guangzhou"&#125;	20</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn_part;</span><br></pre></td></tr></table></figure><h3 id="DML-数据操作"><a href="#DML-数据操作" class="headerlink" title="DML(数据操作)"></a>DML(数据操作)</h3><ul><li>数据更新和删除：在使用hive的过程中，我们一般不会产生删除和更新的操作</li></ul><h4 id="插入数据语法"><a href="#插入数据语法" class="headerlink" title="插入数据语法"></a>插入数据语法</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1.Loading files into tables(导入数据)</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> [<span class="keyword">LOCAL</span>] INPATH <span class="string">'filepath'</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span>(partcol1=val1, partcol2=val2 ...)]</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> [<span class="keyword">LOCAL</span>] INPATH <span class="string">'filepath'</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span>(partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT <span class="string">'inputformat'</span> SERDE <span class="string">'serde'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.Inserting data into Hive Tables from queries</span></span><br><span class="line"><span class="comment">-- 2.1 Standard syntax:</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"><span class="comment">-- 2.2 Hive extension (multiple inserts):</span></span><br><span class="line">FROM from_statement</span><br><span class="line">    <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1</span><br><span class="line">    [<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2]</span><br><span class="line">    [<span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...;</span><br><span class="line">FROM from_statement</span><br><span class="line">    <span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1</span><br><span class="line">    [<span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2]</span><br><span class="line">    [<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2] ...;</span><br><span class="line"><span class="comment">-- 2.3 Hive extension (dynamic partition inserts):</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.Writing data into the filesystem from queries</span></span><br><span class="line"><span class="comment">-- 3.1 Standard syntax:</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">-- (<span class="doctag">Note:</span> Only available starting with Hive 0.11.0)</span></span><br><span class="line"><span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br><span class="line"><span class="comment">-- 3.2 Hive extension (multiple inserts):</span></span><br><span class="line"><span class="keyword">FROM</span> from_statement</span><br><span class="line">    <span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 select_statement1</span><br><span class="line">    [<span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory2 select_statement2] ... </span><br><span class="line">    row_format</span><br><span class="line">    : <span class="keyword">DELIMITED</span> [<span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span> [<span class="keyword">ESCAPED</span> <span class="keyword">BY</span> <span class="built_in">char</span>]] [COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>]</span><br><span class="line">    [<span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>] [<span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>]</span><br><span class="line">    [<span class="literal">NULL</span> DEFINED <span class="keyword">AS</span> <span class="built_in">char</span>] <span class="comment">-- (<span class="doctag">Note:</span> Only available starting with Hive 0.13)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4.Inserting values into tables from SQL(使用传统关系型数据库的方式插入数据，效率较低)</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...)] <span class="keyword">VALUES</span> values_row [, values_row ...]</span><br><span class="line"><span class="keyword">Where</span> values_row <span class="keyword">is</span>:</span><br><span class="line">( <span class="keyword">value</span> [, <span class="keyword">value</span> ...] )</span><br><span class="line"><span class="keyword">where</span> a <span class="keyword">value</span> <span class="keyword">is</span> either <span class="literal">null</span> <span class="keyword">or</span> <span class="keyword">any</span> valid <span class="keyword">SQL</span> literal</span><br></pre></td></tr></table></figure><h4 id="插入数据案例"><a href="#插入数据案例" class="headerlink" title="插入数据案例"></a>插入数据案例</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--**导入数据(使用较多)**</span></span><br><span class="line"><span class="comment">--加载本地数据到hive表（复制文件）</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/test/data/psn_data'</span> <span class="keyword">into</span> <span class="keyword">table</span> psn;<span class="comment">--(/home/test/data/psn_data指的是本地linux目录)</span></span><br><span class="line"><span class="comment">--加载hdfs数据文件到hive表（移动文件）</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/psn_data'</span> <span class="keyword">into</span> <span class="keyword">table</span> psn;<span class="comment">--(/data/psn_data指的是hdfs的目录)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--下面两种方式插入数据的时候需要预先创建好结果表</span></span><br><span class="line"><span class="comment">--**从表中查询数据插入结果表(使用较多)**</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> psn1</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> psn;</span><br><span class="line"><span class="comment">--从表中获取部分列插入到新表中</span></span><br><span class="line">from psn</span><br><span class="line">    <span class="keyword">insert</span> overwrite <span class="keyword">table</span> psn1 <span class="comment">-- 将psn中的id,name字段覆盖psn1</span></span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span></span><br><span class="line">    <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> psn2 <span class="comment">-- 将psn中的id字段追加到psn2                         </span></span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">id</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--注意：overwrite为覆盖，路径千万不要填写根目录，会把所有的数据文件都覆盖</span></span><br><span class="line"><span class="comment">--将查询到的结果导入到hdfs文件系统中</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> psn;</span><br><span class="line"><span class="comment">--将查询的结果导入到本地文件系统中</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> psn;</span><br><span class="line"></span><br><span class="line"><span class="comment">--类似传统SQL语句一样插入数据(效率较低)</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> psn <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'zhangsan'</span>)</span><br></pre></td></tr></table></figure><h3 id="Serde进行数据处理"><a href="#Serde进行数据处理" class="headerlink" title="Serde进行数据处理"></a>Serde进行数据处理</h3><ul><li>Hive Serde用来做序列化和反序列化，构建在数据存储和执行引擎之间，对两者实现解耦<ul><li>hive主要用来存储结构化数据，如果结构化数据存储的格式嵌套比较复杂的时候，可以使用serde的方式，如利用正则表达式匹配的方法来读取数据</li></ul></li><li>语法</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">row_format</span><br><span class="line">: DELIMITED </span><br><span class="line">    [FIELDS TERMINATED BY char [ESCAPED BY char]] </span><br><span class="line">    [COLLECTION ITEMS TERMINATED BY char] </span><br><span class="line">    [MAP KEYS TERMINATED BY char] </span><br><span class="line">    [LINES TERMINATED BY char] </span><br><span class="line"><span class="comment"># 如 serde_name=org.apache.hadoop.hive.serde2.RegexSerDe 表示使用正则进行数据处理</span></span><br><span class="line">: SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><ul><li>案例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于下列数据，希望数据显示的时候包含[]或者""</span></span><br><span class="line"><span class="comment"># /root/data/log文件如下</span></span><br><span class="line">192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] <span class="string">"GET /bg-upper.png HTTP/1.1"</span> 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2019:18:14:35 +0800] <span class="string">"GET /bg-nav.png HTTP/1.1"</span> 304 -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="comment"># org.apache.hadoop.hive.serde2.RegexSerDe：表示使用正则进行数据处理(类名，注意大小写)</span></span><br><span class="line"><span class="comment"># 正则表达式：([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \"(.*)\" (-|[0-9]*) (-|[0-9]*)，前3个括号君表示匹配非空</span></span><br><span class="line">create table logtbl (</span><br><span class="line">    host string,</span><br><span class="line">    identity string,</span><br><span class="line">    t_user string,</span><br><span class="line">    time string,</span><br><span class="line">    request string,</span><br><span class="line">    referer string,</span><br><span class="line">    agent string</span><br><span class="line">)</span><br><span class="line">row format serde <span class="string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span></span><br><span class="line">with serdeproperties (<span class="string">"input.regex"</span> = <span class="string">"([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \"(.*)\" (-|[0-9]*) (-|[0-9]*)"</span>)</span><br><span class="line">stored as textfile;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">load data <span class="built_in">local</span> inpath <span class="string">'/home/test/data/logtbl'</span> into table logtbl;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">select * from logtbl;</span><br></pre></td></tr></table></figure><h3 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h3><ul><li>和关系型数据库差不多，hive内置了很多函数，如substr、count、explore等</li></ul><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><ul><li>分类<ul><li><code>UDF</code>(User-Defined-Function): 一进一出</li><li><code>UDAF</code>(Aggregation): 聚合函数，多进一出，类似count/max/min</li><li><code>UDTF</code>(Table-Generating): 一进多出，如explore</li></ul></li><li>引用依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    无法下载pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar，可手动下载后放到.m2相应目录</span></span><br><span class="line"><span class="comment">    下载地址：https://public.nexus.pentaho.org/repository/proxy-public-3rd-party-release/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>案例</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.exec.UDF</span></span><br><span class="line"><span class="comment">// org.apache.hadoop.io.Text</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TuoMin</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 需要实现evaluate函数，evaluate函数支持重载</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String str = s.toString().substring(<span class="number">0</span>, <span class="number">1</span>) + <span class="string">"***"</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Text(str);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用<ul><li>把程序打包成jar上传到hdfs集群的/jar目录下：<code>hdfs dfs -D dfs.blocksize=1048576 -put bigdata-hive-0.0.1-SNAPSHOT.jar /jar</code></li><li>创建函数：hive&gt; <code>create function sq_tuomin as &#39;cn.aezo.bigdata.hive.func.TuoMin&#39; using jar &quot;hdfs://aezocn/jar/bigdata-hive-0.0.1-SNAPSHOT.jar&quot;;</code></li><li>查询HQL语句：<code>select sq_tuomin(name) from psn;</code> 返回<code>s***</code></li><li>销毁临时函数：hive&gt; <code>drop function sq_tuomin;</code></li></ul></li><li>临时使用：此种方式创建的函数属于临时函数，当关闭了当前会话之后，函数会无法使用，因为jar的引用没有了<ul><li>hive&gt; <code>add jar /home/test/bigdata-hive-0.0.1-SNAPSHOT.jar;</code> 在客户端执行，使用服务器本地目录(/home/test)</li><li>创建临时函数：hive&gt; <code>create temporary function sq_tuomin AS &#39;cn.aezo.bigdata.hive.func.TuoMin&#39;;</code></li></ul></li></ul><h4 id="1-内置运算符"><a href="#1-内置运算符" class="headerlink" title="1.内置运算符"></a>1.内置运算符</h4><h5 id="1-1关系运算符"><a href="#1-1关系运算符" class="headerlink" title="1.1关系运算符"></a>1.1关系运算符</h5><table><thead><tr><th>运算符</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>A = B</td><td>所有原始类型</td><td>如果A与B相等,返回TRUE,否则返回FALSE</td></tr><tr><td>A == B</td><td>无</td><td>失败，因为无效的语法。 SQL使用”=”，不使用”==”。</td></tr><tr><td>A &lt;&gt; B</td><td>所有原始类型</td><td>如果A不等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。</td></tr><tr><td>A &lt; B</td><td>所有原始类型</td><td>如果A小于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。</td></tr><tr><td>A &lt;= B</td><td>所有原始类型</td><td>如果A小于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。</td></tr><tr><td>A &gt; B</td><td>所有原始类型</td><td>如果A大于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。</td></tr><tr><td>A &gt;= B</td><td>所有原始类型</td><td>如果A大于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。</td></tr><tr><td>A IS NULL</td><td>所有类型</td><td>如果A值为”NULL”，返回TRUE,否则返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有类型</td><td>如果A值不为”NULL”，返回TRUE,否则返回FALSE</td></tr><tr><td>A LIKE B</td><td>字符串</td><td>如 果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过sql进行匹配，如果相符返回TRUE，不符返回FALSE。B字符串中 的”<em>”代表任一字符，”%”则代表多个任意字符。例如： (‘foobar’ like ‘foo’)返回FALSE，（ ‘foobar’ like ‘foo</em> _ _’或者 ‘foobar’ like ‘foo%’)则返回TURE</td></tr><tr><td>A RLIKE B</td><td>字符串</td><td>如 果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过java进行匹配，如果相符返回TRUE，不符返回FALSE。例如：（ ‘foobar’ rlike ‘foo’）返回FALSE，（’foobar’ rlike ‘^f.*r$’ ）返回TRUE。</td></tr><tr><td>A REGEXP B</td><td>字符串</td><td>与RLIKE相同。</td></tr></tbody></table><h5 id="1-2算术运算符"><a href="#1-2算术运算符" class="headerlink" title="1.2算术运算符"></a>1.2算术运算符</h5><table><thead><tr><th>运算符</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>A + B</td><td>所有数字类型</td><td>A和B相加。结果的与操作数值有共同类型。例如每一个整数是一个浮点数，浮点数包含整数。所以，一个浮点数和一个整数相加结果也是一个浮点数。</td></tr><tr><td>A – B</td><td>所有数字类型</td><td>A和B相减。结果的与操作数值有共同类型。</td></tr><tr><td>A * B</td><td>所有数字类型</td><td>A和B相乘，结果的与操作数值有共同类型。需要说明的是，如果乘法造成溢出，将选择更高的类型。</td></tr><tr><td>A / B</td><td>所有数字类型</td><td>A和B相除，结果是一个double（双精度）类型的结果。</td></tr><tr><td>A % B</td><td>所有数字类型</td><td>A除以B余数与操作数值有共同类型。</td></tr><tr><td>A &amp; B</td><td>所有数字类型</td><td>运算符查看两个参数的二进制表示法的值，并执行按位”与”操作。两个表达式的一位均为1时，则结果的该位为 1。否则，结果的该位为 0。</td></tr><tr><td>A\</td><td>B</td><td>所有数字类型</td><td>运算符查看两个参数的二进制表示法的值，并执行按位”或”操作。只要任一表达式的一位为 1，则结果的该位为 1。否则，结果的该位为 0。</td></tr><tr><td>A ^ B</td><td>所有数字类型</td><td>运算符查看两个参数的二进制表示法的值，并执行按位”异或”操作。当且仅当只有一个表达式的某位上为 1 时，结果的该位才为 1。否则结果的该位为 0。</td></tr><tr><td>~A</td><td>所有数字类型</td><td>对一个表达式执行按位”非”（取反）。</td></tr></tbody></table><h5 id="1-3逻辑运算符"><a href="#1-3逻辑运算符" class="headerlink" title="1.3逻辑运算符"></a>1.3逻辑运算符</h5><table><thead><tr><th>运算符</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>A AND B</td><td>布尔值</td><td>A和B同时正确时,返回TRUE,否则FALSE。如果A或B值为NULL，返回NULL。</td></tr><tr><td>A &amp;&amp; B</td><td>布尔值</td><td>与”A AND B”相同</td></tr><tr><td>A OR B</td><td>布尔值</td><td>A或B正确,或两者同时正确返返回TRUE,否则FALSE。如果A和B值同时为NULL，返回NULL。</td></tr><tr><td>A \</td><td>B</td><td>布尔值</td><td>与”A OR B”相同</td></tr><tr><td>NOT A</td><td>布尔值</td><td>如果A为NULL或错误的时候返回TURE，否则返回FALSE。</td></tr><tr><td>! A</td><td>布尔值</td><td>与”NOT A”相同</td></tr></tbody></table><h5 id="1-4复杂类型函数"><a href="#1-4复杂类型函数" class="headerlink" title="1.4复杂类型函数"></a>1.4复杂类型函数</h5><table><thead><tr><th>函数</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>map</td><td>(key1, value1, key2, value2, …)</td><td>通过指定的键/值对，创建一个map。</td></tr><tr><td>struct</td><td>(val1, val2, val3, …)</td><td>通过指定的字段值，创建一个结构。结构字段名称将COL1，COL2，…</td></tr><tr><td>array</td><td>(val1, val2, …)</td><td>通过指定的元素，创建一个数组。</td></tr></tbody></table><p>1.5对复杂类型函数操作</p><table><thead><tr><th>函数</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>A[n]</td><td>A是一个数组，n为int型</td><td>返回数组A的第n个元素，第一个元素的索引为0。如果A数组为[‘foo’,’bar’]，则A[0]返回’foo’和A[1]返回”bar”。</td></tr><tr><td>M[key]</td><td>M是Map&lt;K, V&gt;，关键K型</td><td>返回关键值对应的值，例如mapM为 {‘f’ -&gt; ‘foo’, ‘b’ -&gt; ‘bar’, ‘all’ -&gt; ‘foobar’}，则M[‘all’] 返回’foobar’。</td></tr><tr><td>S.x</td><td>S为struct</td><td>返回结构x字符串在结构S中的存储位置。如 foobar {int foo, int bar} foobar.foo的领域中存储的整数。</td></tr></tbody></table><h4 id="2-内置函数"><a href="#2-内置函数" class="headerlink" title="2.内置函数"></a>2.内置函数</h4><h5 id="2-1数学函数"><a href="#2-1数学函数" class="headerlink" title="2.1数学函数"></a>2.1数学函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>BIGINT</td><td>round(double a)</td><td>四舍五入</td></tr><tr><td>DOUBLE</td><td>round(double a, int d)</td><td>小数部分d位之后数字四舍五入，例如round(21.263,2),返回21.26</td></tr><tr><td>BIGINT</td><td>floor(double a)</td><td>对给定数据进行向下舍入最接近的整数。例如floor(21.2),返回21。</td></tr><tr><td>BIGINT</td><td>ceil(double a), ceiling(double a)</td><td>将参数向上舍入为最接近的整数。例如ceil(21.2),返回23.</td></tr><tr><td>double</td><td>rand(), rand(int seed)</td><td>返回大于或等于0且小于1的平均分布随机数（依重新计算而变）</td></tr><tr><td>double</td><td>exp(double a)</td><td>返回e的n次方</td></tr><tr><td>double</td><td>ln(double a)</td><td>返回给定数值的自然对数</td></tr><tr><td>double</td><td>log10(double a)</td><td>返回给定数值的以10为底自然对数</td></tr><tr><td>double</td><td>log2(double a)</td><td>返回给定数值的以2为底自然对数</td></tr><tr><td>double</td><td>log(double base, double a)</td><td>返回给定底数及指数返回自然对数</td></tr><tr><td>double</td><td>pow(double a, double p) power(double a, double p)</td><td>返回某数的乘幂</td></tr><tr><td>double</td><td>sqrt(double a)</td><td>返回数值的平方根</td></tr><tr><td>string</td><td>bin(BIGINT a)</td><td>返回二进制格式</td></tr><tr><td>string</td><td>hex(BIGINT a) hex(string a)</td><td>将整数或字符转换为十六进制格式</td></tr><tr><td>string</td><td>unhex(string a)</td><td>十六进制字符转换由数字表示的字符。</td></tr><tr><td>string</td><td>conv(BIGINT num, int from_base, int to_base)</td><td>将 指定数值，由原来的度量体系转换为指定的试题体系。例如CONV(‘a’,16,2),返回。参考：’1010′ <a href="http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv</a></td></tr><tr><td>double</td><td>abs(double a)</td><td>取绝对值</td></tr><tr><td>int double</td><td>pmod(int a, int b) pmod(double a, double b)</td><td>返回a除b的余数的绝对值</td></tr><tr><td>double</td><td>sin(double a)</td><td>返回给定角度的正弦值</td></tr><tr><td>double</td><td>asin(double a)</td><td>返回x的反正弦，即是X。如果X是在-1到1的正弦值，返回NULL。</td></tr><tr><td>double</td><td>cos(double a)</td><td>返回余弦</td></tr><tr><td>double</td><td>acos(double a)</td><td>返回X的反余弦，即余弦是X，，如果-1&lt;= A &lt;= 1，否则返回null.</td></tr><tr><td>int double</td><td>positive(int a) positive(double a)</td><td>返回A的值，例如positive(2)，返回2。</td></tr><tr><td>int double</td><td>negative(int a) negative(double a)</td><td>返回A的相反数，例如negative(2),返回-2。</td></tr></tbody></table><h5 id="2-2收集函数"><a href="#2-2收集函数" class="headerlink" title="2.2收集函数"></a>2.2收集函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>int</td><td>size(Map&lt;K.V&gt;)</td><td>返回的map类型的元素的数量</td></tr><tr><td>int</td><td>size(Array<t>)</t></td><td>返回数组类型的元素数量</td></tr></tbody></table><h5 id="2-3类型转换函数"><a href="#2-3类型转换函数" class="headerlink" title="2.3类型转换函数"></a>2.3类型转换函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>指定 “type”</td><td>cast(expr as<type>)</type></td><td>类型转换。例如将字符”1″转换为整数:cast(’1′ as bigint)，如果转换失败返回NULL。</td></tr></tbody></table><h5 id="2-4日期函数"><a href="#2-4日期函数" class="headerlink" title="2.4日期函数"></a>2.4日期函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>string</td><td>from_unixtime(bigint unixtime[, string format])</td><td>UNIX_TIMESTAMP参数表示返回一个值’YYYY- MM – DD HH：MM：SS’或YYYYMMDDHHMMSS.uuuuuu格式，这取决于是否是在一个字符串或数字语境中使用的功能。该值表示在当前的时区。</td></tr><tr><td>bigint</td><td>unix_timestamp()</td><td>如果不带参数的调用，返回一个Unix时间戳（从’1970- 01 – 0100:00:00′到现在的UTC秒数）为无符号整数。</td></tr><tr><td>bigint</td><td>unix_timestamp(string date)</td><td>指定日期参数调用UNIX_TIMESTAMP（），它返回参数值’1970- 01 – 0100:00:00′到指定日期的秒数。</td></tr><tr><td>bigint</td><td>unix_timestamp(string date, string pattern)</td><td>指定时间输入格式，返回到1970年秒数：unix_timestamp(’2009-03-20′, ‘yyyy-MM-dd’) = 1237532400</td></tr><tr><td>string</td><td>to_date(string timestamp)</td><td>返回时间中的年月日： to_date(“1970-01-01 00:00:00″) = “1970-01-01″</td></tr><tr><td>string</td><td>to_dates(string date)</td><td>给定一个日期date，返回一个天数（0年以来的天数）</td></tr><tr><td>int</td><td>year(string date)</td><td>返回指定时间的年份，范围在1000到9999，或为”零”日期的0。</td></tr><tr><td>int</td><td>month(string date)</td><td>返回指定时间的月份，范围为1至12月，或0一个月的一部分，如’0000-00-00′或’2008-00-00′的日期。</td></tr><tr><td>int</td><td>day(string date) dayofmonth(date)</td><td>返回指定时间的日期</td></tr><tr><td>int</td><td>hour(string date)</td><td>返回指定时间的小时，范围为0到23。</td></tr><tr><td>int</td><td>minute(string date)</td><td>返回指定时间的分钟，范围为0到59。</td></tr><tr><td>int</td><td>second(string date)</td><td>返回指定时间的秒，范围为0到59。</td></tr><tr><td>int</td><td>weekofyear(string date)</td><td>返回指定日期所在一年中的星期号，范围为0到53。</td></tr><tr><td>int</td><td>datediff(string enddate, string startdate)</td><td>两个时间参数的日期之差。</td></tr><tr><td>int</td><td>date_add(string startdate, int days)</td><td>给定时间，在此基础上加上指定的时间段。</td></tr><tr><td>int</td><td>date_sub(string startdate, int days)</td><td>给定时间，在此基础上减去指定的时间段。</td></tr></tbody></table><h5 id="2-5条件函数"><a href="#2-5条件函数" class="headerlink" title="2.5条件函数"></a>2.5条件函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>T</td><td>if(boolean testCondition, T valueTrue, T valueFalseOrNull)</td><td>判断是否满足条件，如果满足返回一个值，如果不满足则返回另一个值。</td></tr><tr><td>T</td><td>COALESCE(T v1, T v2, …)</td><td>返回一组数据中，第一个不为NULL的值，如果均为NULL,返回NULL。</td></tr><tr><td>T</td><td>CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</td><td>当a=b时,返回c；当a=d时，返回e，否则返回f。</td></tr><tr><td>T</td><td>CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END</td><td>当值为a时返回b,当值为c时返回d。否则返回e。</td></tr></tbody></table><h5 id="2-6字符函数"><a href="#2-6字符函数" class="headerlink" title="2.6字符函数"></a>2.6字符函数</h5><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>int</td><td>length(string A)</td><td>返回字符串的长度</td></tr><tr><td>string</td><td>reverse(string A)</td><td>返回倒序字符串</td></tr><tr><td>string</td><td>concat(string A, string B…)</td><td>连接多个字符串，合并为一个字符串，可以接受任意数量的输入字符串</td></tr><tr><td>string</td><td>concat_ws(string SEP, string A, string B…)</td><td>链接多个字符串，字符串之间以指定的分隔符分开。</td></tr><tr><td>string</td><td>substr(string A, int start) substring(string A, int start)</td><td>从文本字符串中指定的起始位置后的字符。</td></tr><tr><td>string</td><td>substr(string A, int start, int len) substring(string A, int start, int len)</td><td>从文本字符串中指定的位置指定长度的字符。</td></tr><tr><td>string</td><td>upper(string A) ucase(string A)</td><td>将文本字符串转换成字母全部大写形式</td></tr><tr><td>string</td><td>lower(string A) lcase(string A)</td><td>将文本字符串转换成字母全部小写形式</td></tr><tr><td>string</td><td>trim(string A)</td><td>删除字符串两端的空格，字符之间的空格保留</td></tr><tr><td>string</td><td>ltrim(string A)</td><td>删除字符串左边的空格，其他的空格保留</td></tr><tr><td>string</td><td>rtrim(string A)</td><td>删除字符串右边的空格，其他的空格保留</td></tr><tr><td>string</td><td>regexp_replace(string A, string B, string C)</td><td>字符串A中的B字符被C字符替代</td></tr><tr><td>string</td><td>regexp_extract(string subject, string pattern, int index)</td><td>通过下标返回正则表达式指定的部分。regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) returns ‘bar.’</td></tr><tr><td>string</td><td>parse_url(string urlString, string partToExtract [, string keyToExtract])</td><td>返回URL指定的部分。parse_url(‘<a href="http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1′" target="_blank" rel="noopener">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1′</a>, ‘HOST’) 返回：’facebook.com’</td></tr><tr><td>string</td><td>get_json_object(string json_string, string path)</td><td>select a.timestamp, get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a;</td></tr><tr><td>string</td><td>space(int n)</td><td>返回指定数量的空格</td></tr><tr><td>string</td><td>repeat(string str, int n)</td><td>重复N次字符串</td></tr><tr><td>int</td><td>ascii(string str)</td><td>返回字符串中首字符的数字值</td></tr><tr><td>string</td><td>lpad(string str, int len, string pad)</td><td>返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从左侧填补。</td></tr><tr><td>string</td><td>rpad(string str, int len, string pad)</td><td>返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从右侧填补。</td></tr><tr><td>array</td><td><strong>split(string str, string pat)</strong></td><td>将字符串转换为数组 <code>select split(name, &#39;-&#39;) from psn</code></td></tr><tr><td>int</td><td>find_in_set(string str, string strList)</td><td>返回字符串str第一次在strlist出现的位置。如果任一参数为NULL,返回NULL；如果第一个参数包含逗号，返回0。</td></tr><tr><td>array&lt;array<string>&gt;</string></td><td>sentences(string str, string lang, string locale)</td><td>将字符串中内容按语句分组，每个单词间以逗号分隔，最后返回数组。 例如sentences(‘Hello there! How are you?’) 返回：( (“Hello”, “there”), (“How”, “are”, “you”) )</td></tr><tr><td>array&lt;struct&lt;string,double&gt;&gt;</td><td>ngrams(array&lt;array<string>&gt;, int N, int K, int pf)</string></td><td>SELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter;</td></tr><tr><td>array&lt;struct&lt;string,double&gt;&gt;</td><td>context_ngrams(array&lt;array<string>&gt;, array<string>, int K, int pf)</string></string></td><td>SELECT context_ngrams(sentences(lower(tweet)), array(null,null), 100, [, 1000]) FROM twitter;</td></tr></tbody></table><h4 id="3-内置的聚合函数（UDAF）"><a href="#3-内置的聚合函数（UDAF）" class="headerlink" title="3.内置的聚合函数（UDAF）"></a>3.内置的聚合函数（UDAF）</h4><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>bigint</td><td>count(*) , count(expr), count(DISTINCT expr[, expr_., expr_.])</td><td>返回记录条数。</td></tr><tr><td>double</td><td>sum(col), sum(DISTINCT col)</td><td>求和</td></tr><tr><td>double</td><td>avg(col), avg(DISTINCT col)</td><td>求平均值</td></tr><tr><td>double</td><td>min(col)</td><td>返回指定列中最小值</td></tr><tr><td>double</td><td>max(col)</td><td>返回指定列中最大值</td></tr><tr><td>double</td><td>var_pop(col)</td><td>返回指定列的方差</td></tr><tr><td>double</td><td>var_samp(col)</td><td>返回指定列的样本方差</td></tr><tr><td>double</td><td>stddev_pop(col)</td><td>返回指定列的偏差</td></tr><tr><td>double</td><td>stddev_samp(col)</td><td>返回指定列的样本偏差</td></tr><tr><td>double</td><td>covar_pop(col1, col2)</td><td>两列数值协方差</td></tr><tr><td>double</td><td>covar_samp(col1, col2)</td><td>两列数值样本协方差</td></tr><tr><td>double</td><td>corr(col1, col2)</td><td>返回两列数值的相关系数</td></tr><tr><td>double</td><td>percentile(col, p)</td><td>返回数值区域的百分比数值点。0&lt;=P&lt;=1,否则返回NULL,不支持浮点型数值。</td></tr><tr><td>array<double></double></td><td>percentile(col, array(p~1,,\ [, p,,2,,]…))</td><td>返回数值区域的一组百分比值分别对应的数值点。0&lt;=P&lt;=1,否则返回NULL,不支持浮点型数值。</td></tr><tr><td>double</td><td>percentile_approx(col, p[, B])</td><td>Returns an approximate p^th^ percentile of a numeric column (including floating point types) in the group. The B parameter controls approximation accuracy at the cost of memory. Higher values yield better approximations, and the default is 10,000. When the number of distinct values in col is smaller than B, this gives an exact percentile value.</td></tr><tr><td>array<double></double></td><td>percentile_approx(col, array(p~1,, [, p,,2_]…) [, B])</td><td>Same as above, but accepts and returns an array of percentile values instead of a single one.</td></tr><tr><td>array&lt;struct{‘x’,’y’}&gt;</td><td>histogram_numeric(col, b)</td><td>Computes a histogram of a numeric column in the group using b non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that represent the bin centers and heights</td></tr><tr><td>array</td><td>collect_set(col)</td><td>返回无重复记录</td></tr></tbody></table><h4 id="4-内置表生成函数（UDTF）"><a href="#4-内置表生成函数（UDTF）" class="headerlink" title="4.内置表生成函数（UDTF）"></a>4.内置表生成函数（UDTF）</h4><table><thead><tr><th>返回类型</th><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>数组</td><td>explode(array<type> a)</type></td><td>数组一条记录中有多个参数，将参数拆分，每个参数生成一列。</td></tr><tr><td></td><td>json_tuple</td><td>get_json_object 语句：select a.timestamp, get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a; json_tuple语句: select a.timestamp, b.* from log a lateral view json_tuple(a.appevent, ‘eventid’, ‘eventname’) b as f1, f2</td></tr></tbody></table><h3 id="Hive动态分区"><a href="#Hive动态分区" class="headerlink" title="Hive动态分区"></a>Hive动态分区</h3><ul><li>hive的静态分区需要用户在插入数据的时候必须手动指定hive的分区字段值，但是这样的话会导致用户的操作复杂度提高，而且在使用的时候会导致数据只能插入到某一个指定分区，无法让数据散列分布，因此更好的方式是当数据在进行插入的时候，根据数据的某一个字段或某几个字段<strong>值</strong>(静态分区必须要知道所有值，而动态分区无需提前知道)动态的将数据插入到不同的目录中，此时引入动态分区</li><li>hive的动态分区配置</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--hive设置hive动态分区开启。默认：true</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--hive的动态分区模式。默认：strict（至少有一个分区列是静态分区，为了防止动态产生的分区过多）</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 每一个执行mr节点上，允许创建的动态分区的最大数量(100)</span></span><br><span class="line"><span class="comment">-- set hive.exec.max.dynamic.partitions.pernode;</span></span><br><span class="line"><span class="comment">-- 所有执行mr节点上，允许创建的所有动态分区的最大数量(1000)	</span></span><br><span class="line"><span class="comment">-- set hive.exec.max.dynamic.partitions;</span></span><br><span class="line"><span class="comment">-- 所有的mr job允许创建的文件的最大数量(100000)	</span></span><br><span class="line"><span class="comment">-- set hive.exec.max.created.files;</span></span><br></pre></td></tr></table></figure><ul><li>语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--Hive extension (dynamic partition inserts):</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure><ul><li>案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建临时数据库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_dynamic_part_tmp(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    age <span class="built_in">int</span>,</span><br><span class="line">    sex <span class="built_in">int</span>,</span><br><span class="line">    likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    address <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 往临时表加载数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/test/data/psn_dynamic_part'</span> <span class="keyword">into</span> <span class="keyword">table</span> psn_dynamic_part_tmp;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn_dynamic_part_tmp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_dynamic_part(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    address <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(age <span class="built_in">int</span>, sex <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- **插入数据时，此时会产生一个MR任务**</span></span><br><span class="line"><span class="comment">-- 注意select字段的顺序，需要和目标表字段对应，不能select *</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> psn_dynamic_part</span><br><span class="line"><span class="keyword">partition</span> (age,sex)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,likes,address,age,sex <span class="keyword">from</span> psn_dynamic_part_tmp</span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 最终会动态根据值创建dfs分区目录，如：/user/hive/warehouse/psn_dynamic_part/age=18|age=.../sex=1|sex=0</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn_dynamic_part;</span><br></pre></td></tr></table></figure><ul><li>案例数据(/home/test/data/psn_dynamic_part)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1,smalle,18,1,games-music,addr1:shanghai-add2:beijing</span><br><span class="line">2,test1,20,1,book-music1,addr1:guangzhou-add2:beijing</span><br><span class="line">3,test2,18,0,book-music2,addr1:guangzhou</span><br><span class="line">4,test3,18,0,music3,addr1:guangzhou</span><br><span class="line">5,test4,54,0,music2,addr1:shanghai</span><br><span class="line">6,test5,37,1,book-music2,addr1:shanghai-add2:beijing</span><br><span class="line">7,test6,18,0,book,addr1:shanghai-add2:beijing</span><br><span class="line">8,test7,28,1,book,add1:beijing</span><br></pre></td></tr></table></figure><ul><li>动态分区严格模式下</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 动静分区结合，静态分区需要出现在动态分区字段之前</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> psn_dynamic_part</span><br><span class="line"><span class="keyword">partition</span> (age=<span class="number">18</span>,sex)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,likes,address,age,sex</span><br><span class="line"><span class="keyword">from</span> psn_dynamic_part_tmp</span><br><span class="line"><span class="keyword">where</span> age=<span class="number">18</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h3><ul><li>分桶说明<ul><li>Hive分桶表是对列值取hash值得方式，将不同数据放到不同文件中存储</li><li>对于hive中每一个表、分区都可以进一步进行分桶，从而降低每个文件的大小</li><li>由列的hash值除以桶的个数来决定每条数据划分在哪个桶中</li><li>一次作业产生的桶（文件数量）和reduce task个数一致<ul><li>mr运行时会根据bucket的个数自动分配reduce task个数（用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用）</li></ul></li></ul></li><li><code>set hive.enforce.bucketing=true;</code> 开启hive分桶支持(v2.3.8可不用设置)</li><li>Hive分桶的抽样查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--案例</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> xxx_bucket_table tablesample(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> xxx_columns)</span><br><span class="line"><span class="comment">--TABLESAMPLE语法：</span></span><br><span class="line">TABLESAMPLE(BUCKET x <span class="keyword">OUT</span> <span class="keyword">OF</span> y <span class="keyword">ON</span> cols)</span><br><span class="line"><span class="comment">-- x：表示从哪个bucket开始抽取数据，x=1表示从第一个开始提取，当超过bucket文件个数时会报错</span></span><br><span class="line"><span class="comment">-- y：必须为该表总bucket数的倍数或因子。假设bucket文件数为4</span></span><br><span class="line">    <span class="comment">-- 当y=4, 表示从第x个bucket中取4/4=1份数据(即整个x文件的数据)</span></span><br><span class="line">    <span class="comment">-- 当y=8, 表示从第x个bucket中取4/8=0.5份数据(即整个文件的上半部分行数据)</span></span><br><span class="line">    <span class="comment">-- 尽量不要让其除不尽，因此取其倍数或因子</span></span><br></pre></td></tr></table></figure><ul><li>案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建临时数据</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_bucket_tmp(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, age <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/test/data/psn_bucket'</span> <span class="keyword">into</span> <span class="keyword">table</span> psn_bucket_tmp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- **创建分桶表**(可和分区表结合使用，也可单独使用)</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_bucket(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, age <span class="built_in">int</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (age) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 插入数据，会启动一个MR任务(Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 4)</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> psn_bucket <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span>, age <span class="keyword">from</span> psn_bucket_tmp;</span><br><span class="line"><span class="comment">-- 会产生4个文件：/user/hive/warehouse/psn_bucket/000000_1|000001_0|000002_0|000003_0</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> psn_bucket; <span class="comment">-- 此时返回的数据id是乱序(8,4,7,3,6,2,5,1)，因为数据是基于hash分散了</span></span><br><span class="line"><span class="comment">-- 抽样，返回7,3</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span>, age <span class="keyword">from</span> psn_bucket tablesample(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> age);</span><br></pre></td></tr></table></figure><ul><li>案例测试数据(/home/test/data/psn_bucket)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1,tom,11</span><br><span class="line">2,cat,22</span><br><span class="line">3,dog,33</span><br><span class="line">4,hive,44</span><br><span class="line">5,hbase,55</span><br><span class="line">6,mr,66</span><br><span class="line">7,alice,77</span><br><span class="line">8,scala,88</span><br></pre></td></tr></table></figure><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><h4 id="Lateral-View"><a href="#Lateral-View" class="headerlink" title="Lateral View"></a>Lateral View</h4><ul><li>Lateral View用于和UDTF函数(如: explode、split)结合来使用<ul><li>首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表</li><li>主要解决在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题</li></ul></li><li>语法：<code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias (&#39;,&#39; columnAlias)</code></li><li>案例(查询所有爱好和地址的个数，数据参考上文psn_dynamic_part表)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 将每个人的爱好依次查询出来(多行，可能重复)</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">col</span></span><br><span class="line"><span class="comment">book</span></span><br><span class="line"><span class="comment">music2</span></span><br><span class="line"><span class="comment">music3</span></span><br><span class="line"><span class="comment">book</span></span><br><span class="line"><span class="comment">...</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">select</span> explode(likes) <span class="keyword">from</span> psn_dynamic_part;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">key	value</span></span><br><span class="line"><span class="comment">addr1	guangzhou</span></span><br><span class="line"><span class="comment">addr1	guangzhou</span></span><br><span class="line"><span class="comment">addr1	shanghai</span></span><br><span class="line"><span class="comment">add2	beijing</span></span><br><span class="line"><span class="comment">...</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">select</span> explode(address) <span class="keyword">from</span> psn_dynamic_part;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 报错，无法执行：FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions</span></span><br><span class="line"><span class="comment">-- 因此需要结合Lateral View</span></span><br><span class="line"><span class="comment">-- select count(distinct(explode(likes))) from psn_dynamic_part; </span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- **Lateral View**，会产生MR任务</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span>(myCol1)), <span class="keyword">count</span>(<span class="keyword">distinct</span>(myCol2)) <span class="keyword">from</span> psn_dynamic_part</span><br><span class="line">LATERAL <span class="keyword">VIEW</span> explode(likes) myTable1 <span class="keyword">AS</span> myCol1 </span><br><span class="line">LATERAL <span class="keyword">VIEW</span> explode(address) myTable2 <span class="keyword">AS</span> myCol2, myCol3;</span><br></pre></td></tr></table></figure><h4 id="Hive视图"><a href="#Hive视图" class="headerlink" title="Hive视图"></a>Hive视图</h4><ul><li>特点<ul><li>只能查询，不能做加载数据操作</li><li>视图的创建，只是保存一份元数据，查询视图时才执行对应的子查询，不支持物化视图(v3.0.0引入物化视图)</li><li>view的执行优先级更高于外部查询，view定义ORDER BY/LIMIT语句优先级也高于外部</li></ul></li><li>语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建视图</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]view_name </span><br><span class="line">    [(column_name [<span class="keyword">COMMENT</span> column_comment], ...) ]</span><br><span class="line">    [<span class="keyword">COMMENT</span> view_comment]</span><br><span class="line">    [TBLPROPERTIES (property_name = property_value, ...)]</span><br><span class="line">    <span class="keyword">AS</span> <span class="keyword">SELECT</span> ... ;</span><br><span class="line"><span class="comment">--查询视图</span></span><br><span class="line"><span class="keyword">SELECT</span> colums <span class="keyword">FROM</span> <span class="keyword">view</span>;</span><br><span class="line"><span class="comment">--删除视图</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] [db_name.]view_name;</span><br></pre></td></tr></table></figure><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建索引</span></span><br><span class="line"><span class="comment">-- as: 指定索引器</span></span><br><span class="line"><span class="comment">-- in table: 指定索引表，若不指定默认生成在default__psn_dynamic_part_idx_psn_name__表中</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> idx_psn_name <span class="keyword">on</span> <span class="keyword">table</span> psn_dynamic_part(<span class="keyword">name</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span></span><br><span class="line"><span class="keyword">with</span> <span class="keyword">deferred</span> <span class="keyword">rebuild</span></span><br><span class="line"><span class="keyword">in</span> <span class="keyword">table</span> idx_psn_name_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建完索引表后，不会立即创建索引数据，需要手动 rebuild 重建索引才会通过MR任务产生索引数据</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> idx_psn_name_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">--查询索引</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">index</span> <span class="keyword">on</span> psn_dynamic_part;</span><br><span class="line"></span><br><span class="line"><span class="comment">--重建索引（建立索引之后必须重建索引才能生效），会产生MR任务创建索引数据</span></span><br><span class="line"><span class="comment">-- 每次数据增加了，必须重新执行来重建索引，因此所有很少使用，但是在特定场景可进行使用，如分析去年的数据(数据不会改变)</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">index</span> idx_psn_name <span class="keyword">on</span> psn_dynamic_part <span class="keyword">rebuild</span>;</span><br><span class="line"><span class="comment">-- 重建后 idx_psn_name_table 中的数据如下</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">idx_psn_name_table.name	idx_psn_name_table._bucketname	idx_psn_name_table._offsets	idx_psn_name_table.age	idx_psn_name_table.sex</span></span><br><span class="line"><span class="comment">test2	hdfs://aezocn/user/hive/warehouse/psn_dynamic_part/age=18/sex=0/000000_0	[0]	18	0</span></span><br><span class="line"><span class="comment">smalle	hdfs://aezocn/user/hive/warehouse/psn_dynamic_part/age=18/sex=1/000000_0	[0]	18	1</span></span><br><span class="line"><span class="comment">test1	hdfs://aezocn/user/hive/warehouse/psn_dynamic_part/age=20/sex=1/000000_0	[0]	20	1</span></span><br><span class="line"><span class="comment">...</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--删除索引</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">index</span> <span class="keyword">if</span> <span class="keyword">exists</span> idx_psn_name <span class="keyword">on</span> psn_dynamic_part;</span><br></pre></td></tr></table></figure><h3 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h3><ul><li><code>LEFT SEMI JOIN</code> 类似mysql中的exists</li><li>关联多张表时，尽可能使用相同的关联条件字段(不同的条件字段产生MR任务执行更耗时)</li><li>JOIN案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1 smalle tom</span></span><br><span class="line"><span class="keyword">select</span> a.id,a.name, b.name <span class="keyword">as</span> lang </span><br><span class="line"><span class="keyword">from</span> psn_dynamic_part a </span><br><span class="line"><span class="keyword">join</span> psn_bucket b <span class="keyword">on</span> a.id = b.id</span><br><span class="line"><span class="keyword">where</span> a.id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h2><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization</a></p></blockquote><ul><li>hive授权模型<ul><li>Default Hive Authorization (Legacy Mode)<ul><li>hive默认授权: 设计目的仅仅只是为了防止用户产生误操作，而不是防止恶意用户访问未经授权的数据</li></ul></li><li>Storage Based Authorization in the Metastore Server<ul><li>基于存储的授权: 可以对Metastore中的元数据进行保护，但是没有提供更加细粒度的访问控制（例如：列级别、行级别）</li></ul></li><li>SQL Standards Based Authorization in HiveServer2<ul><li>基于SQL标准的Hive授权: <strong>完全兼容SQL的授权模型，推荐使用该模式</strong></li><li>默认包含两种角色: public、admin</li></ul></li></ul></li><li>基于SQL标准的hiveserver2授权模式的限制<ul><li>启用当前认证方式之后，dfs, add, delete, compile, and reset等命令被禁用</li><li>通过set命令设置hive configuration的方式被限制某些用户使用。（可通过修改配置文件hive-site.xml中hive.security.authorization.sqlstd.confwhitelist进行配置）</li><li>添加、删除函数以及宏的操作，仅为具有admin的用户开放</li><li>用户自定义函数（开放支持永久的自定义函数），可通过具有admin角色的用户创建，其他用户都可以使用</li><li>Transform功能被禁用</li></ul></li><li>基于SQL标准的hiveserver2授权模式配置</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hive-site.xml，修改后无需重启 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 拥有admin角色的用户，多个使用逗号分割 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.users.in.admin.role<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>test<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authenticator.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>相关命令</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看所有存在的角色。默认包含两种角色：public、admin</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">roles</span>;</span><br><span class="line"><span class="comment">-- 查看当前具有的角色，尽管是admin账号登录，默认也是public角色，需要使用admin角色时再手动切换</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">current</span> <span class="keyword">roles</span>;</span><br><span class="line"><span class="comment">-- 设置当前用户角色: set role (role_name|all|none);</span></span><br><span class="line"><span class="keyword">set</span> <span class="keyword">role</span> <span class="keyword">admin</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建角色(只有切换成admin才能进行创建角色、赋权等操作)</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">role</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="comment">-- 删除角色</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">role</span> <span class="keyword">test</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将test角色的权限赋予abc这个用户(也可以是角色: grant test to role test2;)</span></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">test</span> <span class="keyword">to</span> <span class="keyword">user</span> abc; <span class="comment">-- 查看赋予用户abc的角色</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">role</span> <span class="keyword">grant</span> <span class="keyword">user</span> abc;</span><br><span class="line"><span class="keyword">show</span> principals <span class="keyword">test</span>; <span class="comment">-- 查看test角色下的用户或子角色</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 授权(给用户或角色)</span></span><br><span class="line"><span class="comment">-- 权限：INSERT | SELECT | UPDATE | DELETE | ALL</span></span><br><span class="line"><span class="comment">-- with grant option 类似mysql，即被赋权用户可以将此权限赋给其他人</span></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> <span class="keyword">table</span> psn <span class="keyword">to</span> <span class="keyword">role</span> <span class="keyword">test</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">update</span> <span class="keyword">on</span> <span class="keyword">table</span> psn <span class="keyword">to</span> <span class="keyword">user</span> abc;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">grant</span> <span class="keyword">role</span> <span class="keyword">test</span> <span class="keyword">on</span> <span class="keyword">table</span> psn; <span class="comment">-- 显示赋值给test用户的psn表的权限</span></span><br><span class="line"><span class="comment">-- 撤销权限：则abc用户将无法再查询abc</span></span><br><span class="line"><span class="keyword">revoke</span> <span class="keyword">select</span> <span class="keyword">on</span> <span class="keyword">table</span> psn <span class="keyword">from</span> <span class="keyword">role</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure><h2 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h2><ul><li>说明<ul><li>Hive的存储层依托于HDFS，Hive的计算层依托于MapReduce，一般Hive的执行效率主要取决于SQL语句的执行效率，因此Hive的优化的核心思想是MapReduce的优化</li></ul></li><li><p>查看Hive执行计划</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--显示SQL的执行计划，添加extended关键字可以查看更加详细的执行计划</span></span><br><span class="line"><span class="keyword">explain</span> <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> psn;</span><br><span class="line"><span class="keyword">explain</span> <span class="keyword">extended</span> <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> psn;</span><br></pre></td></tr></table></figure></li><li><p>Hive的抓取策略</p><ul><li>Hive的某些SQL语句需要转换成MapReduce的操作，某些SQL语句就不需要转换成MapReduce操作</li><li>理论上来说，所有的SQL语句都需要转换成MapReduce操作，只不过Hive在转换SQL语句的过程中会做部分优化，使某些简单的操作不再需要转换成MapReduce，例如<ul><li>select 仅支持本表字段</li><li>where仅对本表字段做条件过滤</li></ul></li><li>设置Hive的数据抓取策略: <code>set hive.fetch.task.conversion=more;</code> 取值 none|more(默认)</li></ul></li><li>Hive本地模式<ul><li>类似于MapReduce的操作，Hive的运行也分为本地模式和集群模式，在开发阶段可以选择使用本地执行，提高SQL语句的执行效率，验证SQL语句是否正确</li><li><strong>设置本地模式(仅开发环境使用)</strong> <code>set hive.exec.mode.local.auto=true;</code></li><li>注意：要想使用Hive的本地模式，文件个数不能超过4个，加载数据文件大小不能超过128M(<code>set hive.exec.mode.local.auto.inputbytes.max=128M</code>)，如果超过了，就算设置了本地模式，也会按照集群模式运行</li></ul></li><li>Hive并行模式<ul><li>在SQL语句足够复杂的情况下，可能在一个SQL语句中包含多个子查询语句，且多个子查询语句之间没有任何依赖关系，此时可以Hive运行的并行度</li><li><strong>设置Hive SQL的并行运行</strong> <code>set hive.exec.parallel=true;</code></li><li>注意：Hive的并行度并不是无限增加的，在一次SQL计算中，可以通过以下参数来设置最大并行的job的个数(<code>set hive.exec.parallel.thread.number;</code>)</li></ul></li><li>Hive严格模式<ul><li>Hive中为了提高SQL语句的执行效率，可以设置严格模式，充分利用Hive的某些特点</li><li>设置Hive的严格模式 <code>set hive.mapred.mode=strict;</code></li><li>注意：当设置严格模式之后，会有如下限制<ul><li>对于分区表，必须添加where对于分区字段的条件过滤</li><li>order by语句必须包含limit输出限制(order by实际很少使用)</li><li>限制执行笛卡尔积的查询</li></ul></li></ul></li><li>Hive排序，Hive中支持多种排序操作适合不同的应用场景<ul><li><code>sort by</code> 对于单个reduce的数据进行排序</li><li><code>distribute by</code> 分区排序，经常和sort by结合使用</li><li><code>cluster by</code> 相当于 sort by + distribute by<ul><li>cluster by不能通过asc、desc的方式指定排序规则；可通过 distribute by column sort by column asc|desc 的方式</li></ul></li><li><code>order by</code> 对于查询结果做全排序，只允许有一个reduce处理（当数据量较大时，应慎用。严格模式下，必须结合limit来使用）</li></ul></li><li><p>Hive join</p><ul><li>Hive 在多个表的join操作时尽可能多的使用相同的连接键，这样在转换MR任务时会转换成少的MR的任务</li><li><p>手动Map join，在map端完成join操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--SQL方式，在SQL语句中添加MapJoin标记（mapjoin hint）</span></span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(smalltable) */</span> smalltable.key, bigtable.value </span><br><span class="line"><span class="keyword">from</span> smalltable <span class="keyword">join</span> bigtable <span class="keyword">on</span> smalltable.key = bigtable.key;</span><br></pre></td></tr></table></figure></li><li><p>开启自动的Map Join(默认开启)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--通过修改以下配置启用自动的mapjoin(默认为true)</span></span><br><span class="line"><span class="comment">--该参数为true时，Hive自动对左边的表统计量，如果是小表就加入内存，即对小表使用Map join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--相关配置参数</span></span><br><span class="line"><span class="comment">--大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize;  </span><br><span class="line"><span class="comment">-- 是否忽略mapjoin hint即mapjoin标记，默认值：true</span></span><br><span class="line"><span class="keyword">set</span> hive.ignore.mapjoin.hint;</span><br></pre></td></tr></table></figure></li><li><p>大表join大表(无很好的解决方案，下面两个小优化点不一定起效)</p><ul><li>空key过滤：有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤</li><li>空key转换：有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上</li></ul></li></ul></li><li><p>Map-Side聚合</p><ul><li><p>Hive的某些SQL操作可以实现map端的聚合，类似于MR的combine操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--通过设置以下参数开启在Map端的聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--相关配置参数</span></span><br><span class="line"><span class="comment">--map端group by执行聚合时处理的多少行数据（默认：100000）</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval;</span><br><span class="line"><span class="comment">--进行聚合的最小比例（预先对100000条数据做聚合，若聚合之后的数据量/100000的值大于该配置0.5，则不会聚合）</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction;</span><br><span class="line"><span class="comment">--map端聚合使用的内存的最大值</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.percentmemory;</span><br><span class="line"><span class="comment">--是否对GroupBy产生的数据倾斜做优化，默认为false</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>合并小文件</p><ul><li><p>Hive在操作的时候，如果文件数目小，容易在文件存储端造成压力，给hdfs造成压力，影响效率</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置合并属性</span></span><br><span class="line"><span class="comment">--是否合并map输出文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--是否合并reduce输出文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--合并文件的大小</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task=<span class="number">256</span>*<span class="number">1000</span>*<span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>合理设置Map以及Reduce的数量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--Map数量相关的参数</span></span><br><span class="line"><span class="comment">--一个split的最大值，即每个map处理文件的最大值</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size;</span><br><span class="line"><span class="comment">--一个节点上split的最小值</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node;</span><br><span class="line"><span class="comment">--一个机架上split的最小值</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack;</span><br><span class="line"><span class="comment">--Reduce数量相关的参数</span></span><br><span class="line"><span class="comment">--强制指定reduce任务的数量</span></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks;</span><br><span class="line"><span class="comment">--每个reduce任务处理的数据量</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br><span class="line"><span class="comment">--每个任务最大的reduce数</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br></pre></td></tr></table></figure></li><li><p>JVM重用</p><ul><li>适用场景：小文件个数过多，task个数过多</li><li>缺点：设置开启之后，task插槽会一直占用资源，不论是否有task运行，直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源</li><li>设置task插 <code>set mapred.job.reuse.jvm.num.tasks=n;</code> n为task插槽个数</li></ul></li></ul><h2 id="压缩和存储"><a href="#压缩和存储" class="headerlink" title="压缩和存储"></a>压缩和存储</h2><ul><li>MR支持的压缩编码: DEFAULT、gzip、bzip2、LZO、LZ4、Snappy(压缩比和压缩效率都适中)</li><li>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）</li></ul><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><ul><li><p>开启Map输出阶段压缩</p><ul><li><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启hive中间传输数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce中map输出压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce中map输出数据的压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="comment">-- 执行查询语句</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> psn;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>开启Reduce输出阶段压缩</p><ul><li><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启hive最终输出数据压缩功能(默认是false, 输出文件为文本文件)</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--开启mapreduce最终输出数据压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--设置mapreduce最终数据输出压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="comment">--设置mapreduce最终数据输出压缩为块压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type=<span class="keyword">BLOCK</span>;</span><br><span class="line"><span class="comment">--测试一下输出结果是否是压缩文件</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/test/out/psn'</span> <span class="keyword">select</span> * <span class="keyword">from</span> psn;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h3><ul><li>Hive支持的存储数的格式主要有：TEXTFILE(默认)、SEQUENCEFILE、ORC、PARQUET<ul><li>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的</li></ul></li><li>行存储的特点<ul><li>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快</li></ul></li><li><p>列存储的特点</p><ul><li><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法</p><p><img src="/data/images/bigdata/hive-store-type.png" alt="行存储和列存储"></p></li></ul></li><li>存储于压缩案例(未测试)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建一个非压缩的的ORC存储方式</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(track_time <span class="keyword">string</span>,<span class="keyword">url</span> <span class="keyword">string</span>,session_id <span class="keyword">string</span>,referer <span class="keyword">string</span>,ip <span class="keyword">string</span>,end_user_id <span class="keyword">string</span>,city_id <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"NONE"</span>);</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_none <span class="keyword">select</span> * <span class="keyword">from</span> log_text;</span><br><span class="line"><span class="comment">-- 查看文件大小</span></span><br><span class="line">dfs -du -h /user/hive/warehouse/log_orc_none/;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建一个SNAPPY压缩的ORC存储方式</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(track_time <span class="keyword">string</span>,<span class="keyword">url</span> <span class="keyword">string</span>,session_id <span class="keyword">string</span>,referer <span class="keyword">string</span>,ip <span class="keyword">string</span>,end_user_id <span class="keyword">string</span>,city_id <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_snappy <span class="keyword">select</span> * <span class="keyword">from</span> log_text;</span><br><span class="line"><span class="comment">-- 查看文件大小(文件大小比上面更小)</span></span><br><span class="line">dfs -du -h /user/hive/warehouse/log_orc_snappy/;</span><br></pre></td></tr></table></figure><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><ul><li>基于HiveServer2 + Zookeeper完成HA</li><li>如在node03 + node04上启动HiveServer2服务(未测试)</li><li>配置如下</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>Hello1234!<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用服务发现 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.support.dynamic.service.discovery<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.zookeeper.namespace<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hiveserver2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:2181,node02:2181,node03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- node04节点的配置则改成 node04 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10001<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>beeline连接 <code>!connect jdbc:hive2://node01,node02,node03/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2 root 123</code></li></ul></div><div></div><div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> smalle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="http://blog.aezo.cn/2021/06/01/bigdata/hive/" title="Hive">http://blog.aezo.cn/2021/06/01/bigdata/hive/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/sql/" rel="tag"># sql</a> <a href="/tags/hadoop/" rel="tag"># hadoop</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2021/04/13/java/java-src/sofastack-src/" rel="next" title="SOFAStack源码分析"><i class="fa fa-chevron-left"></i> SOFAStack源码分析</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2021/07/11/lang/vbs/" rel="prev" title="Visual Basic Script">Visual Basic Script<i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/15698218?v=3&u=ce740bf0b67ff0d74990ba6fc644d6e92f572dcb&s=400" alt="smalle"><p class="site-author-name" itemprop="name">smalle</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">156</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">140</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装"><span class="nav-number">2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用HiveServer2组件"><span class="nav-number">3.</span> <span class="nav-text">使用HiveServer2组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启停"><span class="nav-number">4.</span> <span class="nav-text">启停</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive命令使用"><span class="nav-number">5.</span> <span class="nav-text">Hive命令使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Cli"><span class="nav-number">5.1.</span> <span class="nav-text">Hive Cli</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数操作"><span class="nav-number">5.2.</span> <span class="nav-text">参数操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#源码入口"><span class="nav-number">5.3.</span> <span class="nav-text">源码入口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SQL操作"><span class="nav-number">6.</span> <span class="nav-text">SQL操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简单使用"><span class="nav-number">6.1.</span> <span class="nav-text">简单使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据库操作"><span class="nav-number">6.2.</span> <span class="nav-text">数据库操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDL-表操作"><span class="nav-number">6.3.</span> <span class="nav-text">DDL(表操作)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建表语法"><span class="nav-number">6.3.1.</span> <span class="nav-text">创建表语法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建表案例"><span class="nav-number">6.3.2.</span> <span class="nav-text">创建表案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#修改表结构案例"><span class="nav-number">6.3.3.</span> <span class="nav-text">修改表结构案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#修复分区使用"><span class="nav-number">6.3.4.</span> <span class="nav-text">修复分区使用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DML-数据操作"><span class="nav-number">6.4.</span> <span class="nav-text">DML(数据操作)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#插入数据语法"><span class="nav-number">6.4.1.</span> <span class="nav-text">插入数据语法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#插入数据案例"><span class="nav-number">6.4.2.</span> <span class="nav-text">插入数据案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Serde进行数据处理"><span class="nav-number">6.5.</span> <span class="nav-text">Serde进行数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive函数"><span class="nav-number">6.6.</span> <span class="nav-text">Hive函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义函数"><span class="nav-number">6.6.1.</span> <span class="nav-text">自定义函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-内置运算符"><span class="nav-number">6.6.2.</span> <span class="nav-text">1.内置运算符</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1关系运算符"><span class="nav-number">6.6.2.1.</span> <span class="nav-text">1.1关系运算符</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2算术运算符"><span class="nav-number">6.6.2.2.</span> <span class="nav-text">1.2算术运算符</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3逻辑运算符"><span class="nav-number">6.6.2.3.</span> <span class="nav-text">1.3逻辑运算符</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4复杂类型函数"><span class="nav-number">6.6.2.4.</span> <span class="nav-text">1.4复杂类型函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-内置函数"><span class="nav-number">6.6.3.</span> <span class="nav-text">2.内置函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1数学函数"><span class="nav-number">6.6.3.1.</span> <span class="nav-text">2.1数学函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2收集函数"><span class="nav-number">6.6.3.2.</span> <span class="nav-text">2.2收集函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3类型转换函数"><span class="nav-number">6.6.3.3.</span> <span class="nav-text">2.3类型转换函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-4日期函数"><span class="nav-number">6.6.3.4.</span> <span class="nav-text">2.4日期函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-5条件函数"><span class="nav-number">6.6.3.5.</span> <span class="nav-text">2.5条件函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-6字符函数"><span class="nav-number">6.6.3.6.</span> <span class="nav-text">2.6字符函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-内置的聚合函数（UDAF）"><span class="nav-number">6.6.4.</span> <span class="nav-text">3.内置的聚合函数（UDAF）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-内置表生成函数（UDTF）"><span class="nav-number">6.6.5.</span> <span class="nav-text">4.内置表生成函数（UDTF）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive动态分区"><span class="nav-number">6.7.</span> <span class="nav-text">Hive动态分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分桶"><span class="nav-number">6.8.</span> <span class="nav-text">分桶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#视图"><span class="nav-number">6.9.</span> <span class="nav-text">视图</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Lateral-View"><span class="nav-number">6.9.1.</span> <span class="nav-text">Lateral View</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive视图"><span class="nav-number">6.9.2.</span> <span class="nav-text">Hive视图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#索引"><span class="nav-number">6.10.</span> <span class="nav-text">索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JOIN"><span class="nav-number">6.11.</span> <span class="nav-text">JOIN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#权限管理"><span class="nav-number">7.</span> <span class="nav-text">权限管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive调优"><span class="nav-number">8.</span> <span class="nav-text">Hive调优</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#压缩和存储"><span class="nav-number">9.</span> <span class="nav-text">压缩和存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储格式"><span class="nav-number">9.1.</span> <span class="nav-text">文件存储格式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HA"><span class="nav-number">10.</span> <span class="nav-text">HA</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2016 - <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">smalle</span>&nbsp;&nbsp;&nbsp;&nbsp;<div class="powered-by"> 由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="powered-by"> 主题 - <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="theme-info cnzz" style="margin:0 0 -5px 10px"><script type="text/javascript">var cnzz_protocol="https:"==document.location.protocol?"https://":"http://";document.write(unescape("%3Cspan id='cnzz_stat_icon_1276691827'%3E%3C/span%3E%3Cscript src='"+cnzz_protocol+"s23.cnzz.com/z_stat.php%3Fid%3D1276691827%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"))</script></div></div><div class="ad"> <span style="font-weight:700">AD&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=oby5nolb">阿里云大礼包</a></div></div><div class="aezocn"> <span style="font-weight:700">@AEZO.CN&nbsp;&nbsp;&nbsp;&nbsp;</span><div class="theme-info"> <a target="_blank" href="http://shop.aezo.cn/">杂货铺</a></div></div><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d82223039d601f2f819f8fe140a63468";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.1"></script></body></html>